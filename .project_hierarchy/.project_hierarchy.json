{
  "setup.py": {},
  "tests/test_change_detector.py": {
    "TestChangeDetector": {
      "type": "ClassDef",
      "name": "TestChangeDetector",
      "md_content": [
        "**TestChangeDetector**: The function of this Class is to test the functionality of the ChangeDetector class.\n\n**attributes**: This Class does not have any attributes.\n\n**Code Description**:\nThe TestChangeDetector Class is a subclass of the unittest.TestCase Class, which is used for writing and running tests. It contains several test methods that test different functionalities of the ChangeDetector class.\n\nThe setUpClass() method is a class method that is called once before any tests are run. In this method, the test repository path is defined and created if it does not exist. The Git repository is initialized, and the user information is configured. Some test files are created and added to the repository. Finally, the files are committed.\n\nThe test_get_staged_pys() method is a test method that tests the get_staged_pys() method of the ChangeDetector class. It creates a new Python file, adds it to the repository, and then uses the ChangeDetector class to get the staged Python files. It asserts that the newly created file is present in the staged files list.\n\nThe test_get_unstaged_mds() method is a test method that tests the get_unstaged_mds() method of the ChangeDetector class. It modifies a Markdown file without staging it and then uses the ChangeDetector class to get the unstaged Markdown files. It asserts that the modified file is present in the unstaged files list.\n\nThe test_add_unstaged_mds() method is a test method that tests the add_unstaged_mds() method of the ChangeDetector class. It ensures that there is at least one unstaged Markdown file by calling the test_get_unstaged_mds() method. Then, it uses the ChangeDetector class to add the unstaged files. It checks if there are any remaining unstaged Markdown files after the add operation.\n\nThe tearDownClass() method is a class method that is called once after all the tests have been run. It cleans up the test repository by closing the Git repository and removing the test repository directory.\n\n**Note**: \n- The TestChangeDetector Class is used to test the functionality of the ChangeDetector class.\n- The setUpClass() method is used to set up the test environment before running the tests.\n- The test methods test specific functionalities of the ChangeDetector class.\n- The tearDownClass() method is used to clean up the test environment after running the tests."
      ],
      "code_start_line": 6,
      "code_end_line": 89,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class TestChangeDetector(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n\n    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n\n\n    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n\n\n    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n\n\n    @classmethod\n    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "setUpClass": {
      "type": "FunctionDef",
      "name": "setUpClass",
      "md_content": [
        "**setUpClass**: The function of this Function is to set up the necessary environment for the test class. It creates a test repository, initializes a Git repository, configures the Git user information, creates some test files, and simulates Git operations to add and commit the files.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe code begins by defining the test repository path using the `os.path.join()` function, which joins the directory path of the current file (`__file__`) with the 'test_repo' folder name. This ensures that the test repository is created in the same directory as the test file.\n\nNext, the code checks if the test repository folder does not exist using the `os.path.exists()` function. If the folder does not exist, it creates the folder using the `os.makedirs()` function.\n\nThe code then initializes a Git repository using the `Repo.init()` method, passing the test repository path as an argument. This creates a new Git repository in the test repository folder.\n\nNext, the code configures the Git user information using the `repo.git.config()` method. It sets the email to 'ci@example.com' and the name to 'CI User'. This ensures that the Git commits made during the test have the correct user information.\n\nThe code then creates two test files in the test repository folder using the `open()` function. It opens the files in write mode and writes some content to them. The first file is named 'test_file.py' and contains the line `print(\"Hello, Python\")`. The second file is named 'test_file.md' and contains the line `# Hello, Markdown`.\n\nAfter creating the test files, the code simulates Git operations to add and commit the files. It uses the `repo.git.add()` method with the `A=True` argument to add all files in the repository. This stages the files for commit. Then, it uses the `repo.git.commit()` method with the `-m` argument to commit the changes with the message 'Initial commit'.\n\n**Note**: \n- This function is used to set up the necessary environment for the test class.\n- It assumes that the test repository has already been set up and initialized with Git.\n- The test repository path is obtained using the `test_repo_path` attribute, which is set in the `setUpClass` method of the test class."
      ],
      "code_start_line": 8,
      "code_end_line": 32,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_change_detector.py/TestChangeDetector/test_get_unstaged_mds",
        "repo_agent/change_detector.py/ChangeDetector",
        "repo_agent/change_detector.py/ChangeDetector/get_staged_pys",
        "repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files",
        "repo_agent/change_detector.py/ChangeDetector/add_unstaged_files"
      ]
    },
    "test_get_staged_pys": {
      "type": "FunctionDef",
      "name": "test_get_staged_pys",
      "md_content": [
        "**test_get_staged_pys**: The function of this Function is to test the functionality of the `get_staged_pys` method in the `ChangeDetector` class.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: This function first creates a new Python file called \"new_test_file.py\" and writes the content \"print(\"New Python File\")\" into it. The file is then added to the staging area using the `git.add` method.\n\nNext, an instance of the `ChangeDetector` class is created, passing the path of the test repository as an argument. The `get_staged_pys` method is then called on the `change_detector` object to retrieve a list of staged Python files.\n\nThe function asserts that the newly created file \"new_test_file.py\" is present in the list of staged files by checking if its basename exists in the list.\n\nFinally, the function prints the list of staged Python files.\n\n**Note**: \n- This function is used to test the functionality of the `get_staged_pys` method in the `ChangeDetector` class.\n- It assumes that the test repository has already been set up and initialized.\n- The `ChangeDetector` class and the `os` module need to be imported for this function to work properly."
      ],
      "code_start_line": 34,
      "code_end_line": 48,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "test_get_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_get_unstaged_mds",
      "md_content": [
        "**test_get_unstaged_mds**: The function of this Function is to test the functionality of the `get_unstaged_mds` method in the `ChangeDetector` class.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe code begins by creating a new Markdown file named `test_file.md` in the test repository directory. Some additional content is then appended to the file. \n\nNext, an instance of the `ChangeDetector` class is created, passing the test repository path as an argument. \n\nThe `get_to_be_staged_files` method is called on the `change_detector` object to retrieve a list of unstaged files in the repository.\n\nThe code then asserts that the modified file (`test_file.md`) is present in the list of unstaged files.\n\nFinally, a message is printed to the console, displaying the list of unstaged Markdown files.\n\n**Note**: \n- This function is used to test the functionality of the `get_unstaged_mds` method in the `ChangeDetector` class.\n- It assumes that the test repository has already been set up and initialized with Git.\n- The test repository path is obtained using the `test_repo_path` attribute, which is set in the `setUpClass` method of the `TestChangeDetector` class."
      ],
      "code_start_line": 51,
      "code_end_line": 64,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_change_detector.py/TestChangeDetector/setUpClass"
      ],
      "reference_who": []
    },
    "test_add_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_add_unstaged_mds",
      "md_content": [
        "**test_add_unstaged_mds**: The function of this Function is to test the functionality of adding unstaged Markdown files.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function first ensures that there is at least one unstaged Markdown file by calling the `test_get_unstaged_mds()` function. \n\nThen, it creates an instance of the `ChangeDetector` class, passing the test repository path as a parameter. This `ChangeDetector` object is used to perform the operation of adding unstaged files.\n\nNext, it calls the `add_unstaged_files()` method of the `ChangeDetector` object to add the unstaged Markdown files.\n\nAfter that, it retrieves the list of files that are still unstaged by calling the `get_to_be_staged_files()` method of the `ChangeDetector` object.\n\nFinally, it asserts that the length of the `unstaged_files_after_add` list is equal to 0, indicating that all the unstaged Markdown files have been successfully staged. It also prints the number of remaining unstaged Markdown files after the add operation.\n\n**Note**: It is important to note that this test function assumes the existence of at least one unstaged Markdown file before the add operation is performed."
      ],
      "code_start_line": 67,
      "code_end_line": 82,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tearDownClass": {
      "type": "FunctionDef",
      "name": "tearDownClass",
      "md_content": [
        "**tearDownClass**: The function of this Function is to perform cleanup operations after all the test cases in the test class have been executed.\n\n**parameters**: The parameter of this Function is `cls`, which represents the class itself.\n\n**Code Description**: \nThe `tearDownClass` function is responsible for cleaning up the test repository and closing the repository connection. \n\nIn the code, the function first closes the repository connection by calling `cls.repo.close()`. This ensures that the repository is properly closed and any resources associated with it are released.\n\nNext, the function removes the test repository directory by executing the command `os.system('rm -rf ' + cls.test_repo_path)`. This command uses the `os.system` function to execute the shell command `rm -rf` followed by the path of the test repository directory (`cls.test_repo_path`). The `rm -rf` command is used to forcefully remove a directory and its contents. By executing this command, the function ensures that the test repository directory is completely removed.\n\n**Note**: \n- It is important to call `tearDownClass` after all the test cases in the test class have been executed to perform the necessary cleanup operations.\n- The `tearDownClass` function assumes that the test repository has been initialized and the path to the test repository directory is stored in `cls.test_repo_path`.\n- The `tearDownClass` function uses the `os.system` function to execute a shell command. This can be a potential security risk if the command is constructed using user input. It is important to ensure that the command is constructed safely to prevent any unintended consequences."
      ],
      "code_start_line": 86,
      "code_end_line": 89,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "tests/test_structure_tree.py": {
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [
        "**build_path_tree**: The function of this Function is to build a tree-like structure representing the paths of objects in a project.\n\n**parameters**: \n- who_reference_me: A list of paths representing the objects that reference the current object.\n- reference_who: A list of paths representing the objects that the current object references.\n- doc_item_path: A string representing the path of the current object.\n\n**Code Description**: \nThe function starts by defining a nested function called \"tree\" which returns a defaultdict with the default value set to another defaultdict. This nested function is used to create a tree-like structure.\n\nNext, the function initializes an empty path_tree using the \"tree\" function.\n\nThe function then iterates over the paths in both \"who_reference_me\" and \"reference_who\" lists. For each path, it splits the path into individual parts using the os.sep separator (which represents the path separator for the current operating system). It then traverses the path_tree, creating nested defaultdicts for each part of the path.\n\nAfter that, the function processes the \"doc_item_path\" by splitting it into parts and replacing the last part with a modified version that has a star symbol (*) prepended to it. It then traverses the path_tree again, creating nested defaultdicts for each part of the modified \"doc_item_path\".\n\nFinally, the function defines another nested function called \"tree_to_string\" which takes a tree-like structure and converts it into a string representation. It recursively iterates over the tree, sorting the items and adding each key to the string representation. If the value associated with a key is another dictionary, it recursively calls the \"tree_to_string\" function to add its contents to the string representation.\n\nThe function returns the string representation of the path_tree.\n\n**Note**: \n- The function assumes that the paths in \"who_reference_me\", \"reference_who\", and \"doc_item_path\" are using the operating system's path separator.\n- The function uses the defaultdict class from the collections module to automatically create nested defaultdicts when accessing non-existent keys.\n\n**Output Example**: \n```\ntests\n    test_structure_tree.py\n        build_path_tree\n            ✳️build_path_tree\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 31,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [
        "**tree**: The function of this Function is to create a nested dictionary representing a tree structure.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThe `tree` function initializes and returns a nested dictionary using the `defaultdict` class from the `collections` module. The `defaultdict` class allows us to specify a default value for keys that have not been set yet. In this case, the default value is another `tree` object, which means that each key in the dictionary will have a nested dictionary as its value.\n\n**Note**: \n- This function assumes that the `defaultdict` class has been imported from the `collections` module.\n- The returned nested dictionary can be used to represent a tree structure, where each key represents a node and its value represents its children.\n\n**Output Example**: \nThe output of this function is a nested dictionary representing a tree structure."
      ],
      "code_start_line": 5,
      "code_end_line": 6,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_structure_tree.py/build_path_tree/tree_to_string"
      ]
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [
        "**tree_to_string**: The function of this Function is to convert a nested dictionary representing a tree structure into a string representation.\n\n**parameters**: \n- tree: A nested dictionary representing a tree structure.\n- indent: An integer representing the number of indentation levels.\n\n**Code Description**: \nThe `tree_to_string` function takes a nested dictionary `tree` and an integer `indent` as parameters. It initializes an empty string `s` to store the string representation of the tree. \n\nThe function iterates over the items in the `tree` dictionary, sorted by key. For each key-value pair, it appends the key to the string `s`, preceded by a number of indentation spaces determined by the `indent` parameter. \n\nIf the value associated with the key is another dictionary, the function recursively calls itself with the value as the new `tree` parameter and increments the `indent` by 1. This allows the function to handle nested levels of the tree structure.\n\nFinally, the function returns the string representation of the tree.\n\n**Note**: \n- This function assumes that the input `tree` is a nested dictionary representing a tree structure.\n- The `indent` parameter is optional and defaults to 0 if not provided.\n\n**Output Example**: \nIf the input `tree` is:\n```\n{\n    'A': {\n        'B': {\n            'C': {},\n            'D': {}\n        },\n        'E': {}\n    },\n    'F': {\n        'G': {}\n    }\n}\n```\nThe function will return the following string:\n```\nA\n    B\n        C\n        D\n    E\nF\n    G\n```"
      ],
      "code_start_line": 23,
      "code_end_line": 29,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_structure_tree.py/build_path_tree/tree"
      ],
      "reference_who": []
    }
  },
  "repo_agent/runner.py": {
    "need_to_generate": {
      "type": "FunctionDef",
      "name": "need_to_generate",
      "md_content": [
        "**need_to_generate**: The function of this Function is to determine whether a given `doc_item` should be generated or not based on its item type and the ignore list.\n\n**parameters**: \n- `doc_item: DocItem` - The `DocItem` object representing the item to be checked.\n- `ignore_list: List` - A list of file paths to be ignored.\n\n**Code Description**: \nThis function first retrieves the relative file path of the `doc_item` using the `get_full_name()` method. It then checks if the item type of the `doc_item` is either a file, directory, or repository. If it is any of these types, the function returns `False`, indicating that the item should not be generated.\n\nIf the item type is not one of the above, the function assigns the `father` attribute of the `doc_item` to `doc_item` itself. This is done to traverse up the hierarchy of the `doc_item` until a file is found. \n\nWhile traversing up, the function checks if the current `doc_item` is a file. If it is, it checks if the relative file path starts with any of the file paths in the `ignore_list`. If it does, the function returns `False`, indicating that the item should not be generated. If none of the file paths in the `ignore_list` match the relative file path, the function returns `True`, indicating that the item should be generated.\n\nIf no file is found while traversing up the hierarchy, the function returns `False`, indicating that the item should not be generated.\n\n**Note**: \n- This function assumes that the `doc_item` object has a `get_full_name()` method and an `item_type` attribute.\n- The `ignore_list` should contain file paths that are relative to the root of the project.\n- The function only checks if the relative file path starts with any of the file paths in the `ignore_list`. It does not check for exact matches.\n\n**Output Example**: \nIf the `doc_item` is a file and its relative file path does not start with any of the file paths in the `ignore_list`, the function will return `True`. Otherwise, it will return `False`."
      ],
      "code_start_line": 16,
      "code_end_line": 29,
      "parent": null,
      "params": [
        "doc_item",
        "ignore_list"
      ],
      "have_return": true,
      "code_content": "def need_to_generate(doc_item: DocItem, ignore_list: List) -> bool:\n    \"\"\"只生成item的，文件及更高粒度都跳过。另外如果属于一个blacklist的文件也跳过\"\"\"\n    rel_file_path = doc_item.get_full_name()\n    if doc_item.item_type in [DocItemType._file, DocItemType._dir, DocItemType._repo]:\n        return False\n    doc_item = doc_item.father\n    while doc_item:\n        if doc_item.item_type == DocItemType._file:\n            # 如果当前文件在忽略列表中，或者在忽略列表某个文件路径下，则跳过\n            if any(rel_file_path.startswith(ignore_item) for ignore_item in ignore_list):\n                return False\n            return True\n        doc_item = doc_item.father\n    return False\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/runner.py/Runner",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/markdown_refresh",
        "repo_agent/runner.py/Runner/run",
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/update_existing_item",
        "repo_agent/runner.py/Runner/update_object",
        "repo_agent/runner.py/Runner/get_new_objects",
        "repo_agent/runner.py/recursive_check",
        "repo_agent/file_handler.py/FileHandler",
        "repo_agent/file_handler.py/FileHandler/read_file",
        "repo_agent/file_handler.py/FileHandler/get_obj_code_info",
        "repo_agent/file_handler.py/FileHandler/write_file",
        "repo_agent/file_handler.py/FileHandler/get_modified_file_versions",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes",
        "repo_agent/file_handler.py/FileHandler/generate_file_structure",
        "repo_agent/file_handler.py/FileHandler/convert_to_markdown_file",
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/MetaInfo",
        "repo_agent/doc_meta_info.py/MetaInfo/init_from_project_path",
        "repo_agent/doc_meta_info.py/MetaInfo/from_checkpoint_path",
        "repo_agent/doc_meta_info.py/MetaInfo/checkpoint",
        "repo_agent/doc_meta_info.py/MetaInfo/load_task_list",
        "repo_agent/doc_meta_info.py/MetaInfo/print_task_list",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files",
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta",
        "repo_agent/chat_engine.py/ChatEngine",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc",
        "repo_agent/change_detector.py/ChangeDetector",
        "repo_agent/change_detector.py/ChangeDetector/get_file_diff",
        "repo_agent/change_detector.py/ChangeDetector/parse_diffs",
        "repo_agent/change_detector.py/ChangeDetector/identify_changes_in_structure",
        "repo_agent/change_detector.py/ChangeDetector/add_unstaged_files",
        "repo_agent/project_manager.py/ProjectManager",
        "repo_agent/project_manager.py/ProjectManager/find_all_referencer"
      ]
    },
    "Runner": {
      "type": "ClassDef",
      "name": "Runner",
      "md_content": [
        "**Runner**: The function of this Class is to manage the generation and update of documentation for the project. It contains methods to generate documentation for individual objects, detect changes in the project, update the documentation accordingly, and commit the changes to the repository.\n\n**Attributes**: \n- project_manager: An instance of the ProjectManager class that manages the project hierarchy and file operations.\n- change_detector: An instance of the ChangeDetector class that detects changes in the project files.\n- chat_engine: An instance of the ChatEngine class that interacts with a chatbot to generate documentation.\n- meta_info: An instance of the MetaInfo class that stores the metadata and status of the documentation.\n- CONFIG: A configuration dictionary that contains project-specific settings.\n\n**Code Description**:\n- The `__init__` method initializes the Runner class by creating instances of the ProjectManager, ChangeDetector, and ChatEngine classes. It also checks if the project hierarchy exists and initializes or loads the MetaInfo object accordingly.\n- The `get_all_pys` method is used to retrieve a list of all Python files in a given directory.\n- The `generate_doc_for_a_single_item` method generates documentation for a single object by interacting with the ChatEngine and updating the MetaInfo object.\n- The `first_generate` method generates documentation for all objects in the project hierarchy. It iterates through the topology list of objects, checks if they need to be generated, and calls the `generate_doc_for_a_single_item` method.\n- The `markdown_refresh` method updates the markdown files with the latest documentation information from the MetaInfo object.\n- The `git_commit` method commits the changes to the repository with a specified commit message.\n- The `run` method is the main function that runs the document update process. It first checks if the documentation is being generated for the first time or if there are changes in the project. It then calls the necessary methods to generate or update the documentation accordingly.\n- The `add_new_item` method adds new projects to the JSON file and generates corresponding documentation.\n- The `process_file_changes` method processes the changes in a file by identifying added and removed objects, updating the JSON file, and generating documentation for the changed objects.\n- The `update_existing_item` method updates existing projects by comparing the current and previous versions of the file, identifying added and removed objects, and updating the JSON file and documentation accordingly.\n- The `update_object` method generates documentation content and updates the corresponding field information of an object.\n- The `get_new_objects` method compares the current and previous versions of the file to identify added and deleted objects.\n\n**Note**: \n- The Runner class is responsible for managing the generation and update of documentation for the project.\n- It interacts with the ProjectManager, ChangeDetector, and ChatEngine classes to perform various tasks such as generating documentation, detecting changes, and updating the documentation.\n- The MetaInfo object stores the metadata and status of the documentation, including the document version and the status of each object.\n- The run method is the main entry point for running the document update process. It checks if the documentation is being generated for the first time or if there are changes in the project, and calls the necessary methods to generate or update the documentation accordingly.\n- The add_new_item and process_file_changes methods handle the addition and changes in project files, respectively, by updating the JSON file and generating documentation for the affected objects.\n- The update_existing_item method updates existing projects by comparing the current and previous versions of the file, identifying added and removed objects, and updating the JSON file and documentation accordingly.\n- The get_new_objects method compares the current and previous versions of the file to identify added and deleted objects.\n\n**Output Example**:\n```\nRunner:\n- project_manager: <ProjectManager object>\n- change_detector: <ChangeDetector object>\n- chat_engine: <ChatEngine object>\n- meta_info: <MetaInfo object>\n- CONFIG: {'repo_path': '/path/to/repo', 'project_hierarchy': 'project_hierarchy.json', ...}\n\nCode Description:\n- The Runner class manages the generation and update of documentation for the project.\n- It interacts with the ProjectManager, ChangeDetector, and ChatEngine classes to perform various tasks.\n- The run method is the main entry point for running the document update process.\n- The add_new_item and process_file_changes methods handle the addition and changes in project files.\n- The update_existing_item method updates existing projects.\n- The get_new_objects method identifies added and deleted objects.\n\nNote:\n- The Runner class is responsible for managing the generation and update of documentation for the project.\n- The MetaInfo object stores the metadata and status of the documentation.\n- The run method is the main entry point for running the document update process.\n- The add_new_item and process_file_changes methods handle the addition and changes in project files.\n- The update_existing_item method updates existing projects.\n- The get_new_objects method compares the current and previous versions of the file.\n```"
      ],
      "code_start_line": 31,
      "code_end_line": 404,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class Runner:\n    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n    \n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem, task_len: int, now_task_id: int):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        rel_file_path = doc_item.get_full_name()\n        if doc_item.item_status != DocItemStatus.doc_up_to_date:\n            logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...({now_task_id}/{task_len})\")\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            response_message = self.chat_engine.generate_doc(\n                doc_item = doc_item,\n                file_handler = file_handler,\n            )\n            doc_item.md_content.append(response_message.content)\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            self.markdown_refresh()\n        else:\n            logger.info(f\" 文档已生成，跳过：{doc_item.get_full_name()}\")\n        \n\n    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        topology_list = self.meta_info.get_topology() #将按照此顺序生成文档\n        topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        already_generated = 0\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            for k, doc_item in enumerate(topology_list): #按照拓扑顺序遍历所有的可能obj\n                self.generate_doc_for_a_single_item(doc_item,task_len=len(topology_list), now_task_id=k)\n                already_generated += 1\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"Generation Success: {len(topology_list)} doc generated\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {already_generated} docs are generated at this time\")\n\n    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        file_item_list = self.meta_info.get_all_files()\n        for file_item in tqdm(file_item_list):\n            def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                if doc_item.md_content != []:\n                    return True\n                for _,child in doc_item.children.items():\n                    if recursive_check(child):\n                        return True\n                return False\n            if recursive_check(file_item) == False:\n                logger.info(f\"跳过：{file_item.get_full_name()}\")\n                continue\n            rel_file_path = file_item.get_full_name()\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            # 对于每个文件，转换json内容到markdown\n            markdown = file_handler.convert_to_markdown_file(file_path=rel_file_path)\n            assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n            # 写入markdown内容到.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            \n        logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n\n    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n\n\n    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        task_list = self.meta_info.load_task_list()\n        task_list = [item for item in task_list if need_to_generate(item, ignore_list)]\n        self.meta_info.print_task_list(task_list)\n\n        for k, item in enumerate(task_list):\n            self.generate_doc_for_a_single_item(item,task_len=len(task_list), now_task_id=k)\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        \n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        markdown = markdown.replace('## ', '# ') # 将二级标题转换成一级标题\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n        \n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\") # 提交变更\n         \n\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n    \n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n\n\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n\n        return new_obj, del_obj\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of this Function is to initialize the Runner object.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \n- The function initializes the `project_manager` attribute of the Runner object by creating a new instance of the ProjectManager class. The `repo_path` and `project_hierarchy` parameters are passed to the ProjectManager constructor from the CONFIG dictionary.\n- The function initializes the `change_detector` attribute of the Runner object by creating a new instance of the ChangeDetector class. The `repo_path` parameter is passed to the ChangeDetector constructor from the CONFIG dictionary.\n- The function initializes the `chat_engine` attribute of the Runner object by creating a new instance of the ChatEngine class. The `CONFIG` parameter is passed to the ChatEngine constructor.\n- The function checks if the project hierarchy directory exists in the repository path. If it does not exist, it initializes the `meta_info` attribute of the Runner object by calling the `init_from_project_path` method of the MetaInfo class. The `repo_path` parameter is passed to the `init_from_project_path` method from the CONFIG dictionary. Then, it calls the `checkpoint` method of the MetaInfo class to create a checkpoint in the target directory path.\n- If the project hierarchy directory exists, it initializes the `meta_info` attribute of the Runner object by calling the `from_checkpoint_path` method of the MetaInfo class. The `repo_path` parameter is passed to the `from_checkpoint_path` method from the CONFIG dictionary. Then, it calls the `checkpoint` method of the MetaInfo class to create a checkpoint in the target directory path.\n\n**Note**: \n- The CONFIG dictionary is assumed to contain the necessary configuration values, including the repository path and project hierarchy.\n- The Runner object is responsible for managing the project, detecting changes, and handling chat interactions."
      ],
      "code_start_line": 32,
      "code_end_line": 42,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_all_pys": {
      "type": "FunctionDef",
      "name": "get_all_pys",
      "md_content": [
        "**get_all_pys**: The function of this Function is to retrieve all Python files within a given directory.\n\n**parameters**: \n- directory (str): The directory to search for Python files.\n\n**Code Description**: \nThis function takes a directory path as input and searches for all Python files within that directory and its subdirectories. It uses the `os.walk()` function to traverse through the directory tree and retrieve the file names. For each file, it checks if the file extension is '.py' using the `endswith()` method. If the file has a '.py' extension, its path is appended to the `python_files` list. Finally, the function returns the list of paths to all Python files found.\n\n**Note**: \n- The function assumes that the provided directory path is valid and exists.\n- The function does not perform any recursive search within symbolic links.\n- The function does not differentiate between regular Python files and files with the '.py' extension but are not valid Python files.\n\n**Output Example**: \nIf the function is called with the directory path '/path/to/directory', and the directory contains the following Python files:\n- /path/to/directory/file1.py\n- /path/to/directory/subdirectory/file2.py\n- /path/to/directory/subdirectory/file3.txt\n\nThe function will return the following list:\n['/path/to/directory/file1.py', '/path/to/directory/subdirectory/file2.py']"
      ],
      "code_start_line": 45,
      "code_end_line": 62,
      "parent": "Runner",
      "params": [
        "self",
        "directory"
      ],
      "have_return": true,
      "code_content": "    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "generate_doc_for_a_single_item": {
      "type": "FunctionDef",
      "name": "generate_doc_for_a_single_item",
      "md_content": [
        "**generate_doc_for_a_single_item**: The function of this Function is to generate documentation for a single object.\n\n**parameters**: \n- self: The instance of the Runner class.\n- doc_item: An object of the DocItem class representing the item for which the documentation needs to be generated.\n- task_len: An integer representing the total number of tasks.\n- now_task_id: An integer representing the current task ID.\n\n**Code Description**:\nThe function begins by obtaining the relative file path of the doc_item using the `get_full_name()` method. It then checks if the item's status is not equal to `DocItemStatus.doc_up_to_date`. If the status is not up to date, it logs a message indicating that the documentation for the object is being generated. \n\nNext, it creates an instance of the FileHandler class, passing the repository path and the relative file path as arguments. This FileHandler instance will be used to handle the file associated with the doc_item.\n\nThe function then calls the `generate_doc()` method of the ChatEngine class, passing the doc_item and file_handler as arguments. This method is responsible for generating the actual documentation for the object. The returned response message from the ChatEngine is appended to the `md_content` attribute of the doc_item.\n\nAfter generating the documentation, the item's status is updated to `DocItemStatus.doc_up_to_date`. The function then calls the `checkpoint()` method of the MetaInfo class, passing the target directory path as an argument. This method updates the checkpoint file to indicate that the documentation for the object has been generated.\n\nIf the item's status is already up to date, the function logs a message indicating that the documentation for the object has already been generated and skips the generation process.\n\n**Note**: \n- This function is called within the Runner class and is responsible for generating documentation for a single object.\n- The `generate_doc()` method of the ChatEngine class is used to generate the documentation for the object.\n- The `checkpoint()` method of the MetaInfo class is used to update the checkpoint file after generating the documentation."
      ],
      "code_start_line": 65,
      "code_end_line": 81,
      "parent": "Runner",
      "params": [
        "self",
        "doc_item",
        "task_len",
        "now_task_id"
      ],
      "have_return": false,
      "code_content": "    def generate_doc_for_a_single_item(self, doc_item: DocItem, task_len: int, now_task_id: int):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        rel_file_path = doc_item.get_full_name()\n        if doc_item.item_status != DocItemStatus.doc_up_to_date:\n            logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...({now_task_id}/{task_len})\")\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            response_message = self.chat_engine.generate_doc(\n                doc_item = doc_item,\n                file_handler = file_handler,\n            )\n            doc_item.md_content.append(response_message.content)\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            self.markdown_refresh()\n        else:\n            logger.info(f\" 文档已生成，跳过：{doc_item.get_full_name()}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "first_generate": {
      "type": "FunctionDef",
      "name": "first_generate",
      "md_content": [
        "**first_generate**: The function of this Function is to generate documentation for all objects in the project. It iterates through a list of objects in a specific order and generates documentation for each object. The generated documentation is then synchronized back to the file system. If an error occurs during the generation process, the function will automatically resume from where it left off in the next run.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \n- The function starts by logging a message indicating the start of the documentation generation process.\n- It retrieves a list of objects in a specific order from the meta_info object.\n- It filters the list of objects based on an ignore list provided in the project's configuration.\n- It initializes a counter to keep track of the number of objects that have been generated.\n- If the function is not already in the generation process, it sets the in_generation_process flag to True.\n- It then iterates through the filtered list of objects and calls the generate_doc_for_a_single_item function for each object. It also updates the already_generated counter.\n- After generating documentation for all objects, it updates the document_version in the meta_info object to the commit hash of the current state of the repository.\n- It sets the in_generation_process flag to False.\n- It creates a checkpoint of the meta_info object by saving it to a target directory path.\n- Finally, it logs a message indicating the success of the generation process and the number of documents generated.\n\n**Note**: \n- It is important to note that the generation process must be bound to a specific version of the code. Any modifications to the target repository code during the generation process will result in an inconsistent documentation state.\n- The function relies on the change_detector object to determine the current state of the repository and track any changes made to the code.\n- The function also depends on the meta_info object to retrieve the list of objects to generate documentation for and to store the generated documentation."
      ],
      "code_start_line": 84,
      "code_end_line": 111,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        topology_list = self.meta_info.get_topology() #将按照此顺序生成文档\n        topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        already_generated = 0\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            for k, doc_item in enumerate(topology_list): #按照拓扑顺序遍历所有的可能obj\n                self.generate_doc_for_a_single_item(doc_item,task_len=len(topology_list), now_task_id=k)\n                already_generated += 1\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"Generation Success: {len(topology_list)} doc generated\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {already_generated} docs are generated at this time\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "markdown_refresh": {
      "type": "FunctionDef",
      "name": "markdown_refresh",
      "md_content": [
        "**markdown_refresh**: The function of this Function is to write the latest document information into a markdown format folder, regardless of whether the markdown content has changed or not.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `markdown_refresh` function first retrieves a list of all file items using the `get_all_files` method from the `meta_info` object. It then iterates over each file item in the list. \n\nInside the loop, there is a nested function called `recursive_check` which takes a `doc_item` parameter of type `DocItem` and returns a boolean value. This function is used to check if a file contains any documentation. It recursively checks if the `md_content` attribute of the `doc_item` is not empty. If it is not empty, it returns `True`. If the `md_content` is empty, it iterates over the children of the `doc_item` and recursively calls the `recursive_check` function on each child. If any child returns `True`, it means that the file contains documentation, and the function returns `True`. If none of the children return `True`, it means that the file does not contain documentation, and the function returns `False`.\n\nIf the `recursive_check` function returns `False` for a file item, the loop continues to the next file item.\n\nIf the `recursive_check` function returns `True` for a file item, the relative file path is obtained using the `get_full_name` method of the file item. Then, a `FileHandler` object is created with the repository path and the relative file path. \n\nNext, the `convert_to_markdown_file` method of the `file_handler` object is called to convert the JSON content of the file to markdown format. The resulting markdown content is stored in the `markdown` variable.\n\nFinally, the `write_file` method of the `file_handler` object is called to write the markdown content to a `.md` file in the specified markdown documents folder. The file path is obtained by replacing the `.py` extension of the file with `.md`. \n\nAfter processing all file items, a log message is printed indicating that the markdown document has been refreshed at the specified markdown documents folder.\n\n**Note**: \n- This function assumes that the `meta_info` object has been properly initialized and contains the necessary file item information.\n- The `FileHandler` class and its methods are not provided in the given code, so their functionality and implementation details are not known.\n- The `CONFIG` variable is used to access the repository path and the markdown documents folder path, but its definition and values are not provided in the given code.\n\n**Output Example**: \nA possible appearance of the return value of this function is:\n```\nmarkdown document has been refreshed at /path/to/markdown_docs_folder\n```"
      ],
      "code_start_line": 113,
      "code_end_line": 136,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        file_item_list = self.meta_info.get_all_files()\n        for file_item in tqdm(file_item_list):\n            def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                if doc_item.md_content != []:\n                    return True\n                for _,child in doc_item.children.items():\n                    if recursive_check(child):\n                        return True\n                return False\n            if recursive_check(file_item) == False:\n                logger.info(f\"跳过：{file_item.get_full_name()}\")\n                continue\n            rel_file_path = file_item.get_full_name()\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            # 对于每个文件，转换json内容到markdown\n            markdown = file_handler.convert_to_markdown_file(file_path=rel_file_path)\n            assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n            # 写入markdown内容到.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            \n        logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "git_commit": {
      "type": "FunctionDef",
      "name": "git_commit",
      "md_content": [
        "**git_commit**: The function of this Function is to commit changes to the Git repository with a specified commit message.\n\n**parameters**: \n- commit_message: A string representing the commit message to be associated with the changes.\n\n**Code Description**: \nThe `git_commit` function uses the `subprocess.check_call` method to execute the Git command `git commit` with the specified commit message. The `--no-verify` option is used to bypass any pre-commit hooks that may be configured in the Git repository. The `-m` option is used to specify the commit message.\n\nIf the `git commit` command fails and raises a `subprocess.CalledProcessError`, the function catches the exception and prints an error message indicating that an error occurred while trying to commit.\n\n**Note**: \n- Make sure that the Git command-line tool is installed and accessible from the command prompt or terminal where the script is being executed.\n- Ensure that the script is being executed in a Git repository directory.\n- The commit message should be meaningful and descriptive to provide a clear understanding of the changes being committed."
      ],
      "code_start_line": 138,
      "code_end_line": 142,
      "parent": "Runner",
      "params": [
        "self",
        "commit_message"
      ],
      "have_return": false,
      "code_content": "    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "run": {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: The function of this Function is to run the document update process.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function is responsible for detecting the changed Python files, processing each file, and updating the documents accordingly. It first checks if the document version is empty. If it is empty, it calls the `first_generate()` method to generate the initial documents, checkpoints the target directory path, and returns. If the document version is not empty, it checks if the process is already in the generation process. If it is not, it starts detecting changes by merging the new project hierarchy with the old hierarchy. It handles various scenarios such as creating a new file, deleting a file or object, and changing reference relationships. After merging, it sets the `in_generation_process` flag to True.\n\nNext, it loads the task list and filters out the items that need to be generated based on the `ignore_list`. It then iterates over the task list and calls the `generate_doc_for_a_single_item()` method for each item. After generating the documents for all the items, it sets the `in_generation_process` flag to False and updates the document version to the latest commit hash. It then checkpoints the target directory path, flashes the reference relation, and logs that the documents have been forwarded to the latest version. Finally, it calls the `markdown_refresh()` method.\n\n**Note**: It is important to note that this Function relies on the `MetaInfo` class and other helper methods such as `first_generate()`, `load_doc_from_older_meta()`, `load_task_list()`, `print_task_list()`, `generate_doc_for_a_single_item()`, `checkpoint()`, and `markdown_refresh()`.\n\n**Output Example**: This Function does not return any value."
      ],
      "code_start_line": 145,
      "code_end_line": 195,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        task_list = self.meta_info.load_task_list()\n        task_list = [item for item in task_list if need_to_generate(item, ignore_list)]\n        self.meta_info.print_task_list(task_list)\n\n        for k, item in enumerate(task_list):\n            self.generate_doc_for_a_single_item(item,task_len=len(task_list), now_task_id=k)\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "add_new_item": {
      "type": "FunctionDef",
      "name": "add_new_item",
      "md_content": [
        "**add_new_item**: The function of this Function is to add new projects to the JSON file and generate corresponding documentation.\n\n**parameters**: \n- file_handler (FileHandler): The file handler object for reading and writing files.\n- json_data (dict): The JSON data storing the project structure information.\n\n**Code Description**: \nThe `add_new_item` function takes in a `file_handler` object and a `json_data` dictionary as parameters. It is responsible for adding new projects to the JSON file and generating the corresponding documentation.\n\nFirst, an empty dictionary `file_dict` is created. This dictionary will store the information of the new objects to be added to the JSON file.\n\nNext, a loop iterates over the functions and classes obtained from the `file_handler` object. For each object, the `file_handler` object is used to retrieve the code information. This code information is then passed to the `chat_engine` object's `generate_doc` method, along with the `file_handler` object. The `generate_doc` method returns a response message, from which the markdown content is extracted.\n\nThe markdown content is then added to the `code_info` dictionary, which contains the code information of the object. The `name` of the object is used as the key in the `file_dict` dictionary, and the `code_info` dictionary is added as the value.\n\nAfter iterating over all the objects, the `file_dict` dictionary is added to the `json_data` dictionary, with the `file_handler.file_path` as the key. This updates the JSON data with the new project structure information.\n\nThe updated JSON data is then written back to the JSON file.\n\nNext, the `file_handler` object is used to convert the JSON file content into markdown format. The converted markdown content is then written to a `.md` file in the Markdown Docs folder.\n\nFinally, log messages are generated to indicate that the structure information of the new file has been written to the JSON file, and the Markdown documentation for the new file has been generated.\n\n**Note**: \n- This function assumes that the `file_handler` object has methods for reading and writing files, as well as retrieving code information and converting it to markdown format.\n- The `chat_engine` object is assumed to have a `generate_doc` method that takes in code information and a file handler object, and returns a response message containing the generated documentation.\n- The `json_data` dictionary is assumed to have the file path as the key and a dictionary of objects as the value.\n- The `logger` object is assumed to have a `info` method for logging messages.\n- The `CONFIG` dictionary is assumed to contain a key `'Markdown_Docs_folder'` which specifies the folder for storing the generated Markdown documentation."
      ],
      "code_start_line": 198,
      "code_end_line": 229,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler",
        "json_data"
      ],
      "have_return": false,
      "code_content": "    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        markdown = markdown.replace('## ', '# ') # 将二级标题转换成一级标题\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "process_file_changes": {
      "type": "FunctionDef",
      "name": "process_file_changes",
      "md_content": [
        "**process_file_changes**: The function of this Function is to process changed files according to the absolute file path, including new files and existing files. It is called in the loop of detected changed files.\n\n**parameters**: \n- repo_path (str): The path to the repository.\n- file_path (str): The relative path to the file.\n- is_new_file (bool): Indicates whether the file is new or not.\n\n**Code Description**: \n- The function first creates a FileHandler object, passing the repository path and file path as parameters. This object will be used to perform operations on the changed file.\n- It then reads the content of the file using the FileHandler's `read_file()` method and stores it in the `source_code` variable.\n- The function calls the `parse_diffs()` method of the `change_detector` object (an instance of the ChangeDetector class) to get the changed lines in the file. It passes the result of the `get_file_diff()` method (which takes the file path and the `is_new_file` flag as parameters) as an argument.\n- Next, the function calls the `identify_changes_in_structure()` method of the `change_detector` object to identify the changes in the file's structure. It passes the `changed_lines` and the result of the `get_functions_and_classes()` method of the `file_handler` object (which takes the `source_code` as a parameter) as arguments. The result is stored in the `changes_in_pyfile` variable.\n- The function logs the detected changes using the `logger.info()` method.\n- It then opens the `project_hierarchy.json` file and loads its content into the `json_data` variable.\n- If the file path is found in the `json_data`, the function updates the corresponding item in the `json_data` using the `update_existing_item()` method, passing the `file_handler` and `changes_in_pyfile` as parameters.\n- The function writes the updated `json_data` back to the `project_hierarchy.json` file.\n- It logs that the json structure information of the file has been updated.\n- The function converts the changed part of the json file to markdown content using the `convert_to_markdown_file()` method of the `file_handler` object, passing the `file_path` as a parameter. The result is stored in the `markdown` variable.\n- It writes the markdown content to a `.md` file with the same name as the `.py` file, but with the extension changed, using the `write_file()` method of the `file_handler` object.\n- The function logs that the markdown document of the file has been updated.\n- If the file path is not found in the `json_data`, the function calls the `add_new_item()` method, passing the `file_handler` and `json_data` as parameters.\n- The function calls the `add_unstaged_files()` method of the `change_detector` object to add the updated markdown files to the staging area.\n- If there are files added to the staging area, the function logs the files that have been added.\n\n**Note**: \n- This function is called in the loop of detected changed files, so it will be executed multiple times for different files.\n- The function relies on the `FileHandler`, `ChangeDetector`, and `ProjectManager` classes to perform various operations on the files and project hierarchy.\n- The function updates the project hierarchy JSON file with the changes detected in the file's structure.\n- If the file is new, it adds a new item to the project hierarchy JSON file.\n- It converts the changed part of the JSON file to a Markdown document and saves it as a `.md` file.\n- The updated Markdown files are added to the staging area using Git commands."
      ],
      "code_start_line": 232,
      "code_end_line": 280,
      "parent": "Runner",
      "params": [
        "self",
        "repo_path",
        "file_path",
        "is_new_file"
      ],
      "have_return": false,
      "code_content": "    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "update_existing_item": {
      "type": "FunctionDef",
      "name": "update_existing_item",
      "md_content": [
        "**update_existing_item**: The function of this Function is to update existing projects by modifying the file structure information dictionary based on the changes made in the file.\n\n**parameters**: \n- file_dict (dict): A dictionary containing the file structure information.\n- file_handler (FileHandler): The file handler object.\n- changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n**Code Description**: \nThe function first calls the `get_new_objects` method to get the new and deleted objects in the file. It then iterates over the deleted objects and removes them from the `file_dict` dictionary. \n\nNext, it creates an empty list called `referencer_list` to store information about the objects that reference the added objects. \n\nThe function generates the current file structure information by calling the `generate_file_structure` method of the `file_handler` object. It creates a dictionary called `current_info_dict` to store the current objects' information using their names as keys.\n\nThe function then updates the global file structure information in the `file_dict` dictionary. For each object in the `current_info_dict`, if the object exists in the `file_dict`, its information is updated. If the object does not exist in the `file_dict`, it is added to the dictionary.\n\nNext, the function iterates over the added objects in the `changes_in_pyfile` dictionary. For each added object, it searches for the object's information in the `current_objects` dictionary. If a match is found, it calls the `find_all_referencer` method of the `project_manager` object to get a list of all the objects that reference the added object. It then adds the object's name and the referencer list to the `referencer_list`.\n\nThe function uses a `ThreadPoolExecutor` to concurrently update the objects in the `file_dict` dictionary. For each added object, it retrieves the corresponding referencer list from the `referencer_list` and submits a task to the executor to update the object using the `update_object` method. \n\nFinally, the function returns the updated `file_dict` dictionary.\n\n**Note**: \n- The function assumes that the `get_new_objects`, `generate_file_structure`, and `find_all_referencer` methods are defined in the respective classes.\n- The function uses a thread pool with a maximum of 5 workers to update the objects concurrently.\n\n**Output Example**: \n```python\n{\n    \"object1\": {\n        \"type\": \"class\",\n        \"code_start_line\": 10,\n        \"code_end_line\": 20,\n        \"parent\": \"module1\",\n        \"name_column\": 5\n    },\n    \"object2\": {\n        \"type\": \"function\",\n        \"code_start_line\": 30,\n        \"code_end_line\": 40,\n        \"parent\": \"module1\",\n        \"name_column\": 10\n    },\n    ...\n}\n```"
      ],
      "code_start_line": 286,
      "code_end_line": 357,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "changes_in_pyfile"
      ],
      "have_return": true,
      "code_content": "    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "update_object": {
      "type": "FunctionDef",
      "name": "update_object",
      "md_content": [
        "**update_object**: The function of this Function is to generate documentation content and update the corresponding field information of the object.\n\n**parameters**: \n- file_dict (dict): A dictionary containing the old object information.\n- file_handler: The file handler.\n- obj_name (str): The object name.\n- obj_referencer_list (list): The list of object referencers.\n\n**Code Description**: \nThe `update_object` function takes in a dictionary `file_dict` containing the old object information, a `file_handler`, the name of the object `obj_name`, and a list of object referencers `obj_referencer_list`. \n\nThe function first checks if the `obj_name` exists in the `file_dict`. If it does, it retrieves the object from the dictionary and assigns it to the variable `obj`. \n\nNext, it calls the `generate_doc` method of the `chat_engine` object, passing in the `obj`, `file_handler`, and `obj_referencer_list` as arguments. The `generate_doc` method is responsible for generating the documentation content based on the object and its referencers.\n\nThe response message returned by the `generate_doc` method is then assigned to the `md_content` field of the `obj` dictionary.\n\n**Note**: \n- This function is used to update the documentation content and field information of an object based on its existing information and referencers.\n- The `file_dict` parameter should be a dictionary containing the old object information, where the keys are the object names and the values are dictionaries representing the object information.\n- The `file_handler` parameter should be an instance of the file handler class.\n- The `obj_name` parameter should be a string representing the name of the object to be updated.\n- The `obj_referencer_list` parameter should be a list of object referencers."
      ],
      "code_start_line": 360,
      "code_end_line": 376,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "obj_name",
        "obj_referencer_list"
      ],
      "have_return": false,
      "code_content": "    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_new_objects": {
      "type": "FunctionDef",
      "name": "get_new_objects",
      "md_content": [
        "**get_new_objects**: The function of this Function is to compare the current version and the previous version of a .py file and retrieve the added and deleted objects.\n\n**parameters**: \n- file_handler (FileHandler): The file handler object used to retrieve the modified file versions.\n\n**Code Description**:\nThe function first retrieves the current version and the previous version of the .py file using the `get_modified_file_versions()` method of the `file_handler` object. \n\nThen, it uses the `get_functions_and_classes()` method of the `file_handler` object to parse the current and previous versions of the .py file and retrieve the functions and classes.\n\nThe function creates sets of the names of the functions and classes in the current and previous versions.\n\nIt then calculates the added objects by subtracting the previous objects from the current objects, and calculates the deleted objects by subtracting the current objects from the previous objects.\n\nFinally, the function returns a tuple containing the added and deleted objects.\n\n**Note**: \n- The `file_handler` object must be an instance of the `FileHandler` class.\n- The `get_modified_file_versions()` method of the `file_handler` object should return the current and previous versions of the .py file.\n- The `get_functions_and_classes()` method of the `file_handler` object should return a list of tuples, where each tuple contains the type and name of a function or class.\n\n**Output Example**:\nnew_obj: ['add_context_stack', '__init__']\ndel_obj: []"
      ],
      "code_start_line": 380,
      "code_end_line": 404,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n\n        return new_obj, del_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "recursive_check": {
      "type": "FunctionDef",
      "name": "recursive_check",
      "md_content": [
        "**recursive_check**: The function of this Function is to check whether a file contains any documentation by recursively checking its children.\n\n**parameters**: \n- doc_item: A DocItem object representing a file or directory.\n- ignore_list: A list of file paths to be ignored.\n\n**Code Description**: \nThe function starts by checking if the `md_content` attribute of the `doc_item` is not empty. If it is not empty, it means that the file contains documentation and the function returns True.\n\nIf the `md_content` attribute is empty, the function iterates over the children of the `doc_item`. For each child, it recursively calls the `recursive_check` function. If any of the children return True, it means that the child or its descendants contain documentation, and the function returns True.\n\nIf none of the children or their descendants contain documentation, the function returns False.\n\n**Note**: \n- This function assumes that the `doc_item` object has a `md_content` attribute that represents the content of the file in Markdown format.\n- The function relies on the `children` attribute of the `doc_item` object to iterate over its children.\n- The function does not handle cases where the `doc_item` object is not a file or directory.\n\n**Output Example**: \nIf the `doc_item` object represents a file that contains documentation, the function will return True. Otherwise, it will return False."
      ],
      "code_start_line": 118,
      "code_end_line": 124,
      "parent": null,
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "            def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                if doc_item.md_content != []:\n                    return True\n                for _,child in doc_item.children.items():\n                    if recursive_check(child):\n                        return True\n                return False\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    }
  },
  "repo_agent/file_handler.py": {
    "FileHandler": {
      "type": "ClassDef",
      "name": "FileHandler",
      "md_content": [
        "**FileHandler**: The FileHandler class is responsible for handling file-related operations within the repository. It provides methods for reading and writing file content, retrieving code information for a given object, getting the current and previous versions of a modified file, and generating the file structure of the repository.\n\n**Attributes**:\n- `repo_path`: The path to the repository.\n- `file_path`: The relative path of the file.\n\n**Code Description**:\n- The `__init__` method initializes a FileHandler object with the repository path and file path.\n- The `read_file` method reads the content of the file specified by the file path.\n- The `get_obj_code_info` method retrieves the code information for a given object, such as its type, name, start and end line numbers, parent, and parameters.\n- The `write_file` method writes content to a file specified by the file path.\n- The `get_modified_file_versions` method gets the current and previous versions of the modified file.\n- The `get_end_lineno` method retrieves the end line number of a given node in the Abstract Syntax Tree (AST).\n- The `add_parent_references` method adds a parent reference to each node in the AST.\n- The `get_functions_and_classes` method retrieves all functions and classes in the code content, along with their parameters and hierarchical relationships.\n- The `generate_file_structure` method generates the file structure for a given file path.\n- The `generate_overall_structure` method generates the overall structure of the repository.\n- The `convert_to_markdown_file` method converts the content of a file to markdown format.\n- The `convert_all_to_markdown_files_from_json` method converts all files to markdown format based on the JSON data.\n\n**Note**: The file path provided to the FileHandler methods is relative to the repository root directory.\n\n**Output Example**:\n{\n    \"type\": \"function\",\n    \"name\": \"read_file\",\n    \"md_content\": [],\n    \"code_start_line\": 10,\n    \"code_end_line\": 20,\n    \"parent\": None,\n    \"params\": []\n}"
      ],
      "code_start_line": 11,
      "code_end_line": 320,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class FileHandler:\n    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n\n    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n\n    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n\n\n    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n        \n    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parent_name = node.parent.name if 'name' in dir(node.parent) else None\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                )\n        return functions_and_classes\n        \n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n    \n\n    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        for not_ignored_files in gitignore_checker.check_files_and_folders():\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n        return repo_structure\n    \n\n    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef'] and obj['params']:\n                params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n\n    def convert_all_to_markdown_files_from_json(self):\n        \"\"\"\n        Converts all files to markdown format based on the JSON data.\n\n        Reads the project hierarchy from a JSON file, checks if the Markdown_docs folder exists,\n        creates it if it doesn't, and then iterates through each file in the JSON data.\n        For each file, it converts the file to markdown format and writes it to the Markdown_docs folder.\n\n        Args:\n            self (object): The file_handler object.\n\n        Returns:\n            None\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        # 检查根目录是否存在Markdown_docs文件夹，如果不存在则创建\n        markdown_docs_path = os.path.join(self.repo_path, CONFIG['Markdown_Docs_folder'])\n        if not os.path.exists(markdown_docs_path):\n            os.mkdir(markdown_docs_path)\n\n        # 遍历json_data[\"files\"]列表中的每个字典\n        for rel_file_path, file_dict in json_data.items():\n            md_path = os.path.join(markdown_docs_path, rel_file_path.replace('.py', '.md'))\n            markdown = self.convert_to_markdown_file(rel_file_path)\n            \n            # 检查目录是否存在，如果不存在，就创建它\n            os.makedirs(os.path.dirname(md_path), exist_ok=True)\n\n            # 将markdown文档写入到Markdown_docs文件夹中\n            with open(md_path, 'w', encoding='utf-8') as f:\n                f.write(markdown)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py",
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "repo_agent/doc_meta_info.py",
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of this Function is to initialize the FileHandler object with the provided repo_path and file_path.\n\n**parameters**: \n- repo_path (str): The path to the repository.\n- file_path (str): The relative path of the file.\n\n**Code Description**: \nThe `__init__` function is the constructor of the FileHandler class. It takes in two parameters, `repo_path` and `file_path`, and initializes the corresponding attributes of the FileHandler object.\n\nThe `repo_path` parameter represents the path to the repository. It is assigned to the `self.repo_path` attribute.\n\nThe `file_path` parameter represents the relative path of the file. It is assigned to the `self.file_path` attribute.\n\n**Note**: \n- The `repo_path` and `file_path` parameters should be valid paths.\n- The `self.repo_path` attribute is used to store the path to the repository.\n- The `self.file_path` attribute is used to store the relative path of the file.\n\n**Output Example**: \nIf the `repo_path` is \"/path/to/repository\" and the `file_path` is \"folder/file.py\", the `self.repo_path` attribute will be \"/path/to/repository\" and the `self.file_path` attribute will be \"folder/file.py\"."
      ],
      "code_start_line": 12,
      "code_end_line": 15,
      "parent": "FileHandler",
      "params": [
        "self",
        "repo_path",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/get_obj_code_info",
        "repo_agent/file_handler.py/FileHandler/get_end_lineno",
        "repo_agent/file_handler.py/FileHandler/add_parent_references",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes",
        "repo_agent/file_handler.py/FileHandler/generate_file_structure",
        "repo_agent/file_handler.py/FileHandler/convert_to_markdown_file",
        "repo_agent/file_handler.py/FileHandler/convert_all_to_markdown_files_from_json",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/check_files_and_folders"
      ]
    },
    "read_file": {
      "type": "FunctionDef",
      "name": "read_file",
      "md_content": [
        "**read_file**: The function of this Function is to read the content of a file.\n\n**Parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function first constructs the absolute file path by joining the repository path and the file path. It then opens the file using the `open()` function with the mode set to read ('r') and the encoding set to 'utf-8'. The content of the file is read using the `read()` method and stored in the `content` variable. Finally, the content of the file is returned.\n\n**Note**: This Function assumes that the `repo_path` and `file_path` attributes have been set correctly before calling this function. It is important to ensure that the file exists at the specified path and that the necessary permissions are granted to read the file.\n\n**Output Example**: The content of the current changed file."
      ],
      "code_start_line": 17,
      "code_end_line": 27,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_obj_code_info": {
      "type": "FunctionDef",
      "name": "get_obj_code_info",
      "md_content": [
        "**get_obj_code_info**: The function of this Function is to retrieve code information for a given object.\n\n**parameters**: \n- code_type (str): The type of the code.\n- code_name (str): The name of the code.\n- start_line (int): The starting line number of the code.\n- end_line (int): The ending line number of the code.\n- parent (str): The parent of the code.\n- file_path (str, optional): The file path. Defaults to None.\n\n**Code Description**: \nThe `get_obj_code_info` function takes in several parameters including the type of the code, the name of the code, the starting and ending line numbers of the code, the parent of the code, and an optional file path. It returns a dictionary containing the code information.\n\nThe function first initializes an empty dictionary called `code_info`. It then assigns the provided parameters to the corresponding keys in the `code_info` dictionary. \n\nNext, the function opens the code file specified by the `file_path` parameter (or the default file path if `file_path` is None) using the `open` function. It reads all the lines of the code file and stores them in the `lines` variable. \n\nThe function extracts the code content between the starting and ending line numbers using list slicing and joins the lines together into a single string. This code content is assigned to the `code_content` variable.\n\nThe function also determines the position of the code name in the first line of the code using the `find` method. If the code content contains the word \"return\", the `have_return` variable is set to True, otherwise it is set to False.\n\nFinally, the function assigns the `have_return` variable, the code content, and the position of the code name to the corresponding keys in the `code_info` dictionary.\n\nThe `code_info` dictionary is then returned as the output of the function.\n\n**Note**: \n- The `file_path` parameter is optional. If not provided, the function uses the default file path stored in the `self.file_path` attribute.\n- The function assumes that the code file is encoded in UTF-8.\n\n**Output Example**: \n{\n  'type': 'function',\n  'name': 'get_obj_code_info',\n  'md_content': [],\n  'code_start_line': 10,\n  'code_end_line': 20,\n  'parent': 'FileHandler',\n  'params': ['code_type', 'code_name', 'start_line', 'end_line', 'parent', 'file_path'],\n  'have_return': True,\n  'code_content': 'def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\\n        \"\"\"\\n        Get the code information for a given object.\\n\\n        Args:\\n            code_type (str): The type of the code.\\n            code_name (str): The name of the code.\\n            start_line (int): The starting line number of the code.\\n            end_line (int): The ending line number of the code.\\n            parent (str): The parent of the code.\\n            file_path (str, optional): The file path. Defaults to None.\\n\\n        Returns:\\n            dict: A dictionary containing the code information.\\n        \"\"\"\\n\\n        code_info = {}\\n        code_info[\\'type\\'] = code_type\\n        code_info[\\'name\\'] = code_name\\n        code_info[\\'md_content\\'] = []\\n        code_info[\\'code_start_line\\'] = start_line\\n        code_info[\\'code_end_line\\'] = end_line\\n        code_info[\\'parent\\'] = parent\\n        code_info[\\'params\\'] = params\\n\\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), \\'r\\', encoding=\\'utf-8\\') as code_file:\\n            lines = code_file.readlines()\\n            code_content = \\'\\'.join(lines[start_line-1:end_line])\\n            # 获取对象名称在第一行代码中的位置\\n            name_column = lines[start_line-1].find(code_name)\\n            # 判断代码中是否有return字样\\n            if \\'return\\' in code_content:\\n                have_return = True\\n            else:  \\n                have_return = False\\n            \\n            code_info[\\'have_return\\'] = have_return\\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\\n            # code_info[\\'code_content\\'] = json.dumps(code_content)[1:-1]\\n            code_info[\\'code_content\\'] = code_content\\n            code_info[\\'name_column\\'] = name_column\\n                \\n        return code_info\\n'\n}"
      ],
      "code_start_line": 29,
      "code_end_line": 71,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_type",
        "code_name",
        "start_line",
        "end_line",
        "parent",
        "params",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "write_file": {
      "type": "FunctionDef",
      "name": "write_file",
      "md_content": [
        "**write_file**: The function of this Function is to write content to a file.\n\n**parameters**: \n- file_path (str): The relative path of the file.\n- content (str): The content to be written to the file.\n\n**Code Description**: \nThe `write_file` function takes in a `file_path` and `content` as parameters. It first checks if the `file_path` starts with a forward slash (\"/\"). If it does, it removes the leading slash. \n\nThen, it creates the absolute file path by joining the `repo_path` (which is the base path of the repository) with the `file_path`. It also creates any necessary directories in the file path using `os.makedirs` with the `exist_ok=True` parameter, which ensures that the directories are created if they don't already exist.\n\nFinally, it opens the file at the absolute file path in write mode ('w') with the encoding set to 'utf-8'. It then writes the `content` to the file using the `file.write` method.\n\n**Note**: \n- The `file_path` parameter should be a relative path, and if it starts with a forward slash (\"/\"), it will be removed before creating the absolute file path.\n- The `content` parameter should be a string representing the content to be written to the file.\n- The function assumes that the `repo_path` attribute is defined and represents the base path of the repository."
      ],
      "code_start_line": 73,
      "code_end_line": 89,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path",
        "content"
      ],
      "have_return": false,
      "code_content": "    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_modified_file_versions": {
      "type": "FunctionDef",
      "name": "get_modified_file_versions",
      "md_content": [
        "**get_modified_file_versions**: The function of this Function is to retrieve the current and previous versions of a modified file.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThis Function first initializes a git repository object using the `git.Repo` class, passing in the `repo_path` attribute of the current object as the repository path. \n\nNext, it reads the current version of the file by opening the file in the current working directory using the `open` function and reading its contents. The file path is obtained by joining the `repo_path` and `file_path` attributes of the current object.\n\nThen, it retrieves the previous version of the file by getting the file version from the last commit in the repository. It does this by calling the `iter_commits` method on the repository object, passing in the `file_path` attribute of the current object and setting `max_count` to 1 to limit the number of commits to retrieve. The method returns a list of commits, and if the list is not empty, it retrieves the first commit and attempts to get the previous version of the file. If the file is not found in the commit's tree, it sets `previous_version` to `None`.\n\nFinally, it returns a tuple containing the current version and the previous version of the file.\n\n**Note**: \n- This Function assumes that the `git` module and the `os` module have been imported.\n- The `repo_path` and `file_path` attributes of the current object should be set before calling this Function.\n- The file should be encoded in UTF-8.\n\n**Output Example**:\n```\n(\"Current file version\", \"Previous file version\")\n```"
      ],
      "code_start_line": 92,
      "code_end_line": 116,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_end_lineno": {
      "type": "FunctionDef",
      "name": "get_end_lineno",
      "md_content": [
        "**get_end_lineno**: The function of this Function is to retrieve the end line number of a given node in the code.\n\n**parameters**: \n- node: The node for which to find the end line number.\n\n**Code Description**: \nThis function takes a node as input and returns the end line number of that node. It first checks if the node has a 'lineno' attribute. If not, it means that the node does not have a line number, so it returns -1 to indicate this. \n\nIf the node does have a line number, the function initializes the 'end_lineno' variable with the value of the node's line number. Then, it iterates over all the child nodes of the given node using the 'ast.iter_child_nodes()' function. For each child node, it recursively calls the 'get_end_lineno()' function to get its end line number. If the child node has a valid end line number (greater than -1), the 'end_lineno' variable is updated to the maximum value between its current value and the child node's end line number.\n\nFinally, the function returns the 'end_lineno' value, which represents the end line number of the given node.\n\n**Note**: \n- This function relies on the 'ast' module, which provides a way to parse Python source code and analyze its structure.\n- The function assumes that the 'node' parameter is a valid node object that conforms to the structure defined by the 'ast' module.\n\n**Output Example**: \nIf the given node has an end line number of 10, the function will return 10. If the node does not have a line number, the function will return -1."
      ],
      "code_start_line": 118,
      "code_end_line": 136,
      "parent": "FileHandler",
      "params": [
        "self",
        "node"
      ],
      "have_return": true,
      "code_content": "    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "add_parent_references": {
      "type": "FunctionDef",
      "name": "add_parent_references",
      "md_content": [
        "**add_parent_references**: The function of this Function is to add a parent reference to each node in the Abstract Syntax Tree (AST).\n\n**parameters**: \n- node: The current node in the AST.\n- parent: The parent node of the current node. It is an optional parameter and defaults to None.\n\n**Code Description**: \nThe `add_parent_references` function is a recursive function that traverses the AST and adds a parent reference to each node. It takes the current node as an argument and iterates over its child nodes using the `ast.iter_child_nodes` function. For each child node, it sets the parent reference to the current node by assigning `node` to `child.parent`. Then, it recursively calls the `add_parent_references` function with the child node as the new current node and the current node as the parent.\n\n**Note**: \n- This function is useful when working with ASTs and analyzing the relationships between nodes. By adding parent references, it becomes easier to navigate and manipulate the AST.\n- It is important to note that the `add_parent_references` function modifies the AST in-place and does not return anything."
      ],
      "code_start_line": 138,
      "code_end_line": 150,
      "parent": "FileHandler",
      "params": [
        "self",
        "node",
        "parent"
      ],
      "have_return": false,
      "code_content": "    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "get_functions_and_classes": {
      "type": "FunctionDef",
      "name": "get_functions_and_classes",
      "md_content": [
        "**get_functions_and_classes**: The function of this Function is to retrieve all functions, classes, their parameters (if any), and their hierarchical relationships from the given code content.\n\n**parameters**: \n- code_content: The code content of the whole file to be parsed.\n\n**Code Description**: \nThis function takes the code content as input and parses it using the `ast` module. It then traverses the abstract syntax tree (AST) and identifies all function definitions (`FunctionDef`), class definitions (`ClassDef`), and async function definitions (`AsyncFunctionDef`). For each identified node, it extracts the name, starting line number, ending line number, parent node name (if applicable), and parameters (if any). The extracted information is stored in a list of tuples.\n\n**Note**: \n- The `ast` module is used to parse the code content and extract the required information.\n- The `add_parent_references` method is called internally to add parent references to the AST nodes.\n- The `get_end_lineno` method is used to determine the ending line number of a node.\n- The returned list of tuples contains the following information for each function or class: type of the node, name of the node, starting line number, ending line number, name of the parent node (if applicable), and a list of parameters.\n\n**Output Example**: \n[('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]"
      ],
      "code_start_line": 152,
      "code_end_line": 177,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_content"
      ],
      "have_return": true,
      "code_content": "    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parent_name = node.parent.name if 'name' in dir(node.parent) else None\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                )\n        return functions_and_classes\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "generate_file_structure": {
      "type": "FunctionDef",
      "name": "generate_file_structure",
      "md_content": [
        "**generate_file_structure**: The function of this Function is to generate the file structure for a given file path.\n\n**parameters**: \n- file_path (str): The relative path of the file.\n\n**Code Description**:\nThe `generate_file_structure` function takes a `file_path` as input and generates the file structure for the given file. It first opens the file using the `open` function and reads its content. Then, it calls the `get_functions_and_classes` method to extract the functions and classes from the file content. \n\nFor each structure (function or class) found in the file, the function creates a dictionary entry in the `file_objects` dictionary. The dictionary key is the name of the structure, and the value is the code information obtained from the `get_obj_code_info` method. The code information includes the structure type (function or class), name, start line, end line, parent, and parameters.\n\nFinally, the function returns the `file_objects` dictionary containing the file path and the generated file structure.\n\n**Note**: \n- The `file_path` parameter should be a relative path to the file.\n- The `get_functions_and_classes` and `get_obj_code_info` methods are assumed to be defined elsewhere in the code.\n\n**Output Example**:\n{\n    \"function_name\": {\n        \"type\": \"function\",\n        \"start_line\": 10,\n        ··· ···\n        \"end_line\": 20,\n        \"parent\": \"class_name\"\n    },\n    \"class_name\": {\n        \"type\": \"class\",\n        \"start_line\": 5,\n        ··· ···\n        \"end_line\": 25,\n        \"parent\": None\n    }\n}"
      ],
      "code_start_line": 179,
      "code_end_line": 216,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "generate_overall_structure": {
      "type": "FunctionDef",
      "name": "generate_overall_structure",
      "md_content": [
        "**generate_overall_structure**: The function of this Function is to generate the overall structure of the repository.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThis function starts by initializing an empty dictionary called `repo_structure`. It then creates an instance of the `GitignoreChecker` class, passing the repository path and the path to the `.gitignore` file as arguments. The `GitignoreChecker` class is responsible for checking which files and folders are not ignored by the `.gitignore` file.\n\nNext, the function iterates over the files and folders that are not ignored by the `.gitignore` file. For each file or folder, it calls the `generate_file_structure` method to generate its structure. If an error occurs during the generation of the file structure, an error message is printed and the loop continues to the next file or folder.\n\nFinally, the function returns the `repo_structure` dictionary, which represents the overall structure of the repository.\n\n**Note**: \n- This function assumes that the `generate_file_structure` method is defined and implemented elsewhere in the codebase.\n- The `GitignoreChecker` class and the `generate_file_structure` method are not defined in the given code snippet. They are referenced in the code and should be implemented separately.\n\n**Output Example**: \n{\n    \"file1.txt\": {\n        \"subfile1.txt\": {},\n        \"subfile2.txt\": {}\n    },\n    \"file2.txt\": {},\n    \"folder1\": {\n        \"subfile3.txt\": {},\n        \"subfolder1\": {\n            \"subfile4.txt\": {}\n        }\n    }\n}"
      ],
      "code_start_line": 219,
      "code_end_line": 235,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        for not_ignored_files in gitignore_checker.check_files_and_folders():\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n        return repo_structure\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "convert_to_markdown_file": {
      "type": "FunctionDef",
      "name": "convert_to_markdown_file",
      "md_content": [
        "**convert_to_markdown_file**: The function of this Function is to convert the content of a file to markdown format.\n\n**parameters**: \n- file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n**Code Description**: \nThe function first opens the project_hierarchy.json file and reads its content using the 'utf-8' encoding. It then loads the JSON data into the json_data variable.\n\nIf the file_path parameter is not provided, the function assigns the value of self.file_path to file_path.\n\nThe function searches for the file object in the json_data dictionary that matches the file_path. If no file object is found, it raises a ValueError with a message indicating that no file object was found for the specified file path in project_hierarchy.json.\n\nThe function initializes an empty string variable called markdown. It also creates an empty dictionary called parent_dict to store the parent-child relationship between objects.\n\nThe function sorts the values of the file_dict dictionary based on the 'code_start_line' key in ascending order and assigns the sorted objects to the objects variable.\n\nThe function then iterates over each object in the objects list. For each object, it checks if the 'parent' key is not None. If it is not None, it adds the parent-child relationship to the parent_dict dictionary.\n\nThe function initializes a variable called current_parent to None. It then iterates over each object in the objects list again. For each object, it calculates the level of the object by counting the number of times it needs to traverse the parent_dict dictionary. If the level is equal to 1 and current_parent is not None, it adds a horizontal rule to the markdown string.\n\nThe function updates the current_parent variable with the name of the current object. If the object is a function definition and has parameters, it adds the parameters to the markdown string.\n\nThe function appends the last element of the 'md_content' list of the object to the markdown string. If the 'md_content' list is empty, it appends an empty string.\n\nFinally, the function adds a horizontal rule to the markdown string.\n\n**Note**: \n- The function requires the project_hierarchy.json file to be present and properly formatted.\n- If the file_path parameter is not provided, the function uses the default file path stored in self.file_path.\n- The function assumes that the file object for the specified file path exists in the project_hierarchy.json file. If it does not exist, a ValueError is raised."
      ],
      "code_start_line": 238,
      "code_end_line": 286,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef'] and obj['params']:\n                params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "convert_all_to_markdown_files_from_json": {
      "type": "FunctionDef",
      "name": "convert_all_to_markdown_files_from_json",
      "md_content": [
        "**convert_all_to_markdown_files_from_json**: The function of this Function is to convert all files to markdown format based on the JSON data.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: This function reads the project hierarchy from a JSON file and checks if the \"Markdown_docs\" folder exists. If the folder does not exist, it creates the folder. Then, it iterates through each file in the JSON data. For each file, it converts the file to markdown format and writes it to the \"Markdown_docs\" folder.\n\nFirst, the function opens the JSON file and loads the data into the `json_data` variable. \n\nNext, it checks if the \"Markdown_docs\" folder exists in the root directory of the repository. If the folder does not exist, it creates the folder using the `os.mkdir()` function.\n\nThen, the function iterates through each file in the `json_data` dictionary. For each file, it creates the path for the markdown file by replacing the file extension with \".md\". It then calls the `convert_to_markdown_file()` function to convert the file to markdown format and assigns the result to the `markdown` variable.\n\nAfter that, the function checks if the directory for the markdown file exists. If the directory does not exist, it creates the directory using the `os.makedirs()` function.\n\nFinally, the function writes the markdown content to the markdown file using the `open()` function with the \"w\" mode and the `write()` method.\n\n**Note**: \n- The function assumes that the project hierarchy is stored in a JSON file and the path to the file is specified in the `project_hierarchy` attribute of the `FileHandler` object.\n- The function also assumes that the repository path is specified in the `repo_path` attribute of the `FileHandler` object.\n- The function uses the `CONFIG` dictionary to get the name of the \"Markdown_docs\" folder.\n- The function uses the `convert_to_markdown_file()` function to convert each file to markdown format. The implementation of this function is not provided in the code snippet."
      ],
      "code_start_line": 288,
      "code_end_line": 320,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def convert_all_to_markdown_files_from_json(self):\n        \"\"\"\n        Converts all files to markdown format based on the JSON data.\n\n        Reads the project hierarchy from a JSON file, checks if the Markdown_docs folder exists,\n        creates it if it doesn't, and then iterates through each file in the JSON data.\n        For each file, it converts the file to markdown format and writes it to the Markdown_docs folder.\n\n        Args:\n            self (object): The file_handler object.\n\n        Returns:\n            None\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        # 检查根目录是否存在Markdown_docs文件夹，如果不存在则创建\n        markdown_docs_path = os.path.join(self.repo_path, CONFIG['Markdown_Docs_folder'])\n        if not os.path.exists(markdown_docs_path):\n            os.mkdir(markdown_docs_path)\n\n        # 遍历json_data[\"files\"]列表中的每个字典\n        for rel_file_path, file_dict in json_data.items():\n            md_path = os.path.join(markdown_docs_path, rel_file_path.replace('.py', '.md'))\n            markdown = self.convert_to_markdown_file(rel_file_path)\n            \n            # 检查目录是否存在，如果不存在，就创建它\n            os.makedirs(os.path.dirname(md_path), exist_ok=True)\n\n            # 将markdown文档写入到Markdown_docs文件夹中\n            with open(md_path, 'w', encoding='utf-8') as f:\n                f.write(markdown)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    }
  },
  "repo_agent/config.py": {},
  "repo_agent/doc_meta_info.py": {
    "EdgeType": {
      "type": "ClassDef",
      "name": "EdgeType",
      "md_content": [
        "**EdgeType**: The function of this Class is to define different types of edges in a graph.\n\n**attributes**: This Class does not have any attributes.\n\n**Code Description**: The `EdgeType` Class is an enumeration (Enum) that defines different types of edges in a graph. It has three members: `reference_edge`, `subfile_edge`, and `file_item_edge`. Each member represents a specific type of edge.\n\n- `reference_edge`: This type of edge represents a reference between two objects. It indicates that one object references another object.\n- `subfile_edge`: This type of edge represents a relationship where a file or folder belongs to another folder. It indicates that one file or folder is a subfile or subfolder of another folder.\n- `file_item_edge`: This type of edge represents a relationship where an object belongs to a file. It indicates that one object is associated with a specific file.\n\nThe `auto()` function is used to automatically assign unique values to each member of the enumeration.\n\n**Note**: The `EdgeType` Class is used to categorize and differentiate different types of edges in a graph. It provides a convenient way to represent and work with different edge types in a clear and organized manner."
      ],
      "code_start_line": 18,
      "code_end_line": 21,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class EdgeType(Enum):\n    reference_edge = auto() #一个obj引用另一个obj\n    subfile_edge = auto() # 一个 文件/文件夹 属于一个文件夹\n    file_item_edge = auto() #一个 obj 属于一个文件\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler",
        "repo_agent/file_handler.py/FileHandler/generate_overall_structure",
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemType/to_str",
        "repo_agent/doc_meta_info.py/DocItemType/print_self",
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/has_ans_relation",
        "repo_agent/doc_meta_info.py/DocItem/get_travel_list",
        "repo_agent/doc_meta_info.py/DocItem/check_depth",
        "repo_agent/doc_meta_info.py/DocItem/find_min_ances",
        "repo_agent/doc_meta_info.py/DocItem/parse_tree_path",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/DocItem/find",
        "repo_agent/doc_meta_info.py/DocItem/print_recursive",
        "repo_agent/doc_meta_info.py/DocItem/print_recursive/print_indent",
        "repo_agent/doc_meta_info.py/find_all_referencer",
        "repo_agent/doc_meta_info.py/MetaInfo",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files/walk_tree",
        "repo_agent/doc_meta_info.py/MetaInfo/find_obj_with_lineno",
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/get_subtree_list",
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/find_item",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel2",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json",
        "repo_agent/doc_meta_info.py/walk_file",
        "repo_agent/doc_meta_info.py/parse_one_item",
        "repo_agent/doc_meta_info.py/check_father"
      ]
    },
    "DocItemType": {
      "type": "ClassDef",
      "name": "DocItemType",
      "md_content": [
        "**DocItemType**: The function of this Class is to define the different types of documentation items in the project.\n\n**attributes**: This Class has the following attributes:\n- _repo: Represents the root node of the project, which requires generating a readme file.\n- _dir: Represents a directory in the project.\n- _file: Represents a file in the project.\n- _class: Represents a class in a file.\n- _class_function: Represents a function defined within a class.\n- _function: Represents a regular function within a file.\n- _sub_function: Represents a sub-function defined within a function.\n- _global_var: Represents a global variable within a file.\n\n**Code Description**: The `DocItemType` Class is an enumeration that defines the different types of documentation items in the project. Each item represents a specific type of object or element within the project hierarchy. The purpose of this Class is to provide a standardized way to identify and categorize different elements for documentation generation.\n\nThe Class defines the following item types:\n- `_repo`: This represents the root node of the project, indicating that a readme file needs to be generated for this item.\n- `_dir`: This represents a directory within the project.\n- `_file`: This represents a file within the project.\n- `_class`: This represents a class within a file.\n- `_class_function`: This represents a function defined within a class.\n- `_function`: This represents a regular function within a file.\n- `_sub_function`: This represents a sub-function defined within a function.\n- `_global_var`: This represents a global variable within a file.\n\nThe Class also provides two helper methods:\n- `to_str()`: This method converts the `DocItemType` item to a string representation. It is used to map the item types to their corresponding string names. For example, `_class` is mapped to \"ClassDef\", `_function` is mapped to \"FunctionDef\", and so on.\n- `print_self()`: This method returns a colored string representation of the `DocItemType` item. The color is determined based on the item type, with different colors assigned to different types of items.\n\n**Note**: The `DocItemType` Class is used in conjunction with other classes and functions in the project to generate documentation for different elements. It provides a standardized way to identify and categorize different types of items within the project hierarchy.\n\n**Output Example**: \n- `_repo`: Root\n- `_dir`: Directory\n- `_file`: File\n- `_class`: Class\n- `_class_function`: Class Function\n- `_function`: Function\n- `_sub_function`: Sub-function\n- `_global_var`: Global Variable"
      ],
      "code_start_line": 25,
      "code_end_line": 59,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItemType(Enum):\n    _repo = auto() #根节点，需要生成readme\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_function = auto()\n    _function = auto() #文件内的常规function\n    _sub_function = auto() #function内的定义的subfunction\n    _global_var = auto()\n\n    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        assert False, f\"{self.name}\"\n\n    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n\n    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "to_str": {
      "type": "FunctionDef",
      "name": "to_str",
      "md_content": [
        "**to_str**: The function of this Function is to convert the DocItemType enum value to a string representation.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \n- The function `to_str` is defined as a method of the `DocItemType` enum class.\n- Inside the function, there is a series of if-elif statements to check the value of `self` (which represents the current enum value).\n- If `self` is equal to `DocItemType._class`, the function returns the string \"ClassDef\".\n- If `self` is equal to `DocItemType._function`, `DocItemType._class_function`, or `DocItemType._sub_function`, the function returns the string \"FunctionDef\".\n- If none of the above conditions are met, the function raises an assertion error with the value of `self.name`.\n\n**Note**: \n- This function is used to convert the enum value of `DocItemType` to its corresponding string representation.\n- The function assumes that the enum value is one of the predefined values in the `DocItemType` enum class.\n\n**Output Example**: \n- If `self` is `DocItemType._class`, the function will return the string \"ClassDef\".\n- If `self` is `DocItemType._function`, `DocItemType._class_function`, or `DocItemType._sub_function`, the function will return the string \"FunctionDef\"."
      ],
      "code_start_line": 35,
      "code_end_line": 44,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        assert False, f\"{self.name}\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "print_self": {
      "type": "FunctionDef",
      "name": "print_self",
      "md_content": [
        "**print_self**: The function of this Function is to print the name of the current DocItemType object with a specific color based on its type.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThe code first initializes the `color` variable with the value of `Fore.WHITE`. Then it checks the type of the current DocItemType object (`self`) using if-elif statements. If the type is `DocItemType._dir`, the `color` variable is updated to `Fore.GREEN`. If the type is `DocItemType._file`, the `color` variable is updated to `Fore.YELLOW`. If the type is `DocItemType._class`, the `color` variable is updated to `Fore.BLUE`. If the type is `DocItemType._function`, the `color` variable is updated to `Fore.RED`. Finally, the function returns the name of the current DocItemType object (`self.name`) with the updated color and the style is reset to the default using `Style.RESET_ALL`.\n\n**Note**: \n- This function is designed to be used within the DocItemType class.\n- The `Fore` and `Style` classes are part of the `colorama` library, which provides cross-platform support for colored terminal text.\n- The `name` attribute of the DocItemType object represents the name of the object.\n\n**Output Example**: \nIf the current DocItemType object is of type `DocItemType._file`, the function will return the name of the object (`self.name`) with the color set to yellow. For example, if the name of the object is \"file1\", the output will be displayed as \"file1\" in yellow color."
      ],
      "code_start_line": 46,
      "code_end_line": 56,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_edge_type": {
      "type": "FunctionDef",
      "name": "get_edge_type",
      "md_content": [
        "**get_edge_type**: The function of this Function is to determine the type of edge between two DocItemType objects.\n\n**parameters**: \n- from_item_type: A DocItemType object representing the type of the starting item.\n- to_item_type: A DocItemType object representing the type of the ending item.\n\n**Code Description**:\nThis function takes two parameters, from_item_type and to_item_type, both of which are instances of the DocItemType class. The function returns an EdgeType object, which represents the type of edge between the two given item types.\n\nThe function does not contain any code implementation. It is defined with the \"pass\" statement, indicating that the implementation is missing and needs to be added.\n\n**Note**: \n- This function is incomplete and needs to be implemented to provide the desired functionality."
      ],
      "code_start_line": 58,
      "code_end_line": 59,
      "parent": "DocItemType",
      "params": [
        "from_item_type",
        "to_item_type"
      ],
      "have_return": false,
      "code_content": "    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "DocItemStatus": {
      "type": "ClassDef",
      "name": "DocItemStatus",
      "md_content": [
        "**DocItemStatus**: The function of this Class is to define the status of a documentation item.\n\n**attributes**: This Class does not have any attributes.\n\n**Code Description**: The `DocItemStatus` Class is an enumeration that represents the different status of a documentation item. It is used to indicate whether a documentation item needs to be generated, updated, or if it is up to date. The Class is defined using the `Enum` class from the `enum` module.\n\nThe following are the different status values defined in the `DocItemStatus` enumeration:\n- `doc_up_to_date`: This status indicates that the documentation for the item is up to date and does not need to be generated or updated.\n- `doc_has_not_been_generated`: This status indicates that the documentation for the item has not been generated yet and needs to be generated.\n- `code_changed`: This status indicates that the source code of the item has been modified and the documentation needs to be updated.\n- `add_new_referencer`: This status indicates that a new object has referenced the item and the documentation needs to be updated to reflect this change.\n- `referencer_not_exist`: This status indicates that an object that previously referenced the item has been deleted or no longer references it, and the documentation needs to be updated accordingly.\n\n**Note**: The `DocItemStatus` Class is used in the project in various places to track the status of documentation items and determine whether they need to be generated or updated. It is important to regularly check the status of documentation items and take appropriate actions based on their status to ensure that the documentation remains accurate and up to date."
      ],
      "code_start_line": 62,
      "code_end_line": 67,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class DocItemStatus(Enum):\n    doc_up_to_date = auto() #无需生成文档\n    doc_has_not_been_generated = auto() #文档还未生成，需要生成\n    code_changed = auto() #源码被修改了，需要改文档\n    add_new_referencer = auto() #添加了新的引用者\n    referencer_not_exist = auto() #曾经引用他的obj被删除了，或者不再引用他了\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "DocItem": {
      "type": "ClassDef",
      "name": "DocItem",
      "md_content": [
        "**DocItem**: The function of this Class is to represent a documentation item in the project hierarchy.\n\n**Attributes**:\n- item_type: The type of the documentation item.\n- item_status: The status of the documentation item.\n- obj_name: The name of the object.\n- md_content: A list that stores the documentation content for different versions.\n- content: A dictionary that stores the original information.\n- children: A dictionary that stores the child objects.\n- father: The parent object.\n- depth: The depth of the object in the hierarchy.\n- tree_path: A list that represents the path from the root to the object.\n- max_reference_ansce: The earliest ancestor node that has a reference to the object.\n- reference_who: A list of objects that reference the object.\n- who_reference_me: A list of objects that are referenced by the object.\n- reference_who_name_list: A list of names of objects that reference the object.\n- who_reference_me_name_list: A list of names of objects that are referenced by the object.\n\n**Code Description**:\nThe `DocItem` class represents a documentation item in the project hierarchy. It has various attributes to store information about the item, such as its type, status, name, content, children, parent, depth, path, and references.\n\nThe `item_type` attribute represents the type of the documentation item. It is an instance of the `DocItemType` enumeration, which defines different types of documentation items.\n\nThe `item_status` attribute represents the status of the documentation item. It is an instance of the `DocItemStatus` enumeration, which defines different statuses for the documentation item.\n\nThe `obj_name` attribute stores the name of the object.\n\nThe `md_content` attribute is a list that stores the documentation content for different versions. Each version of the documentation is represented as a string.\n\nThe `content` attribute is a dictionary that stores the original information of the documentation item.\n\nThe `children` attribute is a dictionary that stores the child objects of the documentation item. Each child object is represented as a `DocItem` instance.\n\nThe `father` attribute represents the parent object of the documentation item. It is a reference to another `DocItem` instance.\n\nThe `depth` attribute represents the depth of the documentation item in the project hierarchy. It is an integer value.\n\nThe `tree_path` attribute is a list that represents the path from the root to the documentation item. Each element in the list is a `DocItem` instance.\n\nThe `max_reference_ansce` attribute represents the earliest ancestor node that has a reference to the documentation item. It is a reference to another `DocItem` instance.\n\nThe `reference_who` attribute is a list of objects that reference the documentation item. Each object is represented as a `DocItem` instance.\n\nThe `who_reference_me` attribute is a list of objects that are referenced by the documentation item. Each object is represented as a `DocItem` instance.\n\nThe `reference_who_name_list` attribute is a list of names of objects that reference the documentation item. Each name is a string.\n\nThe `who_reference_me_name_list` attribute is a list of names of objects that are referenced by the documentation item. Each name is a string.\n\nThe `has_ans_relation` method checks if there is an ancestor relationship between two nodes and returns the earlier node.\n\nThe `get_travel_list` method returns a list of all `DocItem` instances in the hierarchy, starting from the current object.\n\nThe `check_depth` method calculates and sets the depth of the documentation item based on its children.\n\nThe `find_min_ances` method finds the minimum ancestor node between two nodes.\n\nThe `parse_tree_path` method sets the `tree_path` attribute for the documentation item and its children.\n\nThe `get_full_name` method returns the full name of the documentation item, including all object names from bottom to top.\n\nThe `find` method searches for a documentation item based on a given path list and returns the corresponding item if found.\n\nThe `print_recursive` method recursively prints the documentation item and its children.\n\n**Note**: None.\n\n**Output Example**: None."
      ],
      "code_start_line": 71,
      "code_end_line": 174,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItem():\n    item_type: DocItemType = DocItemType._class_function\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n\n    obj_name: str = \"\" #对象的名字\n    md_content: List[str] = field(default_factory=list) #存储不同版本的doc\n    content: Dict[Any,Any] = field(default_factory=dict) #原本存储的信息\n\n    children: Dict[str, DocItem] = field(default_factory=dict) #子对象\n    father: Any[DocItem] = None\n\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list) #一整条链路，从root开始\n    max_reference_ansce: Any[DocItem] = None\n\n    reference_who: List[DocItem] = field(default_factory=list) #他引用了谁\n    who_reference_me: List[DocItem] = field(default_factory=list) #谁引用了他\n\n    reference_who_name_list: List[str] = field(default_factory=list) #他引用了谁，这个可能是老版本的\n    who_reference_me_name_list: List[str] = field(default_factory=list) #谁引用了他，这个可能是老版本的\n\n    @staticmethod\n    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n    \n    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n    \n    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n\n\n    \n    @staticmethod\n    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n\n    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n\n    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n    \n    \n    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py",
        "repo_agent/chat_engine.py/get_import_statements",
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "has_ans_relation": {
      "type": "FunctionDef",
      "name": "has_ans_relation",
      "md_content": [
        "**has_ans_relation**: The function of this Function is to determine whether there is an ancestor relationship between two nodes, and if so, return the earlier node.\n\n**parameters**: \n- now_a: A DocItem object representing the first node.\n- now_b: A DocItem object representing the second node.\n\n**Code Description**: \nThe function first checks if `now_b` is in the tree path of `now_a`. If it is, it means that `now_b` is an ancestor of `now_a`, so `now_b` is returned. \nIf `now_a` is in the tree path of `now_b`, it means that `now_a` is an ancestor of `now_b`, so `now_a` is returned. \nIf neither condition is met, it means that there is no ancestor relationship between the two nodes, so `None` is returned.\n\n**Note**: \n- This function assumes that the `tree_path` attribute of the `DocItem` object contains the path from the root node to the current node.\n- The function does not handle the case where `now_a` and `now_b` are the same node.\n\n**Output Example**: \nIf `now_a` is an ancestor of `now_b`, the function will return `now_a`. If `now_b` is an ancestor of `now_a`, the function will return `now_b`. If there is no ancestor relationship between the two nodes, the function will return `None`."
      ],
      "code_start_line": 93,
      "code_end_line": 99,
      "parent": "DocItem",
      "params": [
        "now_a",
        "now_b"
      ],
      "have_return": true,
      "code_content": "    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_travel_list": {
      "type": "FunctionDef",
      "name": "get_travel_list",
      "md_content": [
        "**get_travel_list**: The function of this Function is to retrieve a list of all objects in the hierarchy, starting from the current object.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function starts by initializing a list called `now_list` with the current object (`self`). Then, it iterates over each child object in the `children` dictionary of the current object. For each child, it recursively calls the `get_travel_list()` function and appends the returned list to the `now_list`. Finally, it returns the `now_list`, which contains all the objects in the hierarchy.\n\n**Note**: It is important to note that this function assumes that the current object has a `children` attribute, which is a dictionary containing the child objects. If this attribute is not present, the function may raise an AttributeError.\n\n**Output Example**: \nIf the current object has two child objects, the returned list may look like this:\n`[self, child1, child2, grandchild1, grandchild2, grandchild3, ...]`"
      ],
      "code_start_line": 101,
      "code_end_line": 105,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "check_depth": {
      "type": "FunctionDef",
      "name": "check_depth",
      "md_content": [
        "**check_depth**: The function of this Function is to calculate the depth of a node in a tree structure.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThe code first checks if the node has any children. If the node has no children, it means that it is a leaf node and its depth is set to 0. The function then returns the depth of the node.\n\nIf the node has children, the code initializes a variable `max_child_depth` to 0. It then iterates over each child of the node and recursively calls the `check_depth` function on each child. The depth of each child is calculated and stored in the variable `child_depth`. The code then compares the `child_depth` with the current maximum child depth (`max_child_depth`) and updates `max_child_depth` if `child_depth` is greater.\n\nAfter iterating over all the children, the code sets the depth of the current node to `max_child_depth + 1`. This is because the depth of the current node is equal to the maximum depth of its children plus 1. Finally, the function returns the depth of the node.\n\n**Note**: \n- This function assumes that the node has a `children` attribute which is a dictionary containing the children nodes.\n- The depth of a node is defined as the length of the longest path from the node to a leaf node in the tree.\n\n**Output Example**: \nIf the node has no children, the function will return 0.\nIf the node has children with depths 1, 2, and 3, the function will return 4."
      ],
      "code_start_line": 107,
      "code_end_line": 116,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_min_ances": {
      "type": "FunctionDef",
      "name": "find_min_ances",
      "md_content": [
        "**find_min_ances**: The function of this Function is to find the minimum common ancestor between two DocItem objects.\n\n**parameters**: \n- node_a: A DocItem object representing the first node.\n- node_b: A DocItem object representing the second node.\n\n**Code Description**:\nThe function starts by initializing the position variable to 0. It then asserts that the tree_path of node_a and node_b at position 0 are equal. This ensures that both nodes are part of the same tree.\n\nThe function enters a while loop that continues indefinitely. In each iteration, the position variable is incremented by 1. If the tree_path of node_a and node_b at the current position are not equal, it means that the common ancestor has been found. In this case, the function returns the tree_path value of node_a at the previous position (pos - 1), which represents the minimum common ancestor.\n\n**Note**: \n- This function assumes that both node_a and node_b are valid DocItem objects with valid tree_path attributes.\n- The function expects that node_a and node_b are part of the same tree, otherwise the assertion will fail.\n\n**Output Example**:\nIf node_a.tree_path = ['RepoAgent', 'display', 'book_template'] and node_b.tree_path = ['RepoAgent', 'display', 'book_tools'], the function will return 'display' as the minimum common ancestor."
      ],
      "code_start_line": 121,
      "code_end_line": 127,
      "parent": "DocItem",
      "params": [
        "node_a",
        "node_b"
      ],
      "have_return": true,
      "code_content": "    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "parse_tree_path": {
      "type": "FunctionDef",
      "name": "parse_tree_path",
      "md_content": [
        "**parse_tree_path**: The function of this Function is to generate the tree path for each node in the parse tree.\n\n**parameters**: \n- now_path: A list representing the current path in the parse tree.\n\n**Code Description**:\nThe `parse_tree_path` function takes in the `now_path` parameter, which represents the current path in the parse tree. It then sets the `tree_path` attribute of the current node to the concatenation of the `now_path` list and the current node itself.\n\nNext, it iterates over the `children` dictionary of the current node. For each key-value pair in the dictionary, it recursively calls the `parse_tree_path` function on the child node, passing in the updated `tree_path` as the `now_path` parameter.\n\nThis process continues until all child nodes have been processed.\n\n**Note**: \n- This function is used to generate the tree path for each node in the parse tree. The `tree_path` attribute of each node will be updated accordingly.\n- The `now_path` parameter should be a list representing the current path in the parse tree."
      ],
      "code_start_line": 129,
      "code_end_line": 132,
      "parent": "DocItem",
      "params": [
        "self",
        "now_path"
      ],
      "have_return": false,
      "code_content": "    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_full_name": {
      "type": "FunctionDef",
      "name": "get_full_name",
      "md_content": [
        "**get_full_name**: The function of this Function is to retrieve the full name of an object by traversing its hierarchy from bottom to top.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function starts by checking if the current object has a father (parent) object. If it does not have a father, it means that it is the top-level object, and its own name is returned as the full name.\n\nIf the object has a father, a list called `name_list` is initialized to store the names of the objects in the hierarchy. The current object is assigned to a variable called `now`.\n\nA while loop is then used to traverse the hierarchy from the current object to its father, adding each object's name to the `name_list` in reverse order. This is done by appending the current object's name to the beginning of the `name_list` using the `+` operator.\n\nAfter adding the current object's name to the `name_list`, the `now` variable is updated to the father object. This process continues until the current object does not have a father (i.e., `now` becomes `None`).\n\nOnce the loop is completed, the first element of the `name_list` is removed because it corresponds to the current object's own name. The remaining names in the `name_list` are then joined together using the \"/\" separator to form the full name of the object.\n\nFinally, the full name of the object is returned as a string.\n\n**Note**: It is important to note that this Function assumes that the object hierarchy is correctly defined and that each object has a valid reference to its father object.\n\n**Output Example**: If the object hierarchy is as follows: `repo_agent -> display -> book_template -> book_tools -> generate_repoagent_books.py`, calling `get_full_name()` on the `generate_repoagent_books.py` object would return `\"display/book_template/book_tools/generate_repoagent_books.py\"`."
      ],
      "code_start_line": 134,
      "code_end_line": 145,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements",
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "find": {
      "type": "FunctionDef",
      "name": "find",
      "md_content": [
        "**find**: The function of this Function is to search for a specific file or folder in the repository based on the given path list.\n\n**parameters**: \n- recursive_file_path (list): A list of strings representing the path to the desired file or folder.\n\n**Code Description**: \nThe `find` function is used to search for a specific file or folder in the repository based on the given path list. It starts from the root node of the repository and iteratively traverses the repository's hierarchy until it finds the desired file or folder. If the file or folder is found, it returns the corresponding `DocItem` object. Otherwise, it returns `None`.\n\nThe function first checks if the current `DocItem` object is of type `_repo` (indicating the root node of the repository). If not, it raises an assertion error.\n\nThen, it initializes a variable `pos` to keep track of the current position in the `recursive_file_path` list, and a variable `now` to keep track of the current `DocItem` object being traversed, starting from the root node.\n\nThe function enters a while loop that continues until the end of the `recursive_file_path` list is reached. In each iteration, it checks if the current path element (`recursive_file_path[pos]`) exists as a key in the `children` dictionary of the current `DocItem` object (`now`). If not, it means that the desired file or folder does not exist in the repository, and the function returns `None`. Otherwise, it updates the `now` variable to the corresponding child `DocItem` object and increments the `pos` variable to move to the next path element.\n\nOnce the while loop completes, the function has reached the desired file or folder in the repository hierarchy, and it returns the corresponding `DocItem` object.\n\n**Note**: \n- This function assumes that the `item_type` attribute of the current `DocItem` object is set to `_repo` to indicate the root node of the repository. If this is not the case, an assertion error will be raised.\n- The `recursive_file_path` list should be a valid path to a file or folder in the repository, starting from the root node.\n\n**Output Example**: \nIf the desired file or folder exists in the repository, the function will return the corresponding `DocItem` object. Otherwise, it will return `None`."
      ],
      "code_start_line": 148,
      "code_end_line": 159,
      "parent": "DocItem",
      "params": [
        "self",
        "recursive_file_path"
      ],
      "have_return": true,
      "code_content": "    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "print_recursive": {
      "type": "FunctionDef",
      "name": "print_recursive",
      "md_content": [
        "**print_recursive**: The function of this Function is to recursively print the repo object and its children.\n\n**parameters**: \n- indent (int): The number of spaces to indent the printed output. Default is 0.\n- print_content (bool): Whether to print the content of the repo object. Default is False.\n\n**Code Description**: \nThis function first defines a helper function called `print_indent` which returns a string of spaces and dashes based on the given indent level. \n\nThen, it prints the repo object's type and name with the corresponding indent level using the `print_indent` function. If the repo object has children, it also prints the number of children. \n\nNext, it iterates over each child of the repo object and recursively calls the `print_recursive` function on each child, increasing the indent level by 1.\n\n**Note**: \n- This function assumes that the repo object has a `print_self` method that returns a string representation of the object's type.\n- The `print_content` parameter is not used in the code provided, but it can be used to control whether the content of the repo object is printed or not.\n\n**Output Example**: \n```\n|-EdgeType: reference_edge, 2 children\n  |-EdgeType: subfile_edge, 0 children\n  |-EdgeType: file_item_edge, 0 children\n```"
      ],
      "code_start_line": 161,
      "code_end_line": 174,
      "parent": "DocItem",
      "params": [
        "self",
        "indent",
        "print_content"
      ],
      "have_return": true,
      "code_content": "    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "print_indent": {
      "type": "FunctionDef",
      "name": "print_indent",
      "md_content": [
        "**print_indent**: The function of this Function is to generate an indented string based on the given indent level.\n\n**parameters**: \n- indent (int): The level of indentation. Default is 0.\n\n**Code Description**: \nThe `print_indent` function takes an optional parameter `indent` which represents the level of indentation. If the `indent` is 0, the function returns an empty string. Otherwise, it generates an indented string by concatenating the indentation string \"  \" (two spaces) repeated `indent` times, followed by the \"|\" character and a hyphen (\"-\").\n\n**Note**: \n- The `indent` parameter must be an integer.\n- The function returns an empty string if the `indent` is 0.\n\n**Output Example**: \n- If `indent` is 3, the function will return \"      |-\"\n- If `indent` is 0, the function will return an empty string."
      ],
      "code_start_line": 164,
      "code_end_line": 167,
      "parent": "print_recursive",
      "params": [
        "indent"
      ],
      "have_return": true,
      "code_content": "        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_all_referencer": {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [
        "**find_all_referencer**: The function of this Function is to find all the references to a specific variable in a given file.\n\n**parameters**: \n- repo_path (str): The path to the repository.\n- variable_name (str): The name of the variable to search for.\n- file_path (str): The path to the file where the variable is defined.\n- line_number (int): The line number where the variable is defined.\n- column_number (int): The column number where the variable is defined.\n\n**Code Description**: \nThis function uses the Jedi library to analyze the given file and find all the references to the specified variable. It takes the repository path, variable name, file path, line number, and column number as input parameters.\n\nFirst, it creates a Jedi script object using the file path. Then, it calls the `get_references` method of the script object, passing the line number and column number as arguments. This method returns a list of references to different objects in the file.\n\nNext, the function filters out the references that have the same variable name as the one specified. It creates a new list called `variable_references` using a list comprehension. Each reference in the `references` list is checked if its name matches the specified variable name.\n\nFinally, the function returns a list of tuples containing the relative path of the module where the reference is found, the line number, and the column number of each reference. It excludes the reference that matches the line number and column number of the variable definition itself.\n\nIf any exception occurs during the execution of the function, it prints the error message and the values of the input parameters. It then returns an empty list.\n\n**Note**: \n- This function requires the Jedi library to be installed.\n- The `repo_path` parameter should be the root path of the repository.\n- The `file_path` parameter should be the relative path of the file within the repository.\n- The line number and column number should correspond to the location of the variable definition in the file.\n\n**Output Example**:\n```python\n[('repo_agent/doc_meta_info.py', 10, 5), ('repo_agent/doc_meta_info.py', 15, 10)]\n```"
      ],
      "code_start_line": 178,
      "code_end_line": 191,
      "parent": null,
      "params": [
        "repo_path",
        "variable_name",
        "file_path",
        "line_number",
        "column_number"
      ],
      "have_return": true,
      "code_content": "def find_all_referencer(repo_path, variable_name, file_path, line_number, column_number):\n    \"\"\"复制过来的之前的实现\"\"\"\n    script = jedi.Script(path=os.path.join(repo_path, file_path))\n    references = script.get_references(line=line_number, column=column_number)\n\n    try:\n        # 过滤出变量名为 variable_name 的引用，并返回它们的位置\n        variable_references = [ref for ref in references if ref.name == variable_name]\n        return [(os.path.relpath(ref.module_path, repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n    except Exception as e:\n        # 打印错误信息和相关参数\n        print(f\"Error occurred: {e}\")\n        print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n        return []\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "MetaInfo": {
      "type": "ClassDef",
      "name": "MetaInfo",
      "md_content": [
        "**MetaInfo**: The function of this Class is to store and manage metadata information related to a repository, including the repository path, document version, repository hierarchical tree structure, and other relevant information.\n\n**attributes**: The attributes of this Class are as follows:\n- `repo_path`: A string representing the path of the repository.\n- `document_version`: A string representing the version of the document. An empty string indicates that the document is not yet completed, while a non-empty string corresponds to the commit hash of a target repository.\n- `target_repo_hierarchical_tree`: An instance of the `DocItem` class representing the hierarchical structure of the repository.\n- `in_generation_process`: A boolean value indicating whether the document generation process is in progress.\n\n**Code Description**: The `MetaInfo` class provides various methods to initialize, load, and manipulate metadata information.\n\n- `init_from_project_path(project_abs_path: str) -> MetaInfo`: This static method initializes a new `MetaInfo` object from a given project path. It takes the absolute path of the project as input and returns a `MetaInfo` object. It first generates the overall structure of the repository using the `generate_overall_structure()` method from the `FileHandler` class. Then, it creates a new `MetaInfo` object, sets the repository path, and returns the object.\n\n- `from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo`: This static method reads the metadata information from a given checkpoint directory path. It takes the checkpoint directory path as input and returns a `MetaInfo` object. It reads the project hierarchy JSON file and the meta-info JSON file from the checkpoint directory. It then creates a new `MetaInfo` object, sets the repository path, document version, and in-generation process status, and returns the object.\n\n- `checkpoint(self, target_dir_path: str, flash_reference_relation=False)`: This method saves the `MetaInfo` object to a given target directory path. It takes the target directory path as input and saves the project hierarchy JSON file and the meta-info JSON file to the target directory. If `flash_reference_relation` is set to `True`, it also saves the reference relations between objects.\n\n- `load_task_list(self)`: This method returns a list of `DocItem` objects that need to be processed. It retrieves the topology of the repository using the `get_topology()` method and filters out the `DocItem` objects that are not up to date.\n\n- `print_task_list(self, item_list)`: This method prints the remaining tasks to be done. It takes a list of `DocItem` objects as input and prints the task ID, the reason for document generation, and the path of each task.\n\n- `get_all_files(self) -> List[DocItem]`: This method returns a list of all `DocItem` objects representing files in the repository.\n\n- `find_obj_with_lineno(self, file_node, start_line_num) -> DocItem`: This method finds the `DocItem` object corresponding to a given file node and starting line number. It takes a file node and a starting line number as input and returns the corresponding `DocItem` object.\n\n- `parse_reference(self)`: This method parses the reference relations between objects in the repository. It iterates through all file nodes, finds the referencers of each object, and updates the reference relations accordingly.\n\n- `get_subtree_list(self, now_node: DocItem) -> List[Any]`: This method returns a sorted list of `DocItem` objects representing the subtree of a given node. It first generates a list of all `DocItem` objects using the `get_travel_list()` method, sorts them by depth, and then arranges them in topological order.\n\n- `get_topology(self) -> List[DocItem]`: This method returns a list of `DocItem` objects representing the topological order of the repository. It calls the `parse_reference()` method to parse the reference relations and then calls the `get_subtree_list()` method to get the topological order.\n\n- `_map(self, deal_func: Callable)`: This method applies a given function to all nodes in the repository. It takes a function as input and applies it recursively to all nodes.\n\n- `load_doc_from_older_meta(self, older_meta: MetaInfo)`: This method merges the document from an older version of `MetaInfo` into the current `MetaInfo` object. It takes an older `MetaInfo` object as input and updates the document content and status of corresponding `DocItem` objects.\n\n- `from_project_hierarchy_path(repo_path: str) -> MetaInfo`: This static method initializes a new `MetaInfo` object from a given repository path. It reads the project hierarchy JSON file and creates a new `MetaInfo` object based on the file content.\n\n- `to_hierarchy_json(self, flash_reference_relation = False)`: This method converts the `MetaInfo` object to a hierarchical JSON representation. It returns a dictionary representing the hierarchical structure"
      ],
      "code_start_line": 195,
      "code_end_line": 588,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class MetaInfo():\n    repo_path: str = \"\"\n    document_version: str = \"\" #随时间变化，\"\"代表没完成，否则对应一个目标仓库的commit hash\n    target_repo_hierarchical_tree: DocItem = field(default_factory=\"Docitem\") #整个repo的文件结构\n    \n    in_generation_process: bool = False\n\n    @staticmethod\n    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n    \n    @staticmethod\n    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n\n    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        logger.info(f\"will save MetaInfo at {target_dir_path}\")\n        if not os.path.exists(target_dir_path):\n            os.makedirs(target_dir_path)\n        now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n        with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n            json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n        \n        with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n            meta = {\n                \"repo_path\": self.repo_path,\n                \"doc_version\": self.document_version,\n                \"in_generation_process\": self.in_generation_process,\n            }\n            json.dump(meta, writer, indent=2, ensure_ascii=False)\n    \n    def load_task_list(self):\n        task_list = self.get_topology()\n        return [item for item in task_list if item.item_status != DocItemStatus.doc_up_to_date]\n    \n    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n\n    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n\n    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\"\"\"\n        now_node = file_node\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n\n            \n\n    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n        for file_node in file_nodes:\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"]\n                )\n                \n                for referencer_pos in reference_list:\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    # if now_obj.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n                    #     print(reference_list)\n                    #     print(referencer_node.get_full_name())\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n            # logger.info(f\"find {ref_count} refer-relation in {file_node.get_full_name()}\")\n    \n\n    def get_subtree_list(self, now_node: DocItem) -> List[Any]:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        sorted_items = []\n        while items_by_depth:\n            for item in items_by_depth:\n                if all(referenced in sorted_items for referenced in item.reference_who):\n                    sorted_items.append(item)\n                    items_by_depth.remove(item)\n\n                    def check_father(item):\n                        nonlocal sorted_items\n                        nonlocal items_by_depth\n                        if item.father == None:\n                            return\n                        father_node = item.father\n                        for _,node in father_node.children.items():\n                            if node not in sorted_items:\n                                return\n                        #所有儿子都进去了，父亲也可以进去，并且应该挨着\n                        sorted_items.append(father_node)\n                        items_by_depth.remove(father_node)\n                        check_father(father_node)\n\n                    check_father(item)\n                    break\n\n        # Further optimization for minimizing tree distance could be added here\n        return sorted_items\n\n    def get_topology(self) -> List[DocItem]:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        topology_list = self.get_subtree_list(self.target_repo_hierarchical_tree)\n        return topology_list\n    \n    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n\n    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            nonlocal root_item\n            if now_item.father == None:\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status == DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的应用吧改了\"\"\"\n\n        self.parse_reference() \n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n            # if now_older_item.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n            #     print(now_older_item.get_full_name())\n            #     print(new_reference_names)\n            #     print(old_reference_names)\n            #     print(\"****\")\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n                # print(now_older_item.get_full_name())\n                # print(new_reference_names)\n                # print(old_reference_names)\n                # print(\"*******\")\n\n\n\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n\n        topology_list = self.get_subtree_list(self.target_repo_hierarchical_tree)\n        return topology_list\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n    \n    def to_hierarchy_json(self, flash_reference_relation = False):\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n\n    @staticmethod\n    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "init_from_project_path": {
      "type": "FunctionDef",
      "name": "init_from_project_path",
      "md_content": [
        "**init_from_project_path**: The function of this Function is to initialize a MetaInfo object from a given project path.\n\n**parameters**: \n- project_abs_path (str): The absolute path of the project.\n\n**Code Description**: \nThis function takes the absolute path of a project as input and initializes a new MetaInfo object. It first assigns the value of the 'repo_path' key from the CONFIG dictionary to the 'project_abs_path' variable. Then, it logs an informational message indicating that a new meta-info is being initialized from the project path. \n\nNext, it creates a FileHandler object by passing the 'project_abs_path' and None as arguments. The FileHandler is responsible for handling file-related operations in the project. \n\nAfter that, it calls the 'generate_overall_structure' method of the FileHandler object to generate the overall structure of the project. This method returns a JSON representation of the project hierarchy.\n\nThen, it calls the 'from_project_hierarchy_json' method of the MetaInfo class, passing the generated project hierarchy JSON as an argument. This method creates a new MetaInfo object and initializes it with the provided project hierarchy.\n\nFinally, it assigns the value of 'project_abs_path' to the 'repo_path' attribute of the MetaInfo object and returns the initialized MetaInfo object.\n\n**Note**: \n- The 'CONFIG' variable is assumed to be a dictionary that contains configuration settings for the project.\n- The 'logger' variable is assumed to be an instance of a logger class used for logging messages.\n\n**Output Example**: \nA MetaInfo object initialized with the provided project path and project hierarchy."
      ],
      "code_start_line": 203,
      "code_end_line": 211,
      "parent": "MetaInfo",
      "params": [
        "project_abs_path"
      ],
      "have_return": true,
      "code_content": "    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "from_checkpoint_path": {
      "type": "FunctionDef",
      "name": "from_checkpoint_path",
      "md_content": [
        "**from_checkpoint_path**: The function of this Function is to load the MetaInfo object from a given checkpoint directory path.\n\n**parameters**: \n- checkpoint_dir_path (str): The path of the checkpoint directory.\n\n**Code Description**:\nThe function first constructs the path of the project_hierarchy_json file by joining the checkpoint_dir_path with \".project_hierarchy.json\". It then opens the project_hierarchy_json file in read mode and loads its content into the project_hierarchy_json variable using the json.load() function.\n\nNext, it calls the from_project_hierarchy_json() method of the MetaInfo class, passing the project_hierarchy_json as the argument. This method constructs a new MetaInfo object based on the information extracted from the project_hierarchy_json.\n\nThen, the function opens the \"meta-info.json\" file located in the checkpoint_dir_path in read mode and loads its content into the meta_data variable using the json.load() function. It assigns the values of \"repo_path\", \"doc_version\", and \"in_generation_process\" from the meta_data dictionary to the corresponding attributes of the metainfo object.\n\nFinally, the function logs a message indicating that the meta-info is being loaded from the checkpoint_dir_path with the document version, and returns the metainfo object.\n\n**Note**: \n- This function assumes that the checkpoint directory contains the necessary files: \".project_hierarchy.json\" and \"meta-info.json\".\n- The MetaInfo class should have a static method named from_project_hierarchy_json().\n\n**Output Example**:\n```\nloading meta-info from /path/to/checkpoint_dir, document-version=\"1.0.0\"\n<MetaInfo object at 0x7f9a3e6a2a90>\n```"
      ],
      "code_start_line": 214,
      "code_end_line": 230,
      "parent": "MetaInfo",
      "params": [
        "checkpoint_dir_path"
      ],
      "have_return": true,
      "code_content": "    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "checkpoint": {
      "type": "FunctionDef",
      "name": "checkpoint",
      "md_content": [
        "**checkpoint**: The function of this Function is to save the MetaInfo of the project at the specified target directory path.\n\n**parameters**: \n- target_dir_path: A string representing the path where the MetaInfo will be saved.\n- flash_reference_relation: A boolean value indicating whether to include the flash reference relation in the saved MetaInfo. It is set to False by default.\n\n**Code Description**: \nThe `checkpoint` function first logs an information message indicating the target directory path where the MetaInfo will be saved. It then checks if the target directory path exists, and if not, creates the directory using the `os.makedirs` function.\n\nNext, it calls the `to_hierarchy_json` function to convert the current MetaInfo to a hierarchy JSON representation, passing the `flash_reference_relation` parameter if specified. The resulting JSON is then saved to a file named \".project_hierarchy.json\" in the target directory path using the `json.dump` function.\n\nAfter that, the function creates a dictionary named `meta` containing the repo path, document version, and in-generation process status. This dictionary is then saved to a file named \"meta-info.json\" in the target directory path using the `json.dump` function.\n\n**Note**: \n- The `checkpoint` function is responsible for saving the MetaInfo of the project, which includes the hierarchy JSON representation and additional meta information.\n- The target directory path must be a valid path where the user has write permissions.\n- If the target directory path does not exist, the function will create it.\n- The saved MetaInfo files are named \".project_hierarchy.json\" and \"meta-info.json\" respectively.\n- The saved MetaInfo includes the repo path, document version, and in-generation process status."
      ],
      "code_start_line": 232,
      "code_end_line": 246,
      "parent": "MetaInfo",
      "params": [
        "self",
        "target_dir_path",
        "flash_reference_relation"
      ],
      "have_return": false,
      "code_content": "    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        logger.info(f\"will save MetaInfo at {target_dir_path}\")\n        if not os.path.exists(target_dir_path):\n            os.makedirs(target_dir_path)\n        now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n        with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n            json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n        \n        with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n            meta = {\n                \"repo_path\": self.repo_path,\n                \"doc_version\": self.document_version,\n                \"in_generation_process\": self.in_generation_process,\n            }\n            json.dump(meta, writer, indent=2, ensure_ascii=False)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "load_task_list": {
      "type": "FunctionDef",
      "name": "load_task_list",
      "md_content": [
        "**load_task_list**: The function of this Function is to load the task list by retrieving the topology and filtering out the items that are not up to date.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The code first calls the `get_topology()` method to retrieve the task list. Then, it uses a list comprehension to filter out the items in the task list that have a status of `DocItemStatus.doc_up_to_date`. The filtered task list is then returned.\n\n**Note**: The `get_topology()` method is not provided in the given code, so its behavior and implementation details are unknown. The `DocItemStatus` is also not defined in the given code, so its possible values and meanings are unclear.\n\n**Output Example**: \nIf the task list contains the following items:\n- Item 1 with status `DocItemStatus.doc_up_to_date`\n- Item 2 with status `DocItemStatus.doc_outdated`\n- Item 3 with status `DocItemStatus.doc_up_to_date`\n\nThe function will return a new list containing only Item 2:\n```\n[Item 2]\n```"
      ],
      "code_start_line": 248,
      "code_end_line": 250,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def load_task_list(self):\n        task_list = self.get_topology()\n        return [item for item in task_list if item.item_status != DocItemStatus.doc_up_to_date]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "print_task_list": {
      "type": "FunctionDef",
      "name": "print_task_list",
      "md_content": [
        "**print_task_list**: The function of this Function is to print a table of task information based on the given item list.\n**parameters**: \n- self: The reference to the current instance of the class.\n- item_list: A list of items containing task information.\n\n**Code Description**: \nThe function starts by importing the `PrettyTable` module, which is used to create a formatted table. It then initializes a `task_table` object of type `PrettyTable` with three columns: \"task_id\", \"Doc Generation Reason\", and \"Path\". \n\nNext, a variable `task_count` is initialized to 0. The function then iterates over each item in the `item_list` using the `enumerate()` function. For each item, a new row is added to the `task_table` with the task count, the item's status name, and its full name. The task count is incremented after each iteration.\n\nAfter adding all the rows to the `task_table`, the function prints the header \"Remain tasks to be done\" and then prints the `task_table` using the `print()` function.\n\n**Note**: \n- This function requires the `PrettyTable` module to be installed. Make sure to install it before using this function.\n- The `item_list` parameter should contain objects with attributes `item_status` and `get_full_name()`."
      ],
      "code_start_line": 252,
      "code_end_line": 260,
      "parent": "MetaInfo",
      "params": [
        "self",
        "item_list"
      ],
      "have_return": false,
      "code_content": "    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_all_files": {
      "type": "FunctionDef",
      "name": "get_all_files",
      "md_content": [
        "**get_all_files**: The function of this Function is to retrieve all file nodes from the target repository hierarchical tree.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: The code starts by initializing an empty list called \"files\". Then, it defines a nested function called \"walk_tree\" that takes a \"now_node\" parameter. Inside the \"walk_tree\" function, it checks if the \"now_node\" is a file node (DocItemType._file) and if so, it appends it to the \"files\" list. Next, it iterates over the children of the \"now_node\" and recursively calls the \"walk_tree\" function for each child. This recursive process continues until all nodes in the target repository hierarchical tree have been traversed. Finally, the code calls the \"walk_tree\" function with the \"target_repo_hierarchical_tree\" as the initial \"now_node\" to start the traversal. After traversing the entire tree, the code returns the \"files\" list containing all the file nodes.\n\n**Note**: It is important to note that this function only retrieves file nodes and does not include directory nodes or repository nodes in the returned list.\n\n**Output Example**: \nIf the target repository hierarchical tree contains the following file nodes:\n- File1\n- File2\n- File3\n\nThe function will return a list containing the file nodes:\n[File1, File2, File3]"
      ],
      "code_start_line": 262,
      "code_end_line": 271,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "walk_tree": {
      "type": "FunctionDef",
      "name": "walk_tree",
      "md_content": [
        "**walk_tree**: The function of this Function is to recursively traverse a tree structure and collect all the files in the tree.\n\n**parameters**: \n- now_node: The current node in the tree structure.\n\n**Code Description**:\nThe `walk_tree` function takes a `now_node` parameter, which represents the current node in the tree structure. It starts by checking if the `item_type` attribute of the `now_node` is equal to `DocItemType._file`. If it is, it appends the `now_node` to the `files` list.\n\nNext, it iterates over all the children of the `now_node` using the `items()` method. For each child, it recursively calls the `walk_tree` function passing the child as the new `now_node`. This recursive call allows the function to traverse the entire tree structure.\n\n**Note**: \n- The `walk_tree` function assumes that the `files` list is defined outside the function and is accessible within the scope of the function.\n- The `now_node` parameter should be an instance of a class that has an `item_type` attribute, which is used to determine if the node represents a file or not."
      ],
      "code_start_line": 265,
      "code_end_line": 269,
      "parent": "get_all_files",
      "params": [
        "now_node"
      ],
      "have_return": false,
      "code_content": "        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_obj_with_lineno": {
      "type": "FunctionDef",
      "name": "find_obj_with_lineno",
      "md_content": [
        "**find_obj_with_lineno**: The function of this Function is to find the object in the file node hierarchy that corresponds to a given line number.\n\n**parameters**: \n- self: The current instance of the class.\n- file_node: The root file node of the hierarchy.\n- start_line_num: The line number for which the corresponding object needs to be found.\n\n**Code Description**: \nThis function iterates through the file node hierarchy to find the object that corresponds to the given line number. It starts from the root file node and checks each child node to see if its code start line is less than or equal to the given line number. If a child node is found that satisfies this condition, the function updates the current node to be the child node and continues the search. If no child node is found that satisfies the condition, the function returns the current node.\n\n**Note**: \n- The file node hierarchy is expected to have a specific structure, where each node represents a file or a folder, and each node has a content attribute that contains information about the code start line.\n- The function assumes that the file node hierarchy is already built and available for traversal.\n\n**Output Example**: \nIf the given line number is 10 and the file node hierarchy has the following structure:\n- Root node\n  - Child node 1 (code start line: 5)\n    - Child node 1.1 (code start line: 8)\n    - Child node 1.2 (code start line: 12)\n  - Child node 2 (code start line: 15)\n\nThe function will return the child node 1.2, as its code start line (12) is the closest line number that is less than or equal to the given line number (10)."
      ],
      "code_start_line": 274,
      "code_end_line": 287,
      "parent": "MetaInfo",
      "params": [
        "self",
        "file_node",
        "start_line_num"
      ],
      "have_return": true,
      "code_content": "    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\"\"\"\n        now_node = file_node\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "parse_reference": {
      "type": "FunctionDef",
      "name": "parse_reference",
      "md_content": [
        "**parse_reference**: The function of this Function is to extract all bidirectional reference relationships.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function starts by getting all file nodes using the `get_all_files()` method. Then, it iterates through each file node and initializes a reference count variable. \n\nInside the loop, there is a nested function called `walk_file` which is responsible for traversing all variables within a file. This function takes a parameter `now_obj` of type `DocItem`, which represents the current object being processed.\n\nWithin the `walk_file` function, the `find_all_referencer` method is called to find all references to the `now_obj` variable. This method takes several parameters including the repository path, variable name, file path, line number, and column number. It returns a list of reference positions.\n\nFor each reference position in the `reference_list`, the function retrieves the file path and item from the hierarchical tree using the `find` method. It then finds the corresponding `referencer_node` using the `find_obj_with_lineno` method.\n\nNext, there is a check to ensure that there is no ancestor relationship between the current object (`now_obj`) and the `referencer_node`. If there is no ancestor relationship, the function proceeds to add the reference relationship between the two objects. It appends `now_obj` to the `reference_who` list of `referencer_node` and appends `referencer_node` to the `who_reference_me` list of `now_obj`.\n\nThe function also updates the `max_reference_ansce` attribute of `referencer_node` if necessary. It finds the minimum ancestor between `referencer_node` and `now_obj` using the `find_min_ances` method. If `referencer_node` does not have a `max_reference_ansce` set, it assigns the minimum ancestor. Otherwise, it checks if the minimum ancestor is already present in the `max_reference_ansce` tree path and updates it if necessary.\n\nFinally, the reference count is incremented for each reference found.\n\nThe `walk_file` function is then called recursively for each child object within the current file node.\n\nThe `walk_file` function is called for each child object within the file node.\n\n**Note**: This function is responsible for extracting bidirectional reference relationships between objects. It iterates through all files and their variables, finding references to each variable and establishing the reference relationships. The function also updates the `max_reference_ansce` attribute of the referencer node if necessary."
      ],
      "code_start_line": 291,
      "code_end_line": 337,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n        for file_node in file_nodes:\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"]\n                )\n                \n                for referencer_pos in reference_list:\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    # if now_obj.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n                    #     print(reference_list)\n                    #     print(referencer_node.get_full_name())\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_subtree_list": {
      "type": "FunctionDef",
      "name": "get_subtree_list",
      "md_content": [
        "**get_subtree_list**: The function of this Function is to retrieve a sorted list of DocItems in a subtree based on their topological reference relationships.\n\n**parameters**: \n- self: The instance of the MetaInfo class.\n- now_node: The current DocItem object for which the subtree list needs to be generated.\n\n**Code Description**: \nThe function first obtains a list of all DocItems in the subtree rooted at the given `now_node` by calling the `get_travel_list()` method on `now_node`. These DocItems represent the nodes in the subtree. \n\nNext, the function sorts the DocItems in the `items_by_depth` list based on their depth in the subtree. The depth of a DocItem is determined by the number of levels it is away from the root node.\n\nThe function then iterates over the `items_by_depth` list and checks if all the referenced DocItems of each item are already present in the `sorted_items` list. If all the referenced DocItems are present, the current item is added to the `sorted_items` list and removed from the `items_by_depth` list.\n\nAfter adding an item to the `sorted_items` list, the function checks if the item has a father (parent) node. If it does, it iterates over all the children of the father node and checks if they are already present in the `sorted_items` list. If all the children are present, the father node is added to the `sorted_items` list and removed from the `items_by_depth` list. This process is recursively applied to all ancestor nodes until the root node is reached.\n\nFinally, the function returns the `sorted_items` list, which represents the subtree sorted based on the topological reference relationships.\n\n**Note**: \n- The function assumes that the `now_node` parameter is a valid DocItem object.\n- The function does not consider any other factors for sorting the subtree, such as the distance between nodes.\n\n**Output Example**: \nA possible return value of the function could be a list of DocItems representing the subtree sorted based on the topological reference relationships. For example:\n```\n[\n    DocItem1,\n    DocItem2,\n    DocItem3,\n    ...\n]\n```"
      ],
      "code_start_line": 341,
      "code_end_line": 371,
      "parent": "MetaInfo",
      "params": [
        "self",
        "now_node"
      ],
      "have_return": true,
      "code_content": "    def get_subtree_list(self, now_node: DocItem) -> List[Any]:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        sorted_items = []\n        while items_by_depth:\n            for item in items_by_depth:\n                if all(referenced in sorted_items for referenced in item.reference_who):\n                    sorted_items.append(item)\n                    items_by_depth.remove(item)\n\n                    def check_father(item):\n                        nonlocal sorted_items\n                        nonlocal items_by_depth\n                        if item.father == None:\n                            return\n                        father_node = item.father\n                        for _,node in father_node.children.items():\n                            if node not in sorted_items:\n                                return\n                        #所有儿子都进去了，父亲也可以进去，并且应该挨着\n                        sorted_items.append(father_node)\n                        items_by_depth.remove(father_node)\n                        check_father(father_node)\n\n                    check_father(item)\n                    break\n\n        # Further optimization for minimizing tree distance could be added here\n        return sorted_items\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_topology": {
      "type": "FunctionDef",
      "name": "get_topology",
      "md_content": [
        "**get_topology**: The function of this Function is to calculate the topological order of all objects in the repository.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function first calls the `parse_reference()` method to parse the references between objects in the repository. Then, it calls the `get_subtree_list()` method, passing in the `target_repo_hierarchical_tree` as the parameter, to obtain a list of objects in the repository in a subtree order. Finally, it returns the `topology_list`, which represents the topological order of all objects in the repository.\n\n**Note**: It is important to note that this Function relies on the `parse_reference()` and `get_subtree_list()` methods to perform its calculations. Therefore, it is necessary to ensure that these methods are called before calling this Function.\n\n**Output Example**: A possible appearance of the return value of this Function is a list of `DocItem` objects representing the topological order of all objects in the repository."
      ],
      "code_start_line": 373,
      "code_end_line": 378,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_topology(self) -> List[DocItem]:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        topology_list = self.get_subtree_list(self.target_repo_hierarchical_tree)\n        return topology_list\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "_map": {
      "type": "FunctionDef",
      "name": "_map",
      "md_content": [
        "**_map**: The function of this Function is to apply a given function to all nodes in the hierarchical tree.\n\n**parameters**: \n- deal_func: A callable object that represents the function to be applied to each node in the tree.\n\n**Code Description**: \nThe `_map` function is used to apply a given function, `deal_func`, to all nodes in the hierarchical tree. It takes the `deal_func` as a parameter and defines an inner function called `travel`. The `travel` function is a recursive function that traverses the tree starting from the root node (`self.target_repo_hierarchical_tree`). \n\nInside the `travel` function, the `deal_func` is called with the current node (`now_item`) as an argument. This allows the function `deal_func` to perform some operation on the current node. \n\nAfter calling `deal_func`, the `travel` function recursively calls itself for each child node of the current node. This ensures that the given function is applied to all nodes in the tree.\n\n**Note**: \n- The `deal_func` should be a callable object, such as a function or a method, that takes a single argument representing a node in the tree.\n- The order in which the nodes are processed is not specified, as it depends on the structure of the tree and the order in which the child nodes are stored."
      ],
      "code_start_line": 380,
      "code_end_line": 386,
      "parent": "MetaInfo",
      "params": [
        "self",
        "deal_func"
      ],
      "have_return": false,
      "code_content": "    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "load_doc_from_older_meta": {
      "type": "FunctionDef",
      "name": "load_doc_from_older_meta",
      "md_content": [
        "**load_doc_from_older_meta**: The function of this Function is to merge the documentation from an older version of the meta info into the current version.\n\n**parameters**: \n- self: The instance of the class that the function belongs to.\n- older_meta: The older version of the meta info that contains the previously generated documentation.\n\n**Code Description**: \nThe function starts by logging an informational message indicating that the documentation is being merged from an older version of the meta info. It then retrieves the root item of the target repository hierarchical tree. \n\nThe function defines a nested function called \"find_item\" that takes a DocItem object as input and returns the corresponding DocItem object in the current version of the repository hierarchical tree. This function recursively searches for the item in the tree by traversing its hierarchy. If the item is found, it returns the corresponding DocItem object; otherwise, it returns None.\n\nNext, the function defines another nested function called \"travel\" that takes a DocItem object from the older meta info as input. This function recursively traverses the hierarchy of the older meta info and updates the corresponding DocItem objects in the current version of the repository hierarchical tree. It does this by calling the \"find_item\" function to find the corresponding DocItem object in the current version and then updates its markdown content and item status. If the code content of the older item is different from the code content of the corresponding item in the current version, the item status is set to \"code_changed\".\n\nAfter updating the documentation and item status of the corresponding items, the function calls the \"parse_reference\" method to parse the current bidirectional references and observe any changes in the references.\n\nThe function then defines another nested function called \"travel2\" that is similar to the \"travel\" function. This function traverses the hierarchy of the older meta info and checks if the references to the corresponding items in the current version have changed. It compares the new reference names with the old reference names and updates the item status accordingly. If the new reference names are a subset of the old reference names, it means that some references have been removed, and the item status is set to \"referencer_not_exist\". If the new reference names are different from the old reference names, it means that new references have been added, and the item status is set to \"add_new_referencer\".\n\nFinally, the function calls the \"get_subtree_list\" method to retrieve the subtree list of the current version of the repository hierarchical tree and returns it as the output of the function.\n\n**Note**: \n- The function assumes that the older meta info contains previously generated documentation.\n- The function relies on the \"find_item\" function to find the corresponding DocItem objects in the current version of the repository hierarchical tree.\n- The function updates the markdown content and item status of the corresponding DocItem objects based on the information from the older meta info.\n- The function checks if the code content of the older item is different from the code content of the corresponding item in the current version and updates the item status accordingly.\n- The function parses the current bidirectional references and checks if the references to the corresponding items in the current version have changed.\n- The function sets the item status to \"referencer_not_exist\" if some references have been removed and sets it to \"add_new_referencer\" if new references have been added.\n- The function returns the subtree list of the current version of the repository hierarchical tree.\n\n**Output Example**: \nThe output of the function is a list representing the subtree of the current version of the repository hierarchical tree."
      ],
      "code_start_line": 388,
      "code_end_line": 453,
      "parent": "MetaInfo",
      "params": [
        "self",
        "older_meta"
      ],
      "have_return": true,
      "code_content": "    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            nonlocal root_item\n            if now_item.father == None:\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status == DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的应用吧改了\"\"\"\n\n        self.parse_reference() \n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n            # if now_older_item.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n            #     print(now_older_item.get_full_name())\n            #     print(new_reference_names)\n            #     print(old_reference_names)\n            #     print(\"****\")\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n                # print(now_older_item.get_full_name())\n                # print(new_reference_names)\n                # print(old_reference_names)\n                # print(\"*******\")\n\n\n\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n\n        topology_list = self.get_subtree_list(self.target_repo_hierarchical_tree)\n        return topology_list\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "travel": {
      "type": "FunctionDef",
      "name": "travel",
      "md_content": [
        "**travel**: The function of this Function is to recursively update the metadata of a document item and its children based on the metadata of an older version of the item.\n\n**parameters**: \n- now_older_item: A DocItem object representing the metadata of the older version of the document item.\n\n**Code Description**:\nThe `travel` function takes in a `now_older_item` parameter, which is an instance of the `DocItem` class representing the metadata of an older version of a document item. The function recursively updates the metadata of the current version of the item and its children based on the metadata of the older version.\n\nThe function first calls the `find_item` function to search for the corresponding item in the current version of the document. If the item is not found, it means that the item has been removed in the new version, so the function returns without making any changes.\n\nIf the item is found, the function updates the `md_content` and `item_status` attributes of the current version of the item with the values from the older version. \n\nNext, the function checks if the `now_older_item` has a `code_content` attribute in its `content` dictionary. If it does, it asserts that the corresponding item in the current version also has a `code_content` attribute. If the `code_content` of the older version is different from the `code_content` of the current version, the `item_status` of the current version is set to `DocItemStatus.code_changed`.\n\nFinally, the function recursively calls itself for each child of the `now_older_item` to update their metadata as well.\n\n**Note**: \n- The `travel` function assumes that the `find_item` function is defined and returns the corresponding item in the current version of the document based on the metadata of the older version.\n- The `DocItem` class and `DocItemStatus` enum are assumed to be defined elsewhere in the code.\n\n**Output Example**:\n```\n# Mock up of a possible appearance of the code's return value\n\n# Before travel function is called\nnow_older_item:\n{\n    \"md_content\": \"Older version of the item's markdown content\",\n    \"item_status\": \"Older version of the item's status\",\n    \"content\": {\n        \"code_content\": \"Older version of the item's code content\"\n    },\n    \"children\": {\n        \"child1\": {\n            \"md_content\": \"Older version of child1's markdown content\",\n            \"item_status\": \"Older version of child1's status\",\n            \"content\": {},\n            \"children\": {}\n        },\n        \"child2\": {\n            \"md_content\": \"Older version of child2's markdown content\",\n            \"item_status\": \"Older version of child2's status\",\n            \"content\": {},\n            \"children\": {}\n        }\n    }\n}\n\n# After travel function is called\nnow_older_item:\n{\n    \"md_content\": \"Older version of the item's markdown content\",\n    \"item_status\": \"Older version of the item's status\",\n    \"content\": {\n        \"code_content\": \"Older version of the item's code content\"\n    },\n    \"children\": {\n        \"child1\": {\n            \"md_content\": \"Older version of child1's markdown content\",\n            \"item_status\": \"Older version of child1's status\",\n            \"content\": {},\n            \"children\": {}\n        },\n        \"child2\": {\n            \"md_content\": \"Older version of child2's markdown content\",\n            \"item_status\": \"Older version of child2's status\",\n            \"content\": {},\n            \"children\": {}\n        }\n    }\n}\n```"
      ],
      "code_start_line": 405,
      "code_end_line": 417,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status == DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_item": {
      "type": "FunctionDef",
      "name": "find_item",
      "md_content": [
        "**find_item**: The function of this Function is to find a specific item in a hierarchical structure of DocItems.\n\n**parameters**: \n- now_item: A DocItem object representing the current item being searched.\n- root_item: A DocItem object representing the root item of the hierarchical structure.\n\n**Code Description**: \nThe function `find_item` takes in the current item `now_item` and the root item `root_item` as parameters. It recursively searches for the specified item in the hierarchical structure of DocItems.\n\nThe function first checks if the current item has a father (i.e., if it is the root item). If it is the root item, it returns the root item itself.\n\nIf the current item is not the root item, it recursively calls the `find_item` function with the father of the current item as the new `now_item`. The result of this recursive call is stored in the `father_find_result` variable.\n\nIf the `father_find_result` is `None`, indicating that the specified item was not found in the father item, the function returns `None`.\n\nIf the specified item is found in the father item's children, the function returns the corresponding child item.\n\nIf the specified item is not found in the father item's children, the function returns `None`.\n\n**Note**: \n- The `nonlocal` keyword is used to indicate that the `root_item` variable is defined in an outer scope and should be modified within the function.\n- The function assumes that the hierarchical structure of DocItems is represented using the `father` and `children` attributes of each DocItem object.\n\n**Output Example**: \nIf the specified item is found in the hierarchical structure, the function returns the corresponding DocItem object. Otherwise, it returns `None`."
      ],
      "code_start_line": 393,
      "code_end_line": 402,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_item"
      ],
      "have_return": true,
      "code_content": "        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            nonlocal root_item\n            if now_item.father == None:\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "travel2": {
      "type": "FunctionDef",
      "name": "travel2",
      "md_content": [
        "**travel2**: The function of this Function is to recursively traverse the children of a given `DocItem` object and update the `item_status` attribute of each child based on changes in its referencing items.\n\n**parameters**: \n- `now_older_item` (type: `DocItem`): The `DocItem` object representing the current item being analyzed.\n\n**Code Description**:\nThe `travel2` function starts by finding the `result_item` by calling the `find_item` function with the `now_older_item` as the parameter. If the `result_item` is not found in the new version of the document, the function returns.\n\nNext, the function compares the referencing items of the `result_item` with the referencing items of the `now_older_item`. If there is a difference in the referencing items and the `item_status` of the `result_item` is `DocItemStatus.doc_up_to_date`, the function updates the `item_status` based on the following conditions:\n- If the new referencing items are a subset of the old referencing items, the `item_status` is set to `DocItemStatus.referencer_not_exist`.\n- Otherwise, the `item_status` is set to `DocItemStatus.add_new_referencer`.\n\nAfter updating the `item_status` of the `result_item`, the function recursively calls itself for each child of the `now_older_item`.\n\n**Note**: \n- The commented out code inside the function seems to be for debugging purposes and can be ignored.\n- The `DocItem` class and its attributes are not defined in the given code snippet, so it is assumed that they are defined elsewhere in the project.\n\n**Output Example**:\n```\n# Assuming the `now_older_item` has two children: child1 and child2\n# The `item_status` of child1 is `DocItemStatus.doc_up_to_date` and its referencing items have changed.\n# The `item_status` of child2 is `DocItemStatus.doc_up_to_date` and its referencing items have not changed.\n\n# After calling the `travel2` function:\nchild1.item_status = DocItemStatus.add_new_referencer\nchild2.item_status = DocItemStatus.doc_up_to_date\n```"
      ],
      "code_start_line": 423,
      "code_end_line": 449,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n            # if now_older_item.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n            #     print(now_older_item.get_full_name())\n            #     print(new_reference_names)\n            #     print(old_reference_names)\n            #     print(\"****\")\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n                # print(now_older_item.get_full_name())\n                # print(new_reference_names)\n                # print(old_reference_names)\n                # print(\"*******\")\n\n\n\n            for _, child in now_older_item.children.items():\n                travel2(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "from_project_hierarchy_path": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_path",
      "md_content": [
        "**from_project_hierarchy_path**: The function of this Function is to parse a project hierarchy JSON file and convert it into a MetaInfo object.\n\n**parameters**: \n- repo_path (str): The path to the repository.\n\n**Code Description**:\nThe `from_project_hierarchy_path` function takes a repository path as input and constructs the path to the project hierarchy JSON file. It then checks if the JSON file exists. If the file does not exist, it raises a `NotImplementedError` with a message. \n\nIf the file exists, it opens the JSON file in read mode and loads its contents using the `json.load` function. The loaded JSON data is stored in the `project_hierarchy_json` variable.\n\nFinally, the function calls the `from_project_hierarchy_json` method of the `MetaInfo` class, passing the `project_hierarchy_json` as an argument. The `from_project_hierarchy_json` method is responsible for converting the JSON data into a MetaInfo object.\n\n**Note**: \n- The project hierarchy JSON file should be named \".project_hierarchy.json\" and located in the specified repository path.\n- The `MetaInfo` class should have a `from_project_hierarchy_json` method implemented to handle the conversion from JSON to MetaInfo object.\n\n**Output Example**:\nA possible appearance of the code's return value is an instance of the `MetaInfo` class, which represents the parsed project hierarchy information."
      ],
      "code_start_line": 456,
      "code_end_line": 466,
      "parent": "MetaInfo",
      "params": [
        "repo_path"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "to_hierarchy_json": {
      "type": "FunctionDef",
      "name": "to_hierarchy_json",
      "md_content": [
        "**to_hierarchy_json**: The function of this Function is to convert the hierarchy of files and their metadata into a JSON format.\n\n**parameters**: \n- flash_reference_relation (optional): A boolean value indicating whether to include the flash reference relation in the JSON output. Default is False.\n\n**Code Description**: \nThe `to_hierarchy_json` function takes the current object and converts the hierarchy of files and their metadata into a JSON format. It starts by initializing an empty dictionary `hierachy_json` to store the hierarchy information. Then, it retrieves a list of all file items using the `get_all_files` function.\n\nNext, it iterates over each file item in the `file_item_list` and creates a nested dictionary `file_hierarchy_content` to store the metadata of each file item. The `walk_file` function is defined inside the loop to recursively traverse the hierarchy and populate the `file_hierarchy_content` dictionary.\n\nWithin the `walk_file` function, the metadata of the current file item `now_obj` is added to the `file_hierarchy_content` dictionary. The metadata includes the object name, item type, markdown content, item status, and parent information. If the `flash_reference_relation` parameter is set to True, the function also includes the references to and from the current file item.\n\nThe `walk_file` function then recursively calls itself for each child of the current file item, ensuring that the entire hierarchy is traversed.\n\nAfter the `walk_file` function completes, the `file_hierarchy_content` dictionary is added to the `hierachy_json` dictionary with the file item's full name as the key.\n\nFinally, the `hierachy_json` dictionary, containing the hierarchy information of all file items, is returned as the output of the function.\n\n**Note**: \n- The `flash_reference_relation` parameter is optional and defaults to False. Set it to True if you want to include the flash reference relation in the JSON output.\n- The function assumes that the current object has a `get_all_files` function to retrieve a list of all file items.\n- The function assumes that the current object has a `DocItem` class with attributes such as `obj_name`, `content`, `item_type`, `md_content`, `item_status`, `who_reference_me`, `reference_who`, `father`, and `children`.\n\n**Output Example**: \n{\n  \"file_item_1\": {\n    \"file_item_1\": {\n      \"name\": \"file_item_1\",\n      \"type\": \"file\",\n      \"md_content\": \"This is the markdown content of file_item_1.\",\n      \"item_status\": \"active\",\n      \"parent\": null\n    },\n    \"file_item_2\": {\n      \"name\": \"file_item_2\",\n      \"type\": \"file\",\n      \"md_content\": \"This is the markdown content of file_item_2.\",\n      \"item_status\": \"active\",\n      \"parent\": \"file_item_1\"\n    }\n  },\n  \"file_item_3\": {\n    \"file_item_3\": {\n      \"name\": \"file_item_3\",\n      \"type\": \"file\",\n      \"md_content\": \"This is the markdown content of file_item_3.\",\n      \"item_status\": \"active\",\n      \"parent\": null\n    }\n  }\n}"
      ],
      "code_start_line": 468,
      "code_end_line": 496,
      "parent": "MetaInfo",
      "params": [
        "self",
        "flash_reference_relation"
      ],
      "have_return": true,
      "code_content": "    def to_hierarchy_json(self, flash_reference_relation = False):\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "from_project_hierarchy_json": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_json",
      "md_content": [
        "**from_project_hierarchy_json**: The function of this Function is to parse a project hierarchy JSON and generate a MetaInfo object representing the hierarchical structure of the project.\n\n**parameters**: \n- project_hierarchy_json: A JSON object representing the project hierarchy. It contains the file names and their corresponding content.\n\n**Code Description**:\nThe function starts by creating a target_meta_info object of type MetaInfo, which will store the hierarchical structure of the project. The root node of the hierarchy is represented by the target_repo_hierarchical_tree attribute of the target_meta_info object.\n\nNext, the function iterates over the file names and their content in the project_hierarchy_json. For each file, it checks if the file exists and is not empty. If the file is deleted or blank, it skips the file and continues to the next one.\n\nThe function then splits the file name into a list of directories and file name components. It iterates over the components to create the hierarchical structure in the target_meta_info object. For each component, it checks if it already exists in the current level of the hierarchy. If it doesn't exist, it creates a new DocItem object representing a directory or file, depending on whether it is the last component or not. The newly created DocItem object is added as a child of the current level in the hierarchy. The function also sets the father attribute of the child DocItem object to the current level.\n\nAfter creating the hierarchical structure, the function parses the content of each file. It asserts that the content is of type dict. It then iterates over the key-value pairs in the content. For each pair, it calls the parse_one_item function to recursively parse the item and its parent items. The parse_one_item function creates a new DocItem object representing the item and sets its attributes based on the values in the content. It also updates the hierarchy by adding the item as a child of its parent item.\n\nFinally, the function calls the parse_tree_path and check_depth methods of the target_repo_hierarchical_tree to complete the parsing of the hierarchy. It returns the target_meta_info object representing the project hierarchy.\n\n**Note**: \n- The function assumes that the project hierarchy JSON is valid and follows a specific structure.\n- The function relies on the MetaInfo, DocItem, and DocItemType classes defined in the same module.\n- The function uses the logger object for logging information about deleted or blank files.\n\n**Output Example**:\nA possible appearance of the return value of the function is as follows:\n```\n{\n  \"target_repo_hierarchical_tree\": {\n    \"item_type\": \"_repo\",\n    \"obj_name\": \"full_repo\",\n    \"children\": {\n      \"display\": {\n        \"item_type\": \"_dir\",\n        \"md_content\": \"\",\n        \"obj_name\": \"display\",\n        \"father\": {\n          \"item_type\": \"_repo\",\n          \"obj_name\": \"full_repo\",\n          \"children\": {\n            \"display\": {\n              \"item_type\": \"_dir\",\n              \"md_content\": \"\",\n              \"obj_name\": \"display\",\n              \"father\": {\n                \"item_type\": \"_repo\",\n                \"obj_name\": \"full_repo\",\n                \"children\": {\n                  ...\n                }\n              }\n            },\n            ...\n          }\n        },\n        \"children\": {\n          \"book_template\": {\n            \"item_type\": \"_dir\",\n            \"md_content\": \"\",\n            \"obj_name\": \"book_template\",\n            \"father\": {\n              \"item_type\": \"_dir\",\n              \"md_content\": \"\",\n              \"obj_name\": \"display\",\n              \"father\": {\n                \"item_type\": \"_repo\",\n                \"obj_name\": \"full_repo\",\n                \"children\": {\n                  ...\n                }\n              }\n            },\n            \"children\": {\n              ...\n            }\n          },\n          ...\n        }\n      },\n      ...\n    }\n  }\n}\n```"
      ],
      "code_start_line": 499,
      "code_end_line": 588,
      "parent": "MetaInfo",
      "params": [
        "project_hierarchy_json"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "walk_file": {
      "type": "FunctionDef",
      "name": "walk_file",
      "md_content": [
        "**walk_file**: The function of this Function is to recursively traverse the file hierarchy and collect information about each file or folder.\n\n**parameters**: \n- now_obj: A DocItem object representing the current file or folder in the file hierarchy.\n\n**Code Description**:\nThe function starts by updating the file_hierarchy_content dictionary with information about the current file or folder. It adds the name, type, markdown content, and item status of the current object to the dictionary. If flash_reference_relation is True, it also adds the list of objects that reference the current object and the list of objects that the current object references.\n\nNext, it checks if the current object has a parent. If the parent is not a file, it sets the parent of the current object in the file_hierarchy_content dictionary.\n\nThen, the function recursively calls itself for each child of the current object, passing the child as the new current object.\n\n**Note**: \n- This function assumes that the file_hierarchy_content dictionary and flash_reference_relation variable are defined and accessible.\n- The function modifies the file_hierarchy_content dictionary to store information about each file or folder in the file hierarchy."
      ],
      "code_start_line": 474,
      "code_end_line": 491,
      "parent": null,
      "params": [
        "now_obj"
      ],
      "have_return": false,
      "code_content": "            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "parse_one_item": {
      "type": "FunctionDef",
      "name": "parse_one_item",
      "md_content": [
        "**parse_one_item**: The function of this Function is to parse and process a single item in the doc_meta_info file.\n\n**parameters**: \n- key (str): The key of the item being parsed.\n- value (dict): The dictionary containing the information of the item.\n- item_reflection (dict): The dictionary storing the parsed items.\n\n**Code Description**: \nThe `parse_one_item` function is responsible for parsing and processing a single item in the `doc_meta_info` file. It takes in the key, value, and item_reflection as parameters. The key represents the unique identifier of the item, while the value is a dictionary containing various information about the item. The item_reflection is a dictionary that stores the parsed items.\n\nThe function starts by checking if the key already exists in the item_reflection dictionary. If it does, it means that the item has already been parsed, so the function returns without doing anything else.\n\nNext, the function checks if the item has a parent. If it does, it recursively calls the `parse_one_item` function to parse the parent item first. This ensures that the parent item is parsed before the current item.\n\nAfter that, the function creates a new DocItem object and assigns it to the item_reflection dictionary using the key as the key and the value dictionary as the content. The DocItem object is initialized with the obj_name set to the key and the md_content set to the value's \"md_content\" field.\n\nThe function then checks if the value dictionary contains the \"item_status\" field. If it does, it assigns the corresponding DocItemStatus value to the item_status field of the DocItem object.\n\nSimilarly, the function checks if the value dictionary contains the \"reference_who\" and \"who_reference_me\" fields. If they exist, it assigns the corresponding values to the reference_who_name_list and who_reference_me_name_list fields of the DocItem object.\n\nNext, the function checks if the item has a parent. If it does, it assigns the current item as a child of the parent item in the item_reflection dictionary. It also sets the father field of the current item to the parent item.\n\nIf the item does not have a parent, it means that it is a top-level item in the file. In this case, it assigns the current item as a child of the file_item in the item_reflection dictionary. It also sets the father field of the current item to the file_item.\n\nFinally, the function determines the item_type of the DocItem object based on the value's \"type\" field. If the type is \"ClassDef\", it sets the item_type to DocItemType._class. If the type is \"FunctionDef\", it sets the item_type to DocItemType._function. If the item has a parent and the parent's type is \"FunctionDef\", it sets the item_type to DocItemType._sub_function. If the parent's type is \"ClassDef\", it sets the item_type to DocItemType._class_function.\n\n**Note**: \n- This function assumes that the file_content dictionary is accessible and contains the necessary information for parsing.\n- The function relies on the DocItem, DocItemStatus, and DocItemType classes to store and represent the parsed items.\n\n**Output Example**: \n```\n{\n    \"key\": {\n        \"obj_name\": \"key\",\n        \"content\": {\n            // item content\n        },\n        \"md_content\": \"item markdown content\",\n        \"item_status\": \"DocItemStatus\",\n        \"reference_who_name_list\": [\"reference_who\"],\n        \"who_reference_me_name_list\": [\"who_reference_me\"],\n        \"children\": {\n            // child items\n        },\n        \"father\": \"parent_item\",\n        \"item_type\": \"DocItemType\"\n    }\n}\n```"
      ],
      "code_start_line": 543,
      "code_end_line": 579,
      "parent": null,
      "params": [
        "key",
        "value",
        "item_reflection"
      ],
      "have_return": true,
      "code_content": "            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "check_father": {
      "type": "FunctionDef",
      "name": "check_father",
      "md_content": [
        "**check_father**: The function of this Function is to check if all the children of a given item have been sorted and added to the sorted_items list. If so, it adds the item's father to the sorted_items list and removes it from the items_by_depth list.\n\n**parameters**: \n- item: The item to check its father's children.\n\n**Code Description**:\n- The function first checks if the given item has a father. If it doesn't, it means the item is a root node and there is no need to check its father's children. In this case, the function returns.\n- If the item has a father, the function assigns the father node to the variable `father_node`.\n- The function then iterates over each child node of the father node. For each child node, it checks if it is already present in the `sorted_items` list. If any child node is not present in the `sorted_items` list, it means that not all children have been sorted yet. In this case, the function returns.\n- If all the children have been sorted and added to the `sorted_items` list, the function adds the father node to the `sorted_items` list.\n- Finally, the function removes the father node from the `items_by_depth` list.\n- The function then recursively calls itself with the father node as the new item to check its father's children. This process continues until there are no more fathers to check.\n\n**Note**: \n- This function assumes that the `sorted_items` and `items_by_depth` lists are defined and accessible within the scope of the function.\n- The function uses the `nonlocal` keyword to indicate that the `sorted_items` and `items_by_depth` variables are not local to the function but are defined in an outer scope.\n\n**Output Example**: \n- If all the children of the given item and its ancestors have been sorted, the function will add the item's father to the `sorted_items` list and remove it from the `items_by_depth` list."
      ],
      "code_start_line": 353,
      "code_end_line": 365,
      "parent": null,
      "params": [
        "item"
      ],
      "have_return": true,
      "code_content": "                    def check_father(item):\n                        nonlocal sorted_items\n                        nonlocal items_by_depth\n                        if item.father == None:\n                            return\n                        father_node = item.father\n                        for _,node in father_node.children.items():\n                            if node not in sorted_items:\n                                return\n                        #所有儿子都进去了，父亲也可以进去，并且应该挨着\n                        sorted_items.append(father_node)\n                        items_by_depth.remove(father_node)\n                        check_father(father_node)\n",
      "name_column": 24,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    }
  },
  "repo_agent/chat_engine.py": {
    "get_import_statements": {
      "type": "FunctionDef",
      "name": "get_import_statements",
      "md_content": [
        "**get_import_statements**: The function of this function is to retrieve the import statements from the source code.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The `get_import_statements` function uses the `inspect.getsourcelines` function to retrieve the source code lines of the current module. It then filters the source lines to find the lines that start with either 'import' or 'from'. These lines are considered import statements. The function returns a list of import lines.\n\n**Note**: This function assumes that it is being called from within the module for which the import statements need to be retrieved.\n\n**Output Example**: If the source code contains the following import statements:\n```\nimport sys\nfrom os import path\n```\nThe function will return the following list:\n```\n['import sys', 'from os import path']\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 19,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/chat_engine.py/build_path_tree",
        "repo_agent/chat_engine.py/build_path_tree/tree",
        "repo_agent/chat_engine.py/build_path_tree/tree_to_string",
        "repo_agent/chat_engine.py/ChatEngine/num_tokens_from_string",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referenced_prompt",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referencer_prompt"
      ]
    },
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [
        "**build_path_tree**: The function of this function is to build a hierarchical tree structure based on the given paths.\n\n**parameters**: \n- who_reference_me: A list of paths that reference the current module.\n- reference_who: A list of paths that the current module references.\n- doc_item_path: The path of the current module.\n\n**Code Description**: \nThe `build_path_tree` function first defines a nested function called `tree`, which returns a `defaultdict` object. This `defaultdict` object is used to create a tree-like structure with default values as nested dictionaries.\n\nThe function then initializes a `path_tree` variable by calling the `tree` function.\n\nNext, the function iterates over the `who_reference_me` and `reference_who` lists. For each path in these lists, the function splits the path into individual parts using the `split` method with the `os.sep` separator. It then traverses the `path_tree` by updating the `node` variable to the nested dictionary corresponding to each part of the path. This process effectively builds the hierarchical tree structure.\n\nAfter that, the function processes the `doc_item_path` by splitting it into parts and adding a special character ('✳️') before the last part. It then traverses the `path_tree` again to update the `node` variable to the nested dictionary corresponding to each part of the modified `doc_item_path`.\n\nThe function also defines a nested function called `tree_to_string`, which recursively converts the `path_tree` into a string representation. This function takes an optional `indent` parameter to control the indentation level of each tree node. It iterates over the sorted items of the `tree` dictionary and appends each key to the string representation. If the corresponding value is another dictionary, the function recursively calls itself with an increased indentation level.\n\nFinally, the function returns the string representation of the `path_tree` by calling the `tree_to_string` function.\n\n**Note**: \n- This function relies on the `os` module, so make sure to import it before using this function.\n- The `build_path_tree` function assumes that the `os.sep` separator is used to separate the parts of the paths.\n- The `build_path_tree` function assumes that the `defaultdict` class is available from the `collections` module.\n\n**Output Example**: \nIf the `who_reference_me` list contains `['repo_agent/chat_engine.py', 'repo_agent/utils.py']`, the `reference_who` list contains `['repo_agent/chat_engine.py/build_path_tree', 'repo_agent/utils.py']`, and the `doc_item_path` is `'repo_agent/chat_engine.py/build_path_tree'`, the function will return the following string representation of the `path_tree`:\n\n```\nrepo_agent\n    chat_engine.py\n        build_path_tree\n    utils.py\n```"
      ],
      "code_start_line": 21,
      "code_end_line": 48,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [
        "**tree**: The function of this Function is to create a nested defaultdict object.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The `tree` function creates a defaultdict object with the default factory set to `tree`. A defaultdict is a subclass of the built-in dict class that provides a default value for missing keys. In this case, the default value is another defaultdict with the default factory set to `tree`. This creates a nested defaultdict structure that can be used to represent a tree-like data structure.\n\nThe function returns the created defaultdict object.\n\n**Note**: The `tree` function does not require any parameters and can be called directly to create a nested defaultdict object.\n\n**Output Example**: If the `tree` function is called, it will return a defaultdict object with the default factory set to `tree`. This object can be used to represent a tree-like data structure."
      ],
      "code_start_line": 22,
      "code_end_line": 23,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [
        "**tree_to_string**: The function of this function is to convert a nested dictionary representing a tree structure into a string representation. It recursively traverses the tree and appends each key to the string, indented according to its level in the tree.\n\n**parameters**: \n- tree: A nested dictionary representing a tree structure.\n- indent (optional): An integer representing the current level of indentation. Default value is 0.\n\n**Code Description**: \nThe `tree_to_string` function takes a nested dictionary `tree` and an optional `indent` parameter. It initializes an empty string `s` to store the string representation of the tree.\n\nThe function then iterates over the items of the `tree` dictionary, sorted by key. For each key-value pair, it appends the key to the string `s`, indented by `indent` number of spaces. \n\nIf the value corresponding to the key is another dictionary, the function recursively calls itself with the value as the new `tree` and increments the `indent` by 1. This ensures that the nested dictionaries are also converted to string representation with the appropriate indentation.\n\nFinally, the function returns the string representation of the tree.\n\n**Note**: \n- The function assumes that the input `tree` is a nested dictionary representing a tree structure.\n- The `indent` parameter is used to control the level of indentation in the string representation. It is optional and has a default value of 0.\n- The function uses a depth-first traversal approach to convert the tree to a string representation.\n\n**Output Example**: \nIf the `tree` parameter is the following nested dictionary:\n```\n{\n    'A': {\n        'B': {\n            'C': {}\n        },\n        'D': {}\n    },\n    'E': {}\n}\n```\nThe function will return the following string:\n```\nA\n    B\n        C\n    D\nE\n```"
      ],
      "code_start_line": 40,
      "code_end_line": 46,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "ChatEngine": {
      "type": "ClassDef",
      "name": "ChatEngine",
      "md_content": [
        "**ChatEngine**: The function of this Class is to generate the documentation for functions or classes.\n\n**attributes**: This Class has one attribute:\n- `config`: It stores the configuration settings.\n\n**Code Description**: The `ChatEngine` Class has the following methods:\n\n1. `__init__(self, CONFIG)`: This method is the constructor of the `ChatEngine` Class. It initializes the `config` attribute with the provided `CONFIG` parameter.\n\n2. `num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int`: This method takes a text string as input and returns the number of tokens in the string. It uses the `tiktoken` library to encode the string and then calculates the length of the encoded tokens.\n\n3. `generate_doc(self, doc_item: DocItem, file_handler)`: This method generates the documentation for a given `doc_item` object. It takes the `doc_item` and `file_handler` as parameters. It extracts the necessary information from the `doc_item` object, such as the code type, name, content, and whether it has a return value. It also checks if the code has been referenced by other objects or if it references other objects. It then constructs a system prompt and a user prompt based on the extracted information. The system prompt includes the project structure, code type, code name, code content, and whether it has a return value. The user prompt includes the desired language for the documentation. The method then uses the OpenAI GPT-3.5 Turbo model to generate the documentation based on the prompts. It makes use of the OpenAI API to send the prompts and receive the generated documentation. The generated documentation is returned as the output.\n\n**Note**: The code makes use of the `ProjectManager` class from the `project_manager` module to get the project structure. It also uses the `tiktoken` library to encode the text string and calculate the number of tokens. The documentation generation process involves sending prompts to the OpenAI GPT-3.5 Turbo model using the OpenAI API.\n\n**Output Example**: Mock up a possible appearance of the code's return value."
      ],
      "code_start_line": 50,
      "code_end_line": 281,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChatEngine:\n    \"\"\"\n    ChatEngine is used to generate the doc of functions or classes.\n    \"\"\"\n    def __init__(self, CONFIG):\n        self.config = CONFIG\n\n    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem):\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # 把sys_prompt写入一个txt文件\n        with open(f'prompt_{code_name}.txt', 'w', encoding='utf-8') as f:\n            f.write(sys_prompt + '\\n' + usr_prompt)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=project_structure_prefix,\n                        project_structure=project_structure, \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=combine_ref_situation, \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            reference_letter=reference_letter, \n                            referencer_content=referencer_content,\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of this Function is to initialize an instance of the ChatEngine class.\n\n**parameters**: \n- CONFIG: A configuration object that contains the settings for the chat engine.\n\n**Code Description**: \nThe `__init__` function is the constructor method of the ChatEngine class. It is called when a new instance of the class is created. The function takes a single parameter `CONFIG`, which is an object that contains the configuration settings for the chat engine.\n\nInside the function, the `CONFIG` object is assigned to the `config` attribute of the instance using the `self` keyword. This allows the configuration settings to be accessed by other methods within the class.\n\n**Note**: \n- The `__init__` function is automatically called when a new instance of the ChatEngine class is created.\n- The `CONFIG` parameter should be an object that contains the necessary configuration settings for the chat engine."
      ],
      "code_start_line": 54,
      "code_end_line": 55,
      "parent": "ChatEngine",
      "params": [
        "self",
        "CONFIG"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, CONFIG):\n        self.config = CONFIG\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "num_tokens_from_string": {
      "type": "FunctionDef",
      "name": "num_tokens_from_string",
      "md_content": [
        "**num_tokens_from_string**: The function of this Function is to return the number of tokens in a text string.\n**parameters**: \n- string: A string representing the text for which the number of tokens needs to be calculated.\n- encoding_name (optional): A string representing the name of the encoding to be used. The default value is \"cl100k_base\".\n\n**Code Description**: \nThe `num_tokens_from_string` function takes a text string and an optional encoding name as input parameters. It uses the `tiktoken.get_encoding` function to retrieve the encoding based on the provided encoding name. The function then encodes the input string using the retrieved encoding and calculates the number of tokens in the encoded string using the `len` function. Finally, it returns the number of tokens as an integer.\n\n**Note**: \n- The encoding name parameter is optional and has a default value of \"cl100k_base\". If no encoding name is provided, the function will use the default encoding.\n- The `tiktoken.get_encoding` function is assumed to be defined elsewhere in the codebase.\n\n**Output Example**: \nIf the input string is \"Hello, world!\", and the encoding name is \"cl100k_base\", the function will return the number of tokens in the encoded string, which could be 3."
      ],
      "code_start_line": 57,
      "code_end_line": 61,
      "parent": "ChatEngine",
      "params": [
        "self",
        "string",
        "encoding_name"
      ],
      "have_return": true,
      "code_content": "    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "generate_doc": {
      "type": "FunctionDef",
      "name": "generate_doc",
      "md_content": [
        "**generate_doc**: The function of this Function is to generate a detailed explanation document for a given object based on its code content and combine it with its calling situation in the project.\n\n**parameters**: \n- `doc_item` (DocItem): The DocItem object representing the target object for which the documentation needs to be generated.\n- `file_handler`: The FileHandler object used to handle the file operations.\n\n**Code Description**: \nThe `generate_doc` function takes in a `doc_item` object and a `file_handler` object as parameters. It first extracts the necessary information from the `doc_item` object, such as the code type, code name, code content, and whether the code has a return value. It also checks if the `doc_item` object has any references or is referenced by other objects.\n\nNext, it retrieves the project structure using the `ProjectManager` class and prepares the necessary prompts for the OpenAI GPT-3 chat-based language model. It constructs a system prompt that includes information about the code type, code name, code content, whether it has a return value, and the project structure. It also includes prompts for the references and referencers of the `doc_item` object.\n\nThe function then makes a request to the OpenAI API using the configured language model. It sends the system prompt and a user prompt to generate a response. The response contains the generated documentation for the `doc_item` object.\n\n**Note**: \n- The function uses the OpenAI GPT-3 chat-based language model to generate the documentation.\n- The function handles potential errors, such as connection errors and exceeding the model's maximum context length.\n- The function limits the code length to ensure it fits within the model's maximum token limit.\n\n**Output Example**: \nMock up a possible appearance of the code's return value."
      ],
      "code_start_line": 63,
      "code_end_line": 281,
      "parent": "ChatEngine",
      "params": [
        "self",
        "doc_item",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem):\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # 把sys_prompt写入一个txt文件\n        with open(f'prompt_{code_name}.txt', 'w', encoding='utf-8') as f:\n            f.write(sys_prompt + '\\n' + usr_prompt)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=project_structure_prefix,\n                        project_structure=project_structure, \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=combine_ref_situation, \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            reference_letter=reference_letter, \n                            referencer_content=referencer_content,\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_referenced_prompt": {
      "type": "FunctionDef",
      "name": "get_referenced_prompt",
      "md_content": [
        "**get_referenced_prompt**: The function of this Function is to generate a prompt that displays the objects called by the code.\n\n**parameters**: \n- doc_item: A DocItem object that contains information about the code item.\n\n**Code Description**: \nThe `get_referenced_prompt` function takes a `DocItem` object as input and returns a string prompt that displays the objects called by the code. \n\nThe function first checks if the `doc_item` has any referenced objects. If there are no referenced objects, an empty string is returned.\n\nIf there are referenced objects, the function creates a list called `prompt` to store the prompt lines. The initial line of the prompt is a general description of the code calling the objects.\n\nThen, the function iterates over each referenced object in the `doc_item.reference_who` list. For each referenced object, it generates a prompt line that includes the object's name, document, and raw code. The object's name is obtained using the `get_full_name` method of the `reference_item` object. The document is extracted from the `md_content` attribute of the `reference_item` object, and if it is empty, the prompt displays \"None\". The raw code is obtained from the `code_content` key in the `content` attribute of the `reference_item` object, and if it is not present, an empty string is displayed.\n\nEach prompt line is then appended to the `prompt` list, followed by a line of equal signs as a separator.\n\nFinally, the function joins all the prompt lines in the `prompt` list with newline characters and returns the resulting string.\n\n**Note**: \n- This function assumes that the `doc_item` object has a `reference_who` attribute that is a list of `DocItem` objects representing the referenced objects.\n- The prompt generated by this function is intended to provide information about the objects called by the code, including their names, documents, and raw code.\n\n**Output Example**: \nAs you can see, the code calls the following objects, their code and docs are as following:\nobj: repo_agent/chat_engine.py/get_import_statements\nDocument: None\nRaw code:\n```\ndef get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n```\n=========="
      ],
      "code_start_line": 110,
      "code_end_line": 117,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "get_referencer_prompt": {
      "type": "FunctionDef",
      "name": "get_referencer_prompt",
      "md_content": [
        "**get_referencer_prompt**: The function of this Function is to generate a prompt that displays the objects that reference the given `doc_item`.\n\n**parameters**: \n- `doc_item` (type: DocItem): The `DocItem` object for which the referencers are to be displayed.\n\n**Code Description**: \nThe `get_referencer_prompt` function first checks if the `doc_item` has any objects that reference it. If there are no referencers, an empty string is returned.\n\nIf there are referencers, a prompt is generated to display the objects that reference the `doc_item`. The prompt starts with a header message indicating that the code has been referenced by the following objects.\n\nThen, for each referencer, the function generates a prompt containing the following information:\n- Object name (`obj`): The full name of the referencer object.\n- Document (`Document`): The last line of the Markdown content of the referencer object, if available. Otherwise, it displays \"None\".\n- Raw code: The code content of the referencer object, enclosed in triple backticks. If the code content is not available, it displays \"None\".\n- Divider: A line of equal signs (`=`) to separate each referencer prompt.\n\nThe function appends each referencer prompt to a list of prompts. Finally, it joins all the prompts with newline characters and returns the generated prompt.\n\n**Note**: \n- The `doc_item` is an object of the `DocItem` class, which contains information about a specific code item, such as its name, type, content, and references.\n- The generated prompt provides information about the objects that reference the `doc_item`, including their names, documentation, and code content.\n- If the referencer object does not have any Markdown content or code content, it is indicated as \"None\" in the prompt.\n- The prompt uses triple backticks to enclose the raw code content for better readability.\n\n**Output Example**: \nAlso, the code has been referenced by the following objects, their code and docs are as following:\nobj: repo_agent/chat_engine.py/get_import_statements\nDocument: None\nRaw code:\n```\ndef get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n```\n=========="
      ],
      "code_start_line": 120,
      "code_end_line": 127,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referencer_prompt(doc_item: DocItem):\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    }
  },
  "repo_agent/prompt.py": {},
  "repo_agent/change_detector.py": {
    "ChangeDetector": {
      "type": "ClassDef",
      "name": "ChangeDetector",
      "md_content": [
        "**ChangeDetector**: The function of this Class is to detect changes in a repository and extract information about the modified files and their content.\n\n**Attributes**:\n- repo_path (str): The path to the repository.\n- repo (git.Repo): The Git repository object.\n\n**Code Description**:\nThe `ChangeDetector` class provides methods to detect changes in a repository and extract information about the modified files and their content. It utilizes the GitPython library to interact with the Git repository.\n\nThe `__init__` method initializes a `ChangeDetector` object by setting the `repo_path` attribute and creating a `git.Repo` object for the repository.\n\nThe `get_staged_pys` method retrieves the added Python files in the repository that have been staged. It uses the GitPython library to compare the staging area (index) with the original HEAD commit to identify the newly added files. The method returns a dictionary where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n\nThe `get_file_diff` method retrieves the changes made to a specific file. For new files, it adds them to the staging area and uses the `git diff --staged` command to get the differences. For non-new files, it uses the `git diff HEAD` command to get the differences. The method returns a list of changes made to the file.\n\nThe `parse_diffs` method parses the difference content obtained from the `get_file_diff` method. It extracts the added and deleted object information, such as classes or functions, from the differences. The method returns a dictionary containing the added and deleted line information.\n\nThe `identify_changes_in_structure` method identifies the structures (functions or classes) where changes have occurred. It traverses all changed lines and checks whether each line is within the start and end lines of a structure. If a line is within a structure, the structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary. The method returns a dictionary containing the structures where changes have occurred.\n\nThe `get_to_be_staged_files` method retrieves all unstaged files in the repository that meet certain conditions. It checks if a file, when its extension is changed to `.md`, corresponds to a file that is already staged. It also checks if the file's path is the same as the 'project_hierarchy' field in the CONFIG. The method returns a list of the paths of these files.\n\nThe `add_unstaged_files` method adds the unstaged files that meet the conditions to the staging area. It uses the `git add` command to add the files.\n\n**Note**: The `identify_changes_in_structure` method has a TODO comment indicating that there may be issues with the current implementation. It suggests building a mapping to associate changed line numbers with their function or class names before processing the changed lines.\n\n**Output Example**:\n```\n{\n    'added': [\n        (86, '    '),\n        (87, '    def to_json_new(self, comments = True):'),\n        (88, '        data = {'),\n        (89, '            \"name\": self.node_name,'),\n        ...\n        (95, '')\n    ],\n    'removed': []\n}\n```\n\nThis is an example output of the `parse_diffs` method. It shows the added lines and their line numbers. In this example, the PipelineEngine and AI_give_params are added objects, and there are no removed objects. However, it is important to note that the addition here does not necessarily mean that an object is newly added. The Git diff representation may show modifications as deletions and additions. To determine if an object is newly added, the `get_added_objs()` method should be used."
      ],
      "code_start_line": 12,
      "code_end_line": 229,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChangeDetector:\n    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n\n\n    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n\n    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n    \n    \n    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.\n    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.\n    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.\n    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n    \n    # TODO:可能有错，需要单元测试覆盖； 可能有更好的实现方式\n    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n\n    \n    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate",
        "tests/test_change_detector.py",
        "tests/test_change_detector.py/TestChangeDetector/setUpClass"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: Initializes a ChangeDetector object.\n\n**Parameters**:\n- repo_path (str): The path to the repository.\n\n**Code Description**:\nThe `__init__` method is the constructor of the ChangeDetector class. It takes the `repo_path` parameter, which is the path to the repository, and initializes the `repo_path` attribute of the ChangeDetector object with the provided value.\n\nThe method also initializes the `repo` attribute by creating a git.Repo object using the `repo_path`.\n\n**Note**: The `repo_path` should be a valid path to the repository.\n\n**Returns**:\nNone\n\n**Example**:\n```python\nchange_detector = ChangeDetector('/path/to/repository')\n```"
      ],
      "code_start_line": 13,
      "code_end_line": 24,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "repo_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler",
        "repo_agent/file_handler.py/FileHandler/read_file",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes",
        "repo_agent/change_detector.py/ChangeDetector/get_staged_pys",
        "repo_agent/change_detector.py/ChangeDetector/get_file_diff",
        "repo_agent/change_detector.py/ChangeDetector/parse_diffs",
        "repo_agent/change_detector.py/ChangeDetector/identify_changes_in_structure",
        "repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files"
      ]
    },
    "get_staged_pys": {
      "type": "FunctionDef",
      "name": "get_staged_pys",
      "md_content": [
        "**get_staged_pys**: The function of this Function is to retrieve the added Python files in the repository that have been staged.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function first initializes a variable `repo` with the repository object. Then, it creates an empty dictionary `staged_files` to store the changed Python files. \n\nNext, it uses the `diff` method of the `index` object to get the staged changes in the repository. The `diffs` variable will contain a list of `Diff` objects representing the changes.\n\nThe Function iterates over each `diff` object in the `diffs` list. It checks if the change type is either \"A\" (added) or \"M\" (modified) and if the file path ends with \".py\" (indicating a Python file). If both conditions are met, it determines whether the file is newly created by checking if the change type is \"A\". It then adds the file path as a key to the `staged_files` dictionary and sets the value to `True` if the file is newly created, or `False` if it is modified.\n\nFinally, the Function returns the `staged_files` dictionary.\n\n**Note**: This Function specifically tracks the changes of Python files in Git that have been staged, meaning the files that have been added using `git add`.\n\n**Output Example**: \n```\n{\n    \"path/to/file1.py\": True,\n    \"path/to/file2.py\": False,\n    \"path/to/file3.py\": True\n}\n```"
      ],
      "code_start_line": 26,
      "code_end_line": 50,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "tests/test_change_detector.py/TestChangeDetector/setUpClass"
      ],
      "reference_who": []
    },
    "get_file_diff": {
      "type": "FunctionDef",
      "name": "get_file_diff",
      "md_content": [
        "**get_file_diff**: The function of this Function is to retrieve the changes made to a specific file. For new files, it uses the \"git diff --staged\" command to get the differences between the staged area and the working directory. For non-new files, it uses the \"git diff HEAD\" command to get the differences between the current commit and the working directory.\n\n**parameters**: \n- file_path (str): The relative path of the file.\n- is_new_file (bool): Indicates whether the file is a new file.\n\n**Code Description**: \nThe function first assigns the repository object to the \"repo\" variable. \n\nIf the file is a new file (is_new_file is True), the function performs the following steps:\n1. It constructs a command to add the file to the staging area using the \"git -C\" command and the file path.\n2. It runs the add command using the \"subprocess.run\" function, passing the command as a string and setting the \"shell\" and \"check\" parameters to True.\n3. It retrieves the diff from the staging area using the \"git diff --staged\" command and the file path. The output is a string containing the differences between the staged area and the working directory.\n4. It splits the diff string into a list of lines using the \"splitlines\" method.\n\nIf the file is not a new file, the function performs the following steps:\n1. It retrieves the diff from the current commit (HEAD) using the \"git diff HEAD\" command and the file path. The output is a string containing the differences between the current commit and the working directory.\n2. It splits the diff string into a list of lines using the \"splitlines\" method.\n\nFinally, the function returns the list of changes made to the file.\n\n**Note**: \n- The function assumes that the repository object is already initialized and assigned to the \"repo\" variable.\n- The function uses the \"subprocess.run\" function to execute shell commands, so it requires the \"subprocess\" module to be imported.\n- The function relies on the \"git\" command being available in the system's PATH.\n\n**Output Example**: \nIf the file has the following changes:\n- Line 1: Original content\n- Line 2: Modified content\n- Line 3: Deleted content\n\nThe function would return the following list:\n['- Line 1: Original content', '+ Line 2: Modified content', '- Line 3: Deleted content']"
      ],
      "code_start_line": 53,
      "code_end_line": 75,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "file_path",
        "is_new_file"
      ],
      "have_return": true,
      "code_content": "    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "parse_diffs": {
      "type": "FunctionDef",
      "name": "parse_diffs",
      "md_content": [
        "**parse_diffs**: The function of this Function is to parse the difference content and extract the added and deleted object information, where the object can be a class or a function. It returns a dictionary containing the added and deleted line information.\n\n**parameters**: \n- diffs (list): A list containing the difference content obtained from the get_file_diff() function inside the class.\n\n**Code Description**:\nThe function starts by initializing variables `changed_lines`, `line_number_current`, and `line_number_change`. The `changed_lines` variable is a dictionary with keys \"added\" and \"removed\" and empty lists as values. The `line_number_current` and `line_number_change` variables are used to keep track of the current line numbers.\n\nThe function then iterates over each line in the `diffs` list. It checks if the line contains line number information using regular expression matching. If a match is found, the current line number and changed line number are updated accordingly.\n\nNext, it checks if the line starts with \"+\" and does not start with \"+++\". If it does, it appends a tuple `(line_number_change, line[1:])` to the \"added\" list in the `changed_lines` dictionary. The `line_number_change` is incremented by 1.\n\nSimilarly, if the line starts with \"-\" and does not start with \"---\", it appends a tuple `(line_number_current, line[1:])` to the \"removed\" list in the `changed_lines` dictionary. The `line_number_current` is incremented by 1.\n\nFor lines that do not have any changes, both `line_number_current` and `line_number_change` are incremented by 1.\n\nFinally, the function returns the `changed_lines` dictionary containing the added and removed line information.\n\n**Note**: \n- The function assumes that the `diffs` list contains valid difference content obtained from the `get_file_diff()` function.\n- The function does not differentiate between newly added objects and modified objects. To determine if an object is newly added, the `get_added_objs()` function should be used.\n\n**Output Example**: \n{'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}"
      ],
      "code_start_line": 77,
      "code_end_line": 115,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "diffs"
      ],
      "have_return": true,
      "code_content": "    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "identify_changes_in_structure": {
      "type": "FunctionDef",
      "name": "identify_changes_in_structure",
      "md_content": [
        "**identify_changes_in_structure**: The function of this Function is to identify the structure (function or class) where changes have occurred in the code. It traverses all the changed lines and checks whether each line falls within the start and end line of a structure. If a line is within a structure, that structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary.\n\n**parameters**: \n- changed_lines (dict): A dictionary containing the line numbers where changes have occurred. It has two keys: 'added' and 'removed'. The value for each key is a list of tuples, where each tuple contains the line number and the change content.\n- structures (list): A list of function or class structures obtained from the 'get_functions_and_classes' function. Each structure is represented as a tuple containing the structure type, name, start line number, end line number, and parent structure name.\n\n**Code Description**:\nThe function starts by initializing an empty dictionary called 'changes_in_structures' with two keys: 'added' and 'removed', each associated with an empty set. This dictionary will store the structures where changes have occurred.\n\nNext, the function iterates over the 'changed_lines' dictionary, which contains the line numbers where changes have occurred. For each line number, it iterates over the 'structures' list and checks if the line falls within the start and end line of a structure. If it does, the name of the structure and the name of its parent structure (if any) are added to the corresponding set in the 'changes_in_structures' dictionary, based on whether the line was added or removed.\n\nFinally, the function returns the 'changes_in_structures' dictionary, which contains the structures where changes have occurred, categorized by the change type ('added' or 'removed').\n\n**Note**: \n- The function assumes that the 'changed_lines' and 'structures' parameters are correctly formatted as described in the function's docstring.\n- The function does not handle cases where a line falls within multiple structures.\n\n**Output Example**:\n{'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}"
      ],
      "code_start_line": 121,
      "code_end_line": 148,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "changed_lines",
        "structures"
      ],
      "have_return": true,
      "code_content": "    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_to_be_staged_files": {
      "type": "FunctionDef",
      "name": "get_to_be_staged_files",
      "md_content": [
        "**get_to_be_staged_files**: The function of this Function is to retrieve all unstaged files in the repository that meet certain conditions. It returns a list of the paths of these files.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function first initializes an empty list called `to_be_staged_files` to store the paths of the files that meet the conditions. It then retrieves the paths of the already staged files and stores them in the `staged_files` list.\n\nNext, it retrieves the `project_hierarchy` value from the CONFIG dictionary. The `project_hierarchy` represents the path of the project hierarchy in the repository.\n\nThe Function then retrieves all the differences between the repository index and the HEAD commit using the `diff` method of the `index` object. These differences are stored in the `diffs` list.\n\nIt also retrieves the paths of all the untracked files in the repository using the `untracked_files` attribute. These paths are stored in the `untracked_files` list.\n\nThe Function then iterates over each untracked file in the `untracked_files` list. It constructs the absolute path of the untracked file by joining the `repo_path` and the untracked file path. It also constructs the relative path of the untracked file by using the `relpath` method with the `repo_path` as the base path. \n\nIf the relative path of the untracked file ends with '.md', it further processes the file. It removes the `Markdown_Docs_folder` path from the relative path and constructs the corresponding Python file path by replacing the '.md' extension with '.py'. If this corresponding Python file path is present in the `staged_files` list, it adds the absolute path of the untracked file to the `to_be_staged_files` list.\n\nIf the relative path of the untracked file is equal to the `project_hierarchy` value, it adds the relative path to the `to_be_staged_files` list.\n\nNext, the Function iterates over each unstaged file in the `unstaged_files` list. It constructs the absolute path of the unstaged file and the relative path of the unstaged file in a similar manner as before.\n\nIf the unstaged file ends with '.md', it removes the `Markdown_Docs_folder` path from the relative path and constructs the corresponding Python file path. If this corresponding Python file path is present in the `staged_files` list, it adds the absolute path of the unstaged file to the `to_be_staged_files` list.\n\nIf the relative path of the unstaged file is equal to the `project_hierarchy` value, it adds the relative path to the `to_be_staged_files` list.\n\nFinally, the Function returns the `to_be_staged_files` list.\n\n**Note**: This Function retrieves the paths of the unstaged files in the repository that meet specific conditions. It checks if the file, when its extension is changed to '.md', corresponds to a file that is already staged. It also checks if the file's path is the same as the `project_hierarchy` field in the CONFIG dictionary.\n\n**Output Example**: \n```\n['/path/to/repo/README.md', '/path/to/repo/docs/file.md']\n```"
      ],
      "code_start_line": 151,
      "code_end_line": 218,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__",
        "tests/test_change_detector.py/TestChangeDetector/setUpClass"
      ],
      "reference_who": []
    },
    "add_unstaged_files": {
      "type": "FunctionDef",
      "name": "add_unstaged_files",
      "md_content": [
        "**add_unstaged_files**: The function of this Function is to add unstaged files that meet certain conditions to the staging area.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThis Function first calls the `get_to_be_staged_files()` method to get a list of unstaged files that meet certain conditions. Then, it iterates over each file path in the list and constructs a command to add the file to the staging area using the `git add` command. The command is executed using the `subprocess.run()` method with the `shell=True` and `check=True` parameters, which ensures that the command is executed in a shell and raises an exception if the command fails. Finally, the Function returns the list of unstaged files that were added to the staging area.\n\n**Note**: \n- This Function assumes that the `get_to_be_staged_files()` method is implemented and returns a list of file paths.\n- The `git` command is executed using the `subprocess.run()` method, so make sure that the `git` command is available in the system environment.\n\n**Output Example**: \nIf there are two unstaged files that meet the conditions, the Function will add them to the staging area and return the list of file paths:\n```\n['path/to/file1.py', 'path/to/file2.py']\n```"
      ],
      "code_start_line": 221,
      "code_end_line": 229,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate",
        "tests/test_change_detector.py/TestChangeDetector/setUpClass"
      ],
      "reference_who": []
    }
  },
  "repo_agent/project_manager.py": {
    "ProjectManager": {
      "type": "ClassDef",
      "name": "ProjectManager",
      "md_content": [
        "**ProjectManager**: The function of this Class is to manage the project hierarchy and provide functionality to retrieve the project structure and find all references of a variable in a given file.\n\n**attributes**: \n- repo_path (str): The path of the repository.\n- project (jedi.Project): The Jedi project associated with the repository.\n- project_hierarchy (str): The path of the project hierarchy file.\n\n**Code Description**: \nThe `ProjectManager` class is responsible for managing the project hierarchy and providing methods to retrieve the project structure and find all references of a variable in a given file.\n\nThe `__init__` method initializes the `ProjectManager` object by setting the `repo_path` attribute to the provided repository path, creating a Jedi project associated with the repository, and setting the `project_hierarchy` attribute to the path of the project hierarchy file.\n\nThe `get_project_structure` method is used to retrieve the project structure. It defines a nested function `walk_dir` that recursively traverses the repository directory and appends the directory and file names to the `structure` list. The method then calls `walk_dir` with the repository path as the root directory and returns the project structure as a string.\n\nThe `find_all_referencer` method is used to find all references of a variable in a given file. It takes the variable name, file path, line number, and column number as arguments. It creates a Jedi script object with the file path and uses the `get_references` method to retrieve all references of the variable at the specified location. The method filters out the references with the same variable name and returns a list of tuples containing the file path, line number, and column number of each reference.\n\n**Note**: \n- This class requires the `jedi` library to be installed.\n- The `get_project_structure` method only includes directories and Python files in the project structure.\n- The `find_all_referencer` method assumes that the provided file path is relative to the repository path.\n\n**Output Example**: \n- `get_project_structure`:\n  ```\n  RepoAgent\n    assets\n      images\n    display\n      book_template\n      book_tools\n        generate_repoagent_books.py\n        generate_summary_from_book.py\n      books\n      scripts\n    examples\n      init.py\n    repo_agent\n      __init__.py\n      __pycache__\n      change_detector.py\n      chat_engine.py\n      config.py\n      doc_meta_info.py\n      file_handler.py\n      project_manager.py\n      prompt.py\n      runner.py\n      utils\n        __pycache__\n        gitignore_checker.py\n    setup.py\n    tests\n      __init__.py\n      test_change_detector.py\n  ```\n\n- `find_all_referencer`:\n  ```\n  [\n    ('repo_agent/chat_engine.py', 10, 5),\n    ('repo_agent/chat_engine.py', 15, 10),\n    ('repo_agent/chat_engine.py', 20, 15)\n  ]\n  ```",
        "**ProjectManager**: The function of this Class is to manage a project by providing methods to retrieve the project structure and find all references of a variable in a given file.\n\n**attributes**: \n- `repo_path (str)`: The path of the repository where the project is located.\n- `project (jedi.Project)`: The Jedi project object representing the project.\n- `project_hierarchy (str)`: The path of the project hierarchy file.\n\n**Code Description**: \nThe `ProjectManager` class has three attributes: `repo_path`, `project`, and `project_hierarchy`. The `repo_path` attribute stores the path of the repository where the project is located. The `project` attribute is an instance of the `jedi.Project` class, which represents the project. The `project_hierarchy` attribute stores the path of the project hierarchy file.\n\nThe `ProjectManager` class has two methods: `get_project_structure()` and `find_all_referencer()`.\n\nThe `get_project_structure()` method is used to retrieve the structure of the project. It internally calls the `walk_dir()` function to recursively traverse the repository directory and collect the names of directories and Python files. The collected structure is then returned as a formatted string.\n\nThe `find_all_referencer()` method is used to find all references of a variable in a given file. It takes four parameters: `variable_name`, `file_path`, `line_number`, and `column_number`. It uses the `jedi.Script` class to create a script object for the given file path. It then calls the `get_references()` method of the script object to retrieve all references of the variable at the specified line and column. The method filters out the references with the same variable name and returns a list of tuples containing the file path, line number, and column number of each reference.\n\nIf an error occurs during the execution of the `find_all_referencer()` method, an error message is printed along with the parameters that caused the error. An empty list is returned in case of an error.\n\n**Note**: \n- The `jedi` module is used for code analysis and introspection.\n- The `get_project_structure()` method assumes that the repository directory contains only directories and Python files. Other file types are ignored.\n- The `find_all_referencer()` method assumes that the given file path is relative to the repository directory.\n\n**Output Example**: \nExample output of the `get_project_structure()` method:\n```\nproject_folder\n  subfolder1\n    file1.py\n    file2.py\n  subfolder2\n    file3.py\n  file4.py\n```\n\nExample output of the `find_all_referencer()` method:\n```\n[('subfolder1/file1.py', 10, 5), ('subfolder1/file2.py', 5, 10)]\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 52,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ProjectManager:\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n\n    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n    \n    def find_all_referencer(self, variable_name, file_path, line_number, column_number):\n        \"\"\"\n        Find all references of a variable in a given file.\n\n        Args:\n            variable_name (str): The name of the variable to search for.\n            file_path (str): The path of the file to search in.\n            line_number (int): The line number where the variable is located.\n            column_number (int): The column number where the variable is located.\n\n        Returns:\n            list: A list of tuples containing the file path, line number, and column number of each reference.\n        \n        \"\"\"\n        script = jedi.Script(path=os.path.join(self.repo_path, file_path))\n        references = script.get_references(line=line_number, column=column_number)\n\n        try:\n            # Filter out references with variable_name and return their positions\n            variable_references = [ref for ref in references if ref.name == variable_name]\n            return [(os.path.relpath(ref.module_path, self.repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n        except Exception as e:\n            # Print error message and related parameters\n            print(f\"Error occurred: {e}\")\n            print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n            return []\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of this Function is to initialize a ProjectManager object.\n\n**parameters**: \n- repo_path: The path to the repository.\n- project_hierarchy: The path to the project hierarchy file.\n\n**Code Description**: \nThe `__init__` function is the constructor of the ProjectManager class. It takes in two parameters: `repo_path` and `project_hierarchy`. \n\nInside the function, the `repo_path` parameter is assigned to the `repo_path` attribute of the ProjectManager object. \n\nThen, a new jedi.Project object is created using the `repo_path` as the argument, and assigned to the `project` attribute of the ProjectManager object. The jedi.Project object is used to interact with the project's Python code.\n\nThe `project_hierarchy` parameter is joined with the `repo_path` and the \".project_hierarchy.json\" file extension to create the path to the project hierarchy file. This path is assigned to the `project_hierarchy` attribute of the ProjectManager object.\n\n**Note**: \n- The `jedi` module is assumed to be imported before using this function.\n- The `repo_path` and `project_hierarchy` attributes are assumed to be defined outside of the `__init__` function and are not shown in the provided code.\n\nRaw code:\n```\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n```"
      ],
      "code_start_line": 5,
      "code_end_line": 8,
      "parent": "ProjectManager",
      "params": [
        "self",
        "repo_path",
        "project_hierarchy"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/project_manager.py/ProjectManager/get_project_structure",
        "repo_agent/project_manager.py/ProjectManager/get_project_structure/walk_dir"
      ]
    },
    "get_project_structure": {
      "type": "FunctionDef",
      "name": "get_project_structure",
      "md_content": [
        "**get_project_structure**: The function of this Function is to retrieve the hierarchical structure of a project.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: This Function uses a recursive approach to traverse the project directory and retrieve the hierarchical structure. It starts by calling the `walk_dir` function with the root directory path. The `walk_dir` function takes two parameters: `root` and `prefix`. \n\nInside the `walk_dir` function, the base name of the current directory is appended to the `structure` list with the provided `prefix`. Then, a new prefix is created by adding two spaces to the current prefix. \n\nNext, the function iterates over the sorted list of names in the current directory. If a name starts with a dot, indicating a hidden file or directory, it is ignored. If the name corresponds to a directory, the `walk_dir` function is recursively called with the path of the subdirectory and the new prefix. If the name corresponds to a Python file (ends with '.py'), it is appended to the `structure` list with the new prefix.\n\nAfter traversing the entire project directory, the `structure` list is joined with newline characters and returned as a string.\n\n**Note**: This Function ignores hidden files and directories (those starting with a dot) and only includes Python files in the project structure.\n\n**Output Example**:\n```\nRepoAgent\n  assets\n    images\n  display\n    book_template\n    book_tools\n      generate_repoagent_books.py\n      generate_summary_from_book.py\n    books\n    scripts\n  examples\n    init.py\n  repo_agent\n    __init__.py\n    __pycache__\n    change_detector.py\n    chat_engine.py\n    config.py\n    doc_meta_info.py\n    file_handler.py\n    project_manager.py\n    prompt.py\n    runner.py\n    utils\n      __pycache__\n      gitignore_checker.py\n  setup.py\n  tests\n    __init__.py\n    test_change_detector.py\n```",
        "**get_project_structure**: The function of this Function is to retrieve the structure of the project.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: \nThe `get_project_structure` function is responsible for retrieving the structure of the project. It uses a helper function called `walk_dir` to recursively traverse the project directory and collect the names of all directories and Python files.\n\nInside the `get_project_structure` function, an empty list called `structure` is initialized. Then, the `walk_dir` function is called with the `repo_path` attribute of the ProjectManager object as the root directory. The `walk_dir` function appends the names of directories and Python files to the `structure` list.\n\nAfter the `walk_dir` function completes, the `structure` list is joined with newline characters using the `join` method, and the resulting string is returned.\n\n**Note**: \n- The `os` module is assumed to be imported before using this function.\n- The `repo_path` attribute is assumed to be defined outside of the `get_project_structure` function and is not shown in the provided code.\n\n**Output Example**: \nIf the project structure is as follows:\n```\nproject/\n  ├── dir1/\n  │   ├── file1.py\n  │   └── file2.py\n  ├── dir2/\n  │   ├── file3.py\n  │   └── file4.py\n  └── file5.py\n```\nThe return value of `get_project_structure` would be:\n```\nproject\n  dir1\n    file1.py\n    file2.py\n  dir2\n    file3.py\n    file4.py\n  file5.py\n```"
      ],
      "code_start_line": 10,
      "code_end_line": 25,
      "parent": "ProjectManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/project_manager.py/ProjectManager/__init__"
      ],
      "reference_who": []
    },
    "walk_dir": {
      "type": "FunctionDef",
      "name": "walk_dir",
      "md_content": [
        "**walk_dir**: The function of this Function is to recursively traverse a directory and append the structure of the directory to a list.\n\n**parameters**: \n- root: The root directory to start the traversal from.\n- prefix: The prefix to be added to each directory or file name in the structure.\n\n**Code Description**: \nThe `walk_dir` function takes in a `root` directory and an optional `prefix` parameter. It appends the structure of the directory to a list called `structure`. \n\nThe function starts by appending the basename of the `root` directory to the `structure` list, with the `prefix` added in front. \n\nThen, a new prefix is created by adding two spaces to the current `prefix`. This new prefix will be used for the subdirectories and files within the `root` directory.\n\nNext, the function iterates over the sorted list of names in the `root` directory using the `os.listdir` function. For each name, it performs the following checks:\n\n1. If the name starts with a dot (indicating a hidden file or directory), it is ignored and the loop moves on to the next name.\n2. If the name corresponds to a subdirectory, the `walk_dir` function is called recursively with the subdirectory path as the new `root` and the new prefix as the `prefix`.\n3. If the name corresponds to a file and ends with the \".py\" extension, it is appended to the `structure` list with the new prefix added in front.\n\nThis process continues until all directories and files within the `root` directory have been traversed.\n\n**Note**: \n- The `walk_dir` function relies on the `os` module, so make sure to import it before using this function.\n- The `structure` list is assumed to be defined outside of the `walk_dir` function and is not shown in the provided code."
      ],
      "code_start_line": 11,
      "code_end_line": 21,
      "parent": "get_project_structure",
      "params": [
        "root",
        "prefix"
      ],
      "have_return": false,
      "code_content": "        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/project_manager.py/ProjectManager/__init__"
      ],
      "reference_who": []
    },
    "find_all_referencer": {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [
        "**find_all_referencer**: The function of this Function is to find all references of a given variable in a specified file.\n\n**parameters**: \n- variable_name (str): The name of the variable to search for.\n- file_path (str): The path of the file to search in.\n- line_number (int): The line number where the variable is located.\n- column_number (int): The column number where the variable is located.\n\n**Code Description**: \nThis function takes in the name of a variable, the path of a file, and the line and column numbers where the variable is located. It uses the Jedi library to analyze the Python code in the specified file and find all references to the variable. \n\nFirst, it creates a Jedi Script object using the file path provided. Then, it calls the `get_references` method of the Script object, passing in the line and column numbers. This returns a list of references to the variable in the file.\n\nNext, the function filters out the references that have the same variable name as the one provided. It creates a new list, `variable_references`, by iterating over the references and only keeping the ones with matching variable names. \n\nFinally, the function returns a list of tuples, where each tuple contains the file path, line number, and column number of a reference to the variable. It uses the `os.path.relpath` function to get the relative path of each reference's module, relative to the repository path. It also excludes the reference that matches the original line and column numbers, as it is the location of the variable declaration.\n\n**Note**: \n- This function requires the Jedi library to be installed.\n- The `self.repo_path` attribute is assumed to be the path to the repository where the file is located.\n\n**Output Example**: \nIf the variable name is \"my_variable\" and there are two references to it in the file \"example.py\" at line 10, column 5 and line 15, column 8, the function would return the following list:\n[(\"example.py\", 10, 5), (\"example.py\", 15, 8)]"
      ],
      "code_start_line": 27,
      "code_end_line": 52,
      "parent": "ProjectManager",
      "params": [
        "self",
        "variable_name",
        "file_path",
        "line_number",
        "column_number"
      ],
      "have_return": true,
      "code_content": "    def find_all_referencer(self, variable_name, file_path, line_number, column_number):\n        \"\"\"\n        Find all references of a variable in a given file.\n\n        Args:\n            variable_name (str): The name of the variable to search for.\n            file_path (str): The path of the file to search in.\n            line_number (int): The line number where the variable is located.\n            column_number (int): The column number where the variable is located.\n\n        Returns:\n            list: A list of tuples containing the file path, line number, and column number of each reference.\n        \n        \"\"\"\n        script = jedi.Script(path=os.path.join(self.repo_path, file_path))\n        references = script.get_references(line=line_number, column=column_number)\n\n        try:\n            # Filter out references with variable_name and return their positions\n            variable_references = [ref for ref in references if ref.name == variable_name]\n            return [(os.path.relpath(ref.module_path, self.repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n        except Exception as e:\n            # Print error message and related parameters\n            print(f\"Error occurred: {e}\")\n            print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n            return []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    }
  },
  "repo_agent/utils/gitignore_checker.py": {
    "GitignoreChecker": {
      "type": "ClassDef",
      "name": "GitignoreChecker",
      "md_content": [
        "**GitignoreChecker**: The function of this Class is to check files and folders in a specified directory against the patterns defined in a .gitignore file. It returns a list of file paths that are not ignored and have the '.py' extension.\n\n**attributes**: \n- directory (str): The directory to be checked.\n- gitignore_path (str): The path to the .gitignore file.\n- folder_patterns (list): A list of folder patterns extracted from the .gitignore file.\n- file_patterns (list): A list of file patterns extracted from the .gitignore file.\n\n**Code Description**: \nThe `GitignoreChecker` class is responsible for loading and parsing the patterns defined in a .gitignore file, and then checking files and folders in a specified directory against these patterns. It provides methods to initialize the class, load and parse the .gitignore file, split the patterns into folder and file patterns, and check if a given path matches any of the patterns.\n\nThe class has an `__init__` method that takes in the `directory` and `gitignore_path` as arguments and initializes the class attributes. The `directory` is the directory to be checked, and the `gitignore_path` is the path to the .gitignore file. The method also calls the `_load_gitignore_patterns` method to load and parse the .gitignore file, and assigns the folder and file patterns to the `folder_patterns` and `file_patterns` attributes.\n\nThe `_load_gitignore_patterns` method is a private method that loads and parses the .gitignore file. It first tries to open the specified .gitignore file and read its content. If the file is not found, it falls back to the default .gitignore path. The method then calls the `_parse_gitignore` method to parse the content and returns the split patterns.\n\nThe `_parse_gitignore` method is a private method that parses the .gitignore content and returns the patterns as a list. It iterates over each line in the content, strips leading and trailing whitespace, and checks if the line is not empty and does not start with '#'. If the line meets these conditions, it appends it to the patterns list. The method returns the patterns list.\n\nThe `_split_gitignore_patterns` method is a private method that splits the .gitignore patterns into folder and file patterns. It iterates over each pattern and checks if it ends with '/'. If it does, it appends the pattern without the trailing '/' to the folder patterns list. Otherwise, it appends the pattern to the file patterns list. The method returns a tuple containing the folder and file patterns lists.\n\nThe `_is_ignored` method is a private method that checks if a given path matches any of the patterns. It takes in the `path`, `patterns`, and `is_dir` as arguments. It iterates over each pattern and checks if the path matches the pattern using the `fnmatch.fnmatch` function. If the path matches the pattern, it returns True. If the `is_dir` is True and the pattern ends with '/', it also checks if the path matches the pattern without the trailing '/'. If the path matches the pattern without the trailing '/', it returns True. If no match is found, it returns False.\n\nThe `check_files_and_folders` method checks all files and folders in the specified directory against the split gitignore patterns. It uses the `os.walk` function to iterate over all files and folders in the directory. It filters out ignored directories by modifying the `dirs` list using a list comprehension and the `_is_ignored` method. For each file, it constructs the file path and relative path, and checks if the file is not ignored, has the '.py' extension, and appends the relative path to the `not_ignored_files` list. Finally, it returns the `not_ignored_files` list.\n\n**Note**: \n- The `directory` and `gitignore_path` should be valid paths.\n- The `.gitignore` file should follow the standard format.\n- The returned file paths are relative to the `directory`.\n\n**Output Example**: \nIf the directory contains the following files and folders:\n- folder1/\n- folder2/\n- file1.py\n- file2.py\n- file3.txt\n\nAnd the .gitignore file contains the following patterns:\n- folder1/\n- *.txt\n\nThe `check_files_and_folders` method will return:\n- ['file1.py', 'file2.py']"
      ],
      "code_start_line": 5,
      "code_end_line": 116,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class GitignoreChecker:\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py",
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of this Function is to initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n**parameters**: \n- directory (str): The directory to be checked.\n- gitignore_path (str): The path to the .gitignore file.\n\n**Code Description**: \nThe `__init__` function is the constructor of the GitignoreChecker class. It takes two parameters: `directory` and `gitignore_path`. The `directory` parameter represents the directory to be checked for ignored files, and the `gitignore_path` parameter represents the path to the .gitignore file.\n\nInside the function, the `directory` and `gitignore_path` parameters are assigned to the `self.directory` and `self.gitignore_path` attributes of the GitignoreChecker object, respectively.\n\nThe function then calls the `_load_gitignore_patterns` method to load and parse the .gitignore file. The folder patterns and file patterns obtained from the .gitignore file are assigned to the `self.folder_patterns` and `self.file_patterns` attributes of the GitignoreChecker object, respectively.\n\n**Note**: \n- The `directory` and `gitignore_path` parameters should be valid strings representing the directory path and the path to the .gitignore file, respectively.\n- The function assumes that the .gitignore file exists at the specified `gitignore_path` or falls back to the default .gitignore path if the file is not found.\n\n**Output Example**:\n```\nGitignoreChecker(directory='/path/to/directory', gitignore_path='/path/to/.gitignore')\n```\nRaw code:```\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n```"
      ],
      "code_start_line": 6,
      "code_end_line": 16,
      "parent": "GitignoreChecker",
      "params": [
        "self",
        "directory",
        "gitignore_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_load_gitignore_patterns",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_parse_gitignore",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_split_gitignore_patterns",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_is_ignored"
      ]
    },
    "_load_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_load_gitignore_patterns",
      "md_content": [
        "**_load_gitignore_patterns**: The function of this Function is to load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function starts by attempting to open and read the .gitignore file specified by the `gitignore_path` attribute. If the file is found, its content is stored in the `gitignore_content` variable. If the file is not found, the function falls back to the default .gitignore path and reads its content instead.\n\nNext, the function calls the `_parse_gitignore` method to parse the `gitignore_content` and obtain a list of patterns. These patterns are then passed to the `_split_gitignore_patterns` method, which splits them into two separate lists - one for folder patterns and one for file patterns.\n\nFinally, the function returns a tuple containing the folder patterns and file patterns.\n\n**Note**: \n- This function assumes that the `gitignore_path` attribute has been properly set before calling this method.\n- If the specified .gitignore file is not found, the function falls back to the default .gitignore path.\n- The function relies on the `_parse_gitignore` and `_split_gitignore_patterns` methods to perform the parsing and splitting of patterns.\n\n**Output Example**:\n```\n(['folder_pattern1', 'folder_pattern2'], ['file_pattern1', 'file_pattern2'])\n```"
      ],
      "code_start_line": 18,
      "code_end_line": 37,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "_parse_gitignore": {
      "type": "FunctionDef",
      "name": "_parse_gitignore",
      "md_content": [
        "**_parse_gitignore**: The function of this Function is to parse the content of a .gitignore file and extract the patterns as a list.\n\n**parameters**: \n- gitignore_content (str): The content of the .gitignore file.\n\n**Code Description**:\nThe `_parse_gitignore` function takes the content of a .gitignore file as input and returns a list of patterns extracted from the content. \n\nThe function starts by initializing an empty list called `patterns`. It then iterates over each line in the `gitignore_content` by splitting it using the `splitlines()` method. \n\nFor each line, it removes any leading or trailing whitespace characters using the `strip()` method. If the line is not empty and does not start with a '#' character (indicating a comment), it appends the line to the `patterns` list.\n\nFinally, the function returns the `patterns` list containing all the patterns extracted from the .gitignore content.\n\n**Note**: \n- The function assumes that the `gitignore_content` parameter is a string representing the content of a .gitignore file.\n- The function ignores empty lines and lines starting with '#' as they are considered comments in a .gitignore file.\n\n**Output Example**:\nIf the `gitignore_content` is:\n```\n# Ignore compiled files\n*.pyc\n*.class\n\n# Ignore log files\n*.log\n```\nThe function will return the following list:\n```\n['*.pyc', '*.class', '*.log']\n```"
      ],
      "code_start_line": 40,
      "code_end_line": 55,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_content"
      ],
      "have_return": true,
      "code_content": "    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "_split_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_split_gitignore_patterns",
      "md_content": [
        "**_split_gitignore_patterns**: The function of this Function is to split the .gitignore patterns into folder patterns and file patterns.\n**parameters**: \n- gitignore_patterns (list): A list of patterns from the .gitignore file.\n**Code Description**: \nThis function takes a list of patterns from the .gitignore file as input and splits them into two separate lists: one for folder patterns and one for file patterns. It iterates through each pattern in the input list and checks if it ends with a forward slash (\"/\"). If it does, it appends the pattern to the folder_patterns list after removing the trailing slash. If it doesn't end with a slash, it appends the pattern to the file_patterns list. Finally, it returns a tuple containing the folder_patterns and file_patterns lists.\n**Note**: \n- The input list should contain patterns from the .gitignore file.\n**Output Example**: \nIf the input gitignore_patterns is ['folder/', 'file.txt', 'folder/subfolder/'], the function will return (['folder', 'folder/subfolder'], ['file.txt'])."
      ],
      "code_start_line": 58,
      "code_end_line": 75,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_patterns"
      ],
      "have_return": true,
      "code_content": "    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "_is_ignored": {
      "type": "FunctionDef",
      "name": "_is_ignored",
      "md_content": [
        "**_is_ignored**: The function of this Function is to check if the given path matches any of the patterns.\n\n**parameters**: \n- path (str): The path to check.\n- patterns (list): A list of patterns to check against.\n- is_dir (bool): True if the path is a directory, False otherwise.\n\n**Code Description**: \nThe `_is_ignored` function takes in a path, a list of patterns, and a boolean flag indicating whether the path is a directory or not. It iterates through each pattern in the list and checks if the path matches any of the patterns using the `fnmatch.fnmatch` function. If a match is found, the function returns True. Additionally, if the path is a directory and the pattern ends with a forward slash (\"/\"), it checks if the path matches the pattern without the trailing slash. If a match is found, the function also returns True. If no match is found after iterating through all the patterns, the function returns False.\n\n**Note**: \n- The `fnmatch` module is used to perform pattern matching with the `fnmatch.fnmatch` function.\n- The function assumes that the patterns provided are in the same format as those used in `.gitignore` files.\n\n**Output Example**: \n- If the path is \"/path/to/file.txt\", the patterns are [\"*.txt\", \"folder/\"], and the is_dir flag is False, the function will return True.\n- If the path is \"/path/to/folder/\", the patterns are [\"*.txt\", \"folder/\"], and the is_dir flag is True, the function will return True.\n- If the path is \"/path/to/file.txt\", the patterns are [\"*.md\", \"folder/\"], and the is_dir flag is False, the function will return False."
      ],
      "code_start_line": 78,
      "code_end_line": 95,
      "parent": "GitignoreChecker",
      "params": [
        "path",
        "patterns",
        "is_dir"
      ],
      "have_return": true,
      "code_content": "    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "check_files_and_folders": {
      "type": "FunctionDef",
      "name": "check_files_and_folders",
      "md_content": [
        "**check_files_and_folders**: The function of this Function is to check all files and folders in the given directory against the split gitignore patterns. It returns a list of files that are not ignored and have the '.py' extension. The returned file paths are relative to the self.directory.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe function starts by initializing an empty list called `not_ignored_files` to store the paths of files that are not ignored and have the '.py' extension.\n\nThen, it uses the `os.walk()` function to iterate through all the files and folders in the given directory (`self.directory`). For each directory, it checks if it should be ignored by calling the `_is_ignored()` function with the directory name and the folder patterns. If the directory should not be ignored, it is kept in the `dirs` list.\n\nNext, it iterates through all the files in each directory. For each file, it constructs the absolute file path by joining the root directory path and the file name. It also calculates the relative file path by calling the `os.path.relpath()` function with the absolute file path and the self.directory as arguments.\n\nThen, it checks if the file should be ignored by calling the `_is_ignored()` function with the file name and the file patterns. It also checks if the file has the '.py' extension by using the `endswith()` method. If the file should not be ignored and has the '.py' extension, its relative path is added to the `not_ignored_files` list.\n\nFinally, the function returns the `not_ignored_files` list.\n\n**Note**: \n- This function assumes that the `self.directory` attribute has been properly set before calling this function.\n- The `_is_ignored()` function is not defined in the given code snippet, so its behavior is not described here.\n\n**Output Example**: \nIf the given directory contains the following files and folders:\n- folder1/\n- folder2/\n- file1.py\n- file2.py\n- file3.txt\n\nAnd the gitignore patterns ignore the 'folder1' directory, the 'file2.py' file, and all files with the '.txt' extension, the function will return the following list:\n- ['file1.py']"
      ],
      "code_start_line": 97,
      "code_end_line": 116,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    }
  },
  "display/book_tools/generate_summary_from_book.py": {
    "create_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_readme_if_not_exist",
      "md_content": [
        "**create_readme_if_not_exist**: The function of this Function is to create a README.md file in a given directory if it doesn't already exist.\n\n**parameters**: \n- dire: The directory in which the README.md file should be created.\n\n**Code Description**: \nThe `create_readme_if_not_exist` function takes in a directory path `dire` as a parameter. It checks if a README.md file already exists in the directory. If it doesn't, the function creates a new README.md file and writes the directory name as a header in the file.\n\nThe function first constructs the path to the README.md file by joining the `dire` directory path with the filename 'README.md'. It then checks if the file already exists using the `os.path.exists` function.\n\nIf the README.md file doesn't exist, the function opens the file in write mode using the `open` function with the 'w' mode. It then retrieves the base name of the directory using the `os.path.basename` function and writes it as a header in the README.md file using the `write` method of the file object. The header is formatted as a markdown heading using the `format` method of the string.\n\n**Note**: \n- This function assumes that the `os` module has been imported before calling this function.\n- The function assumes that the `dire` parameter is a valid directory path.\n\nRaw code:```\ndef create_readme_if_not_exist(dire):\n    readme_path = os.path.join(dire, 'README.md')\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, 'w') as readme_file:\n            dirname = os.path.basename(dire)\n            readme_file.write('# {}\\n'.format(dirname))\n```"
      ],
      "code_start_line": 6,
      "code_end_line": 12,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "def create_readme_if_not_exist(dire):\n    readme_path = os.path.join(dire, 'README.md')\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, 'w') as readme_file:\n            dirname = os.path.basename(dire)\n            readme_file.write('# {}\\n'.format(dirname))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "display/book_tools/generate_summary_from_book.py/output_markdown",
        "display/book_tools/generate_summary_from_book.py/is_markdown_file",
        "display/book_tools/generate_summary_from_book.py/main"
      ]
    },
    "output_markdown": {
      "type": "FunctionDef",
      "name": "output_markdown",
      "md_content": [
        "**output_markdown**: The function of this Function is to generate a markdown file that represents the directory structure of a given directory.\n\n**parameters**: \n- dire: The directory to be processed.\n- base_dir: The base directory of the project.\n- output_file: The output file object to write the markdown content.\n- iter_depth: The current depth of iteration.\n\n**Code Description**: \nThe `output_markdown` function takes in a directory `dire`, the base directory `base_dir`, an output file object `output_file`, and an iteration depth `iter_depth`. It is used to generate a markdown file that represents the directory structure of the given directory.\n\nThe function first iterates over the files and directories in the `dire` directory. For each file or directory, it performs the following steps:\n\n1. If the item is a directory, it calls the `create_readme_if_not_exist` function to create a README.md file if it doesn't already exist in the directory.\n\n2. It then checks if the README.md file exists in the directory. If it does, it creates a markdown link to it in the output file. The relative path to the README.md file is calculated using `os.path.relpath`.\n\n3. The function recursively calls itself for nested directories, passing the nested directory path, base directory, output file, and incremented iteration depth.\n\n4. If the item is a file and it is a markdown file, it checks if it should be included in the output. Files named 'SUMMARY.md' and 'README.md' are excluded unless the iteration depth is greater than 0 and the file is not 'README.md'. If the file should be included, it creates a markdown link to it in the output file.\n\n**Note**: \n- The `create_readme_if_not_exist` function is called to create a README.md file in each directory if it doesn't already exist."
      ],
      "code_start_line": 42,
      "code_end_line": 65,
      "parent": null,
      "params": [
        "dire",
        "base_dir",
        "output_file",
        "iter_depth"
      ],
      "have_return": false,
      "code_content": "def output_markdown(dire, base_dir, output_file, iter_depth=0):\n    for filename in os.listdir(dire):\n        print('add readme ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            create_readme_if_not_exist(file_or_path)\n\n    for filename in os.listdir(dire):\n        print('deal with ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            # Check if README.md exists in the directory\n            readme_path = os.path.join(file_or_path, 'README.md')\n            if os.path.exists(readme_path):\n                # If README.md exists, create a markdown link to it\n                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')\n                output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(filename, relative_path))\n            # Recursively call output_markdown for nested directories\n            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)\n        else:\n            if is_markdown_file(filename):\n                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:\n                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)\n                    output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(is_markdown_file(filename), relative_path))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist"
      ],
      "reference_who": []
    },
    "markdown_file_in_dir": {
      "type": "FunctionDef",
      "name": "markdown_file_in_dir",
      "md_content": [
        "**markdown_file_in_dir**: The function of this Function is to check if there is a markdown file in a given directory.\n\n**parameters**: \n- dire: A string representing the directory path to be checked.\n\n**Code Description**: \nThis function takes a directory path as input and checks if there is a markdown file (.md or .markdown) present in that directory or any of its subdirectories. It uses the `os.walk()` function to traverse through the directory tree and `re.search()` function to match the file extensions.\n\nThe function starts by calling `os.walk(dire)`, which returns a generator that yields a tuple for each directory in the directory tree rooted at `dire`. Each tuple contains three elements: the path to the directory, a list of subdirectories, and a list of filenames in that directory.\n\nThe function then iterates over each directory and its corresponding filenames. For each filename, it uses `re.search()` to check if the file extension matches either \".md\" or \".markdown\". If a match is found, the function immediately returns `True`, indicating that a markdown file exists in the directory.\n\nIf no markdown file is found after checking all directories and filenames, the function returns `False`.\n\n**Note**: \n- This function only checks for markdown files with the extensions \".md\" or \".markdown\". It does not consider other file formats.\n- The function does not perform any error handling for invalid directory paths or other exceptions that may occur during the file search process.\n\n**Output Example**: \nIf the function is called with the directory path \"display/book_tools\", and there is a markdown file named \"example.md\" in the \"display/book_tools\" directory, the function will return `True`."
      ],
      "code_start_line": 69,
      "code_end_line": 74,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": true,
      "code_content": "def markdown_file_in_dir(dire):\n    for root, dirs, files in os.walk(dire):\n        for filename in files:\n            if re.search('.md$|.markdown$', filename):\n                return True\n    return False\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "is_markdown_file": {
      "type": "FunctionDef",
      "name": "is_markdown_file",
      "md_content": [
        "**is_markdown_file**: The function of this Function is to determine whether a given filename is a markdown file.\n\n**parameters**: \n- filename: A string representing the name of the file.\n\n**Code Description**: \nThe function first uses the `re.search()` function to search for a match of the pattern '.md$|.markdown$' in the given filename. If there is no match, it means that the file is not a markdown file, so the function returns False.\n\nIf there is a match, the function checks the length of the matched group. If the length is equal to the length of '.md', it means that the file extension is '.md'. In this case, the function returns the filename without the last 3 characters (which represent the '.md' extension).\n\nIf the length is equal to the length of '.markdown', it means that the file extension is '.markdown'. In this case, the function returns the filename without the last 9 characters (which represent the '.markdown' extension).\n\n**Note**: \n- This function relies on the `re` module, so make sure to import it before using this function.\n- The function assumes that the filename provided is a valid string.\n\n**Output Example**: \n- If the filename is 'example.md', the function will return 'example'.\n- If the filename is 'README.markdown', the function will return 'README'."
      ],
      "code_start_line": 77,
      "code_end_line": 84,
      "parent": null,
      "params": [
        "filename"
      ],
      "have_return": true,
      "code_content": "def is_markdown_file(filename):\n    match = re.search('.md$|.markdown$', filename)\n    if not match:\n        return False\n    elif len(match.group()) is len('.md'):\n        return filename[:-3]\n    elif len(match.group()) is len('.markdown'):\n        return filename[:-9]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist"
      ],
      "reference_who": []
    },
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of this Function is to generate a summary file for a book.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: \nThe `main` function is the entry point for generating a summary file for a book. It first retrieves the name of the book from the command line arguments. Then, it creates a directory path for the book by joining the `./books` directory with the book name and the `src` subdirectory. \n\nNext, it checks if the directory path exists. If it doesn't, it creates the directory using the `os.makedirs` function. \n\nAfter ensuring that the directory exists, the function proceeds to create the summary file. It opens a file named `SUMMARY.md` inside the book directory in write mode. It writes the header `# Summary` to the file.\n\nThen, it calls the `output_markdown` function, passing the directory path, the same directory path, and the output file as arguments. This function is responsible for generating the content of the summary file.\n\nFinally, the function prints a message indicating that the GitBook auto summary is finished and returns 0.\n\n**Note**: \n- This function assumes that the book name is provided as a command line argument.\n- The `output_markdown` function is expected to be defined elsewhere in the codebase.\n\n**Output Example**: \n```\n# Summary\n\n[Generated summary content]\n```\n"
      ],
      "code_start_line": 87,
      "code_end_line": 109,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    book_name = sys.argv[1]\n\n    # mkdir the book folder\n    dir_input = os.path.join('./books', book_name, 'src')\n\n    # check the dst_dir\n    if not os.path.exists(dir_input):\n        print(dir_input)\n        os.makedirs(dir_input)\n    # Ensure the directory exists or create it\n    if not os.path.exists(dir_input):\n        os.makedirs(dir_input)\n\n    # Then proceed to create the file\n    output_path = os.path.join(dir_input, 'SUMMARY.md')\n    output = open(output_path, 'w')\n    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')\n    output.write('# Summary\\n\\n')\n    output_markdown(dir_input, dir_input, output)\n\n    print('GitBook auto summary finished:) ')\n    return 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist"
      ],
      "reference_who": []
    }
  },
  "display/book_tools/generate_repoagent_books.py": {
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: The function of this Function is to copy the contents of a specified directory to a destination directory, and create a README.md file in the destination directory if it does not exist.\n\n**parameters**: This Function does not have any parameters.\n\n**Code Description**: \n- The function first retrieves the command line arguments passed to the script, which are the markdown_docs_folder, book_name, and repo_path.\n- It then creates the destination directory path by joining the './books', book_name, and 'src' directories.\n- The docs_dir variable is set to the path of the markdown_docs_folder within the repo_path.\n- The function checks if the destination directory does not exist, and if so, it creates the directory using os.makedirs() and prints a message indicating the creation of the directory.\n- It then iterates over the items in the docs_dir directory using os.listdir().\n- For each item, it constructs the source path by joining the docs_dir and the item, and the destination path by joining the dst_dir and the item.\n- If the item is a directory, it uses shutil.copytree() to recursively copy the directory and its contents to the destination directory, and prints a message indicating the copy operation.\n- If the item is a file, it uses shutil.copy2() to copy the file to the destination directory, and prints a message indicating the copy operation.\n- The function then defines a nested function called create_book_readme_if_not_exist() that takes a directory path as an argument.\n- Inside this nested function, it constructs the path to the README.md file within the specified directory.\n- If the README.md file does not exist, it creates the file using open() in write mode, and writes the book_name as the content of the file.\n- Finally, the main function calls the create_book_readme_if_not_exist() function with the dst_dir as the argument to create the README.md file in the destination directory.\n\n**Note**: \n- This function assumes that the command line arguments are passed correctly and in the expected order.\n- The function uses the shutil module from the standard library to perform the file and directory copying operations.\n- If the destination directory already exists, the function will not overwrite any existing files or directories."
      ],
      "code_start_line": 7,
      "code_end_line": 44,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    markdown_docs_folder = sys.argv[1]\n    book_name = sys.argv[2]\n    repo_path = sys.argv[3]\n\n    # mkdir the book folder\n    dst_dir = os.path.join('./books', book_name, 'src')\n    docs_dir = os.path.join(repo_path, markdown_docs_folder)\n\n    # check the dst_dir\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(\"mkdir %s\" % dst_dir)\n\n    # cp the Markdown_Docs_folder to dst_dir\n    for item in os.listdir(docs_dir):\n        src_path = os.path.join(docs_dir, item)\n        dst_path = os.path.join(dst_dir, item)\n\n        # check the src_path\n        if os.path.isdir(src_path):\n            # if the src_path is a folder, use shutil.copytree to copy\n            shutil.copytree(src_path, dst_path)\n            print(\"copytree %s to %s\" % (src_path, dst_path))\n        else:\n            # if the src_path is a file, use shutil.copy2 to copy\n            shutil.copy2(src_path, dst_path)\n            print(\"copy2 %s to %s\" % (src_path, dst_path))\n\n    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n\n    # create book README.md if not exist\n    create_book_readme_if_not_exist(dst_dir)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "create_book_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_book_readme_if_not_exist",
      "md_content": [
        "**create_book_readme_if_not_exist**: The function of this Function is to create a README.md file if it does not already exist in the specified directory.\n\n**parameters**: \n- dire: The directory path where the README.md file should be created.\n\n**Code Description**:\nThe function first constructs the path to the README.md file by joining the specified directory path (`dire`) with the filename 'README.md'. \n\nNext, it checks if the README.md file already exists in the specified directory by using the `os.path.exists()` function. If the file does not exist, it proceeds to create it.\n\nTo create the file, it opens the README.md file in write mode using the `open()` function with the 'w' flag. It then writes the content to the file using the `write()` method of the file object. In this case, it writes a single line containing the book name surrounded by '#' symbols.\n\n**Note**: \n- This function assumes that the `os` module has been imported and is available.\n- The `book_name` variable used in the code snippet is not defined in the given code. It should be replaced with the actual book name before using this function."
      ],
      "code_start_line": 36,
      "code_end_line": 41,
      "parent": "main",
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  }
}