{
  "setup.py": {},
  "tests/test_change_detector.py": {
    "TestChangeDetector": {
      "type": "ClassDef",
      "name": "TestChangeDetector",
      "md_content": [
        "**TestChangeDetector**: TestChangeDetector的功能是执行一系列的测试用例，用于测试ChangeDetector类的各个方法。\n\n**属性**: \n- test_repo_path: 测试仓库的路径\n- repo: Git仓库对象\n\n**代码描述**: \nTestChangeDetector是一个继承自unittest.TestCase的测试类，用于测试ChangeDetector类的各个方法。在setUpClass()方法中，首先定义了测试仓库的路径，并创建了测试仓库文件夹。然后，初始化了Git仓库，并配置了Git用户信息。接下来，创建了一些测试文件，并使用Git操作将文件添加和提交到仓库中。\n\n在test_get_staged_pys()方法中，首先创建了一个新的Python文件并暂存，然后使用ChangeDetector类检查暂存文件，并断言新文件在暂存文件列表中。\n\n在test_get_unstaged_mds()方法中，修改了一个Markdown文件但不暂存，然后使用ChangeDetector类获取未暂存的Markdown文件，并断言修改的文件在未暂存文件列表中。\n\n在test_add_unstaged_mds()方法中，首先调用了test_get_unstaged_mds()方法，确保有一个未暂存的Markdown文件。然后使用ChangeDetector类添加未暂存的Markdown文件，并检查文件是否被暂存。\n\n在tearDownClass()方法中，清理了测试仓库。\n\n**注意**: \n- 在使用TestChangeDetector类之前，需要先安装并导入unittest和os模块。\n- 在执行测试用例之前，需要确保已经安装了Git，并配置了正确的Git用户信息。\n- 在执行测试用例之后，可以调用tearDownClass()方法清理测试仓库。"
      ],
      "code_start_line": 6,
      "code_end_line": 89,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class TestChangeDetector(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n\n    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n\n\n    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n\n\n    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n\n\n    @classmethod\n    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "setUpClass": {
      "type": "FunctionDef",
      "name": "setUpClass",
      "md_content": [
        "**setUpClass**: setUpClass函数的功能是在测试类中设置一些类级别的变量和操作。\n\n**参数**: cls (类对象) - 表示当前测试类的类对象。\n\n**代码描述**: setUpClass函数首先定义了一个测试仓库的路径，该路径是当前文件所在目录下的test_repo文件夹。然后，如果测试仓库文件夹不存在，则创建它。接下来，使用Repo.init方法初始化了一个Git仓库，并使用repo.git.config方法配置了Git用户信息。然后，创建了两个测试文件test_file.py和test_file.md，并向其中分别写入了一些内容。最后，使用repo.git.add和repo.git.commit方法模拟了Git操作，将文件添加和提交到仓库中。\n\n**注意**: 在使用setUpClass函数之前，需要确保已经创建了测试仓库，并且在测试仓库中存在test_file.md文件。"
      ],
      "code_start_line": 8,
      "code_end_line": 32,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_change_detector.py/TestChangeDetector/test_get_unstaged_mds"
      ]
    },
    "test_get_staged_pys": {
      "type": "FunctionDef",
      "name": "test_get_staged_pys",
      "md_content": [
        "**test_get_staged_pys**: test_get_staged_pys函数的功能是获取暂存的Python文件。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先创建一个新的Python文件并将其暂存。然后，使用ChangeDetector类检查暂存文件，并将结果保存在staged_files变量中。最后，断言新文件在暂存文件列表中，并打印出暂存的Python文件列表。\n**注意**: 在使用该函数之前，需要确保已经初始化了test_repo_path变量，并且已经导入了os和ChangeDetector类。"
      ],
      "code_start_line": 34,
      "code_end_line": 48,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "test_get_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_get_unstaged_mds",
      "md_content": [
        "**test_get_unstaged_mds**: test_get_unstaged_mds函数的功能是获取未暂存的Markdown文件。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先在测试仓库中创建一个Markdown文件，并向其中添加额外的内容。然后，使用ChangeDetector类获取未暂存的文件列表。最后，断言修改的文件在未暂存文件列表中，并打印出未暂存的Markdown文件列表。\n**注意**: 使用该函数前需要确保已经创建了测试仓库，并且在测试仓库中存在test_file.md文件。"
      ],
      "code_start_line": 51,
      "code_end_line": 64,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_change_detector.py/TestChangeDetector/setUpClass"
      ],
      "reference_who": []
    },
    "test_add_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_add_unstaged_mds",
      "md_content": [
        "**test_add_unstaged_mds**: test_add_unstaged_mds函数的功能是将未暂存的Markdown文件添加到暂存区。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先调用test_get_unstaged_mds函数，确保存在一个未暂存的Markdown文件。然后，使用ChangeDetector类创建一个change_detector对象，并将未暂存的文件添加到暂存区。接着，通过调用change_detector对象的get_to_be_staged_files函数，获取暂存操作后的未暂存文件列表。最后，使用self.assertEqual函数断言暂存操作后的未暂存文件列表长度为0。此外，函数还打印了暂存操作后剩余的未暂存Markdown文件数。\n**注意**: 在使用该函数之前，需要确保存在未暂存的Markdown文件。"
      ],
      "code_start_line": 67,
      "code_end_line": 82,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tearDownClass": {
      "type": "FunctionDef",
      "name": "tearDownClass",
      "md_content": [
        "**tearDownClass**: tearDownClass函数的作用是清理测试类的资源。\n**参数**: cls - 类方法的第一个参数，表示当前类。\n**代码描述**: tearDownClass函数用于清理测试类的资源。在函数内部，首先调用cls.repo.close()关闭测试仓库，然后使用os.system('rm -rf ' + cls.test_repo_path)命令删除测试仓库的路径。\n**注意**: 使用该函数时需要确保已经创建了测试仓库，并且在测试类的最后调用该函数以释放资源。"
      ],
      "code_start_line": 86,
      "code_end_line": 89,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "tests/test_structure_tree.py": {
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [
        "**build_path_tree**: build_path_tree函数的功能是构建路径树。\n**parameters**: build_path_tree函数的参数有三个：\n- who_reference_me: 一个包含路径的列表，表示引用我的对象的路径列表。\n- reference_who: 一个包含路径的列表，表示我引用的对象的路径列表。\n- doc_item_path: 表示文档项的路径。\n\n**Code Description**: build_path_tree函数首先定义了一个内部函数tree，用于创建一个默认字典的树结构。然后，函数创建了一个path_tree变量，用于存储路径树。\n\n接下来，函数使用两个循环遍历who_reference_me和reference_who列表中的路径。对于每个路径，函数将其拆分为多个部分，并使用这些部分构建路径树。最后，函数处理doc_item_path，将其拆分为多个部分，并在最后一个对象前面加上星号。然后，函数使用这些部分构建路径树。\n\n函数还定义了一个内部函数tree_to_string，用于将路径树转换为字符串。该函数使用递归的方式遍历路径树，并将每个节点的键和值添加到字符串中。\n\n最后，函数返回路径树的字符串表示。\n\n**Note**: 在使用build_path_tree函数时，需要确保传入正确的参数，并且参数的格式符合要求。此外，函数依赖于os模块，因此需要确保os模块已经导入。\n\n**Output Example**: \n```\nwho_reference_me\n    path1\n        subpath1\n            ✳️doc_item_path\n    path2\n        subpath2\n            ✳️doc_item_path\nreference_who\n    path3\n        subpath3\n            ✳️doc_item_path\n    path4\n        subpath4\n            ✳️doc_item_path\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 31,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [
        "**tree**: tree函数的功能是返回一个默认字典的树结构。\n\n**parameters**: tree函数没有参数。\n\n**Code Description**: tree函数通过调用defaultdict(tree)来创建一个默认字典的树结构。默认字典是一种特殊的字典，它可以在访问不存在的键时自动创建一个默认值。在这里，我们使用tree作为默认值，这样就可以创建一个无限深度的树结构。函数返回创建的树结构。\n\n**Note**: 使用tree函数时需要注意以下几点：\n- tree函数没有参数，直接调用即可。\n- 返回的树结构是一个默认字典，可以通过键值对的方式进行访问和操作。\n\n**Output Example**: 假设调用tree函数后，返回的树结构为{'A': {'B': {'C': {}, 'D': {}}, 'E': {}}, 'F': {}}。"
      ],
      "code_start_line": 5,
      "code_end_line": 6,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_structure_tree.py/build_path_tree/tree_to_string"
      ]
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [
        "**tree_to_string**: tree_to_string函数的功能是将树结构转换为字符串。\n**parameters**: tree_to_string函数的参数有两个：\n- tree: 一个树结构，以字典的形式表示。\n- indent: 可选参数，表示缩进的层数，默认为0。\n**Code Description**: tree_to_string函数通过递归的方式将树结构转换为字符串。它首先遍历树的每个节点，对于每个节点，它将节点的键加入到字符串中，并根据缩进层数添加相应的缩进。然后，如果节点的值是一个字典，它会递归调用tree_to_string函数，将该字典作为新的树结构进行处理。最后，函数返回生成的字符串。\n**Note**: 使用该函数时需要注意以下几点：\n- tree参数必须是一个字典，表示树结构。\n- indent参数表示缩进的层数，可以根据需要进行调整。\n**Output Example**: 假设tree参数为{'A': {'B': {'C': {}, 'D': {}}, 'E': {}}, 'F': {}}, 则函数的返回值为：\n```\nA\n    B\n        C\n        D\n    E\nF\n```"
      ],
      "code_start_line": 23,
      "code_end_line": 29,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_structure_tree.py/build_path_tree/tree"
      ],
      "reference_who": []
    }
  },
  "repo_agent/runner.py": {
    "need_to_generate": {
      "type": "FunctionDef",
      "name": "need_to_generate",
      "md_content": [
        "**need_to_generate**: need_to_generate函数的功能是判断是否需要生成文档。\n\n**参数**: need_to_generate函数接受两个参数：\n- doc_item: DocItem类型的参数，表示要检查的文件或目录。\n- ignore_list: List类型的参数，表示需要忽略的文档列表。\n\n**代码描述**: need_to_generate函数首先判断doc_item的item_status属性是否为DocItemStatus.doc_up_to_date。如果是，则说明文档已经生成且是最新的，直接返回False。接下来，函数获取doc_item的完整名称，并判断doc_item的item_type是否为[DocItemType._file, DocItemType._dir, DocItemType._repo]之一。如果是，则说明doc_item是文件、目录或仓库级别的对象，不需要生成文档，直接返回False。然后，函数将doc_item的father属性赋值给doc_item，即将doc_item指向其父级对象。接着，函数进入一个循环，判断doc_item是否存在。如果存在，则继续执行循环体内的代码。在循环体内，函数首先判断doc_item的item_type是否为DocItemType._file。如果是，则说明当前对象是文件级别的对象。接下来，函数会遍历ignore_list中的每个元素，判断rel_file_path是否以ignore_item开头。如果是，则说明当前文件在忽略列表中，直接返回False。如果不是，则说明当前文件不在忽略列表中，返回True。最后，函数将doc_item的father属性赋值给doc_item，即将doc_item指向其父级对象。循环继续执行，直到doc_item不存在。最后，如果循环结束后仍未返回True，则说明当前对象及其父级对象都不需要生成文档，返回False。\n\n**注意**: 在调用need_to_generate函数时，需要传入正确的doc_item对象和ignore_list列表。另外，需要确保doc_item对象正确实现了相关属性和方法。\n\n**输出示例**: 假设doc_item的item_status为DocItemStatus.doc_up_to_date，则函数返回False。"
      ],
      "code_start_line": 21,
      "code_end_line": 36,
      "parent": null,
      "params": [
        "doc_item",
        "ignore_list"
      ],
      "have_return": true,
      "code_content": "def need_to_generate(doc_item: DocItem, ignore_list: List) -> bool:\n    \"\"\"只生成item的，文件及更高粒度都跳过。另外如果属于一个blacklist的文件也跳过\"\"\"\n    if doc_item.item_status == DocItemStatus.doc_up_to_date:\n        return False\n    rel_file_path = doc_item.get_full_name()\n    if doc_item.item_type in [DocItemType._file, DocItemType._dir, DocItemType._repo]:\n        return False\n    doc_item = doc_item.father\n    while doc_item:\n        if doc_item.item_type == DocItemType._file:\n            # 如果当前文件在忽略列表中，或者在忽略列表某个文件路径下，则跳过\n            if any(rel_file_path.startswith(ignore_item) for ignore_item in ignore_list):\n                return False\n            return True\n        doc_item = doc_item.father\n    return False\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/runner.py/load_whitelist",
        "repo_agent/runner.py/Runner",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/markdown_refresh",
        "repo_agent/runner.py/Runner/run",
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/update_existing_item",
        "repo_agent/runner.py/Runner/update_object",
        "repo_agent/runner.py/Runner/get_new_objects",
        "repo_agent/runner.py/recursive_check",
        "repo_agent/runner.py/to_markdown"
      ]
    },
    "load_whitelist": {
      "type": "FunctionDef",
      "name": "load_whitelist",
      "md_content": [
        "**load_whitelist**: load_whitelist函数的功能是加载白名单数据。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先判断CONFIG中的\"whitelist_path\"是否为None，如果不为None，则断言该路径存在。然后使用json库打开该路径下的json文件，并将数据加载到white_list_json_data变量中。最后，将white_list_json_data作为函数的返回值。如果\"whitelist_path\"为None，则返回None。\n**注意**: 使用该函数前需要确保CONFIG中的\"whitelist_path\"是正确的json文件路径，并且该文件存在。\n**输出示例**: 假设white_list_json_data的值为[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]，则函数的返回值为[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]。"
      ],
      "code_start_line": 38,
      "code_end_line": 47,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def load_whitelist():\n    if CONFIG[\"whitelist_path\"] != None:\n        assert os.path.exists(CONFIG[\"whitelist_path\"]), f\"whitelist_path must be a json-file,and must exists: {CONFIG['whitelist_path']}\"\n        with open(CONFIG[\"whitelist_path\"], \"r\") as reader:\n            white_list_json_data = json.load(reader)\n        # for i in range(len(white_list_json_data)):\n        #     white_list_json_data[i][\"file_path\"] = white_list_json_data[i][\"file_path\"].replace(\"https://github.com/huggingface/transformers/blob/v4.36.1/\",\"\")\n        return white_list_json_data\n    else:\n        return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "Runner": {
      "type": "ClassDef",
      "name": "Runner",
      "md_content": [
        "**Runner**: Runner的功能是生成文档并更新项目中的文档。\n\n**属性**：Runner具有以下属性：\n- project_manager: 一个ProjectManager对象，用于管理项目的路径和层次结构。\n- change_detector: 一个ChangeDetector对象，用于检测代码的变更。\n- chat_engine: 一个ChatEngine对象，用于生成文档内容。\n- meta_info: 一个MetaInfo对象，用于存储文档的元信息。\n- runner_lock: 一个线程锁对象，用于保证多线程操作的安全性。\n\n**代码描述**：Runner类是用于生成和更新项目文档的核心类。它包含了生成文档的主要逻辑和相关的辅助方法。在初始化时，Runner会创建一个ProjectManager对象、一个ChangeDetector对象和一个ChatEngine对象，并根据配置文件初始化MetaInfo对象。Runner类提供了生成所有文档、运行文档更新过程和处理文件变更的方法。\n\n- get_all_pys(directory)方法：该方法用于获取给定目录中的所有Python文件的路径。\n\n- generate_doc_for_a_single_item(doc_item)方法：该方法用于为一个对象生成文档。它首先检查是否需要生成文档，然后调用ChatEngine对象的generate_doc方法生成文档内容，并将内容添加到DocItem对象的md_content属性中。\n\n- first_generate()方法：该方法用于生成所有文档。它首先获取需要生成文档的任务列表，然后使用多线程并发执行生成文档的任务。生成文档完成后，更新MetaInfo对象的文档版本信息。\n\n- markdown_refresh()方法：该方法用于将最新的文档信息写入到Markdown格式的文件夹中。它遍历MetaInfo对象中的所有文件项，将每个文件的文档内容转换为Markdown格式，并写入对应的.md文件中。\n\n- git_commit(commit_message)方法：该方法用于提交代码变更。\n\n- run()方法：该方法用于运行文档更新过程。它首先判断文档版本，如果版本为空，则执行首次生成文档的过程；否则，执行文档更新过程。文档更新过程包括检测代码变更、生成文档、更新文档版本和刷新Markdown文档。\n\n- add_new_item(file_handler, json_data)方法：该方法用于向JSON文件中添加新的项目，并生成相应的文档。它首先获取文件的结构信息，然后调用ChatEngine对象的generate_doc方法生成文档内容，并将内容添加到新项目的结构信息中。最后，将更新后的结构信息写入JSON文件，并将文档内容转换为Markdown格式写入对应的.md文件中。\n\n- process_file_changes(repo_path, file_path, is_new_file)方法：该方法用于处理文件变更。根据文件的绝对路径，包括新文件和已存在文件，处理变更文件的逻辑。它首先获取变更文件的操作器，然后根据变更的行数和文件的结构信息，识别出变更的对象。根据对象的类型，更新JSON文件中的结构信息，并将文档内容转换为Markdown格式写入对应的.md文件中。\n\n- update_existing_item(file_dict, file_handler, changes_in_pyfile)方法：该方法用于更新已存在的项目。它根据变更的对象和引用关系，更新JSON文件中的结构信息，并将文档内容转换为Markdown格式写入对应的.md文件中。\n\n- update_object(file_dict, file_handler, obj_name, obj_referencer_list)方法：该方法用于生成文档内容并更新对象的相关字段信息。它调用ChatEngine对象的generate_doc方法生成文档内容，并将内容更新到对象的md_content字段中。\n\n**注意**：在运行文档更新过程时，需要保证目标仓库代码不能修改，即一个文档的生成过程必须绑定代码为一个版本。\n\n**输出示例**：无",
        "**Runner**: Runner类的功能是生成文档并更新项目中的文档。\n\n**属性**: Runner类具有以下属性：\n- project_manager: 一个ProjectManager对象，用于管理项目的路径和层次结构。\n- change_detector: 一个ChangeDetector对象，用于检测代码的变更。\n- chat_engine: 一个ChatEngine对象，用于生成文档内容。\n- meta_info: 一个MetaInfo对象，用于存储文档的元信息。\n- runner_lock: 一个threading.Lock对象，用于线程同步。\n\n**代码描述**: Runner类包含了生成文档和更新文档的功能。在初始化Runner对象时，会创建一个ProjectManager对象、一个ChangeDetector对象和一个ChatEngine对象，并根据配置文件初始化MetaInfo对象。在生成文档时，Runner类会根据变更的Python文件，逐个处理文件并更新相应的文档。在更新文档时，Runner类会根据文件的变更信息，更新文件的结构信息和文档内容，并将更新后的内容写入到Markdown文件中。\n\n**注意**: 在使用Runner类时，需要确保相关的配置文件已正确设置，并且需要正确实现相关的属性和方法。\n\n**输出示例**: 假设Runner对象的project_manager属性为一个ProjectManager对象，则示例输出如下：\nRaw code:```\nclass Runner:\n    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n        self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n        self.runner_lock = threading.Lock()\n\n    def run(self):\n        \"\"\"\n        运行文档更新过程。\n\n        该方法检测代码的变更，处理每个变更的文件，并相应地更新文档。\n\n        返回值:\n            None\n        \"\"\"\n        # 生成文档的过程...\n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        向JSON文件中添加新的项目，并生成相应的文档。\n\n        参数:\n            file_handler (FileHandler): 用于读写文件的FileHandler对象。\n            json_data (dict): 存储项目结构信息的JSON数据。\n\n        返回值:\n            None\n        \"\"\"\n        # 添加新项目的过程...\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        根据文件的绝对路径处理文件的变更，包括新文件和已存在的文件。\n\n        参数:\n            repo_path (str): 仓库的路径。\n            file_path (str): 文件的相对路径。\n            is_new_file (bool): 表示文件是否是新文件。\n\n        返回值:\n            None\n        \"\"\"\n        # 处理文件变更的过程...\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        更新已存在的项目。\n\n        参数:\n            file_dict (dict): 包含文件结构信息的字典。\n            file_handler (FileHandler): FileHandler对象。\n            changes_in_pyfile (dict): 包含文件中变更对象信息的字典。\n\n        返回值:\n            dict: 更新后的文件结构信息字典。\n        \"\"\"\n        # 更新已存在项目的过程...\n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        生成文档内容并更新对象的相应字段信息。\n\n        参数:\n            file_dict (dict): 包含旧对象信息的字典。\n            file_handler: FileHandler对象。\n            obj_name (str): 对象的名称。\n            obj_referencer_list (list): 对象的引用者列表。\n\n        返回值:\n            None\n        \"\"\"\n        # 生成文档内容并更新对象信息的过程...\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        通过比较当前版本和上一个版本的.py文件，获取新增和删除的对象。\n\n        参数:\n            file_handler (FileHandler): FileHandler对象。\n\n        返回值:\n            tuple: 包含新增和删除对象的元组，格式为(new_obj, del_obj)\n\n        输出示例:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        # 获取新增和删除对象的过程...\n```"
      ],
      "code_start_line": 49,
      "code_end_line": 452,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class Runner:\n    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        self.runner_lock = threading.Lock()\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n    \n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        rel_file_path = doc_item.get_full_name()\n\n        ignore_list = CONFIG.get('ignore_list', [])\n        if not need_to_generate(doc_item, ignore_list):\n            logger.info(f\"内容被忽略/文档已生成，跳过：{doc_item.get_full_name()}\")\n        else:\n            logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...\")\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            response_message = self.chat_engine.generate_doc(\n                doc_item = doc_item,\n                file_handler = file_handler,\n            )\n            doc_item.md_content.append(response_message.content)\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        \n\n    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(check_task_available_func) #将按照此顺序生成文档\n        # topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"成功生成了 {before_task_len - len(task_manager.task_dict)} 个文档\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {before_task_len - len(task_manager.task_dict)} docs are generated at this time\")\n\n    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        with self.runner_lock:\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n                if recursive_check(file_item) == False:\n                    # logger.info(f\"不存在文档内容，跳过：{file_item.get_full_name()}\")\n                    continue\n                rel_file_path = file_item.get_full_name()\n                # file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n                    \n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n                # 写入markdown内容到.md文件\n                file_path = os.path.join(CONFIG['Markdown_Docs_folder'], file_item.get_file_name().replace('.py', '.md'))\n                if file_path.startswith('/'):\n                    # 移除开头的 '/'\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, 'w', encoding='utf-8') as file:\n                    file.write(markdown)\n\n            logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n\n    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n\n\n    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(self.meta_info.target_repo_hierarchical_tree,task_available_func=check_task_available_func)\n        self.meta_info.print_task_list([cont.extra_info for cont in task_manager.task_dict.values()])\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        \n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n        \n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\") # 提交变更\n         \n\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n    \n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n\n\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化Runner对象。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: 在这个函数中，首先创建了一个ProjectManager对象，传入了两个参数repo_path和project_hierarchy，这两个参数的值分别来自于CONFIG字典的'repo_path'和'project_hierarchy'键对应的值。然后创建了一个ChangeDetector对象，传入了一个参数repo_path，该参数的值也来自于CONFIG字典的'repo_path'键对应的值。接下来创建了一个ChatEngine对象，传入了一个参数CONFIG，该参数的值为CONFIG字典。然后通过判断指定路径是否存在，来决定是通过MetaInfo的init_from_project_path方法初始化一个MetaInfo对象，还是通过MetaInfo的from_checkpoint_path方法从指定路径加载一个MetaInfo对象。然后将加载的MetaInfo对象的white_list属性赋值为load_whitelist函数的返回值。最后创建了一个threading.Lock对象，赋值给了runner_lock属性。\n\n**注意**: 在使用该代码时需要注意以下几点：\n- 需要确保CONFIG字典中'repo_path'和'project_hierarchy'键对应的值是正确的。\n- 需要确保指定路径下存在相应的文件或目录。"
      ],
      "code_start_line": 50,
      "code_end_line": 62,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        self.runner_lock = threading.Lock()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_all_pys": {
      "type": "FunctionDef",
      "name": "get_all_pys",
      "md_content": [
        "**get_all_pys**: get_all_pys函数的功能是获取给定目录中的所有Python文件。\n**参数**: 这个函数的参数。\n**代码描述**: 这个函数的描述。\nget_all_pys函数接受一个目录作为参数，并返回一个包含所有Python文件路径的列表。函数使用os.walk()遍历给定目录及其子目录中的所有文件。对于每个文件，函数检查文件名是否以'.py'结尾，如果是，则将文件的完整路径添加到python_files列表中。最后，函数返回python_files列表作为结果。\n**注意**: 使用这段代码需要注意的事项。\n这个函数只能获取给定目录及其子目录中的Python文件，不能获取其他类型的文件。\n**输出示例**: 模拟代码返回值的可能外观。\n假设给定目录中有两个Python文件，分别位于目录A和目录B中，函数将返回一个包含这两个文件路径的列表。例如，如果给定目录为'/home/user/project'，则函数的返回值可能是['/home/user/project/A/file1.py', '/home/user/project/B/file2.py']。"
      ],
      "code_start_line": 64,
      "code_end_line": 81,
      "parent": "Runner",
      "params": [
        "self",
        "directory"
      ],
      "have_return": true,
      "code_content": "    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "generate_doc_for_a_single_item": {
      "type": "FunctionDef",
      "name": "generate_doc_for_a_single_item",
      "md_content": [
        "**generate_doc_for_a_single_item**: generate_doc_for_a_single_item函数的功能是为一个对象生成文档。\n**parameters**: 这个函数的参数是doc_item: DocItem。\n**Code Description**: 这个函数首先获取doc_item的完整名称，然后根据配置文件中的ignore_list判断是否需要生成文档。如果不需要生成文档，则打印日志信息并跳过。如果需要生成文档，则打印日志信息并开始生成文档。生成文档的过程包括使用FileHandler处理文件，调用chat_engine的generate_doc方法生成文档内容，并将生成的内容添加到doc_item的md_content中。最后更新doc_item的状态为DocItemStatus.doc_up_to_date，并调用meta_info的checkpoint方法进行检查点操作。\n**Note**: 使用这个函数时需要注意以下几点：\n- 需要传入一个DocItem对象作为参数。\n- 需要在配置文件中配置ignore_list，用于判断是否需要生成文档。\n- 生成文档的过程中会使用FileHandler处理文件，需要确保FileHandler类已经正确实现。\n- 生成文档的过程中会调用chat_engine的generate_doc方法，需要确保chat_engine对象已经正确初始化并实现了generate_doc方法。\n- 生成文档完成后会更新doc_item的状态为DocItemStatus.doc_up_to_date，需要确保doc_item对象正确实现了相关属性和方法。"
      ],
      "code_start_line": 84,
      "code_end_line": 101,
      "parent": "Runner",
      "params": [
        "self",
        "doc_item"
      ],
      "have_return": false,
      "code_content": "    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        rel_file_path = doc_item.get_full_name()\n\n        ignore_list = CONFIG.get('ignore_list', [])\n        if not need_to_generate(doc_item, ignore_list):\n            logger.info(f\"内容被忽略/文档已生成，跳过：{doc_item.get_full_name()}\")\n        else:\n            logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...\")\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            response_message = self.chat_engine.generate_doc(\n                doc_item = doc_item,\n                file_handler = file_handler,\n            )\n            doc_item.md_content.append(response_message.content)\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "first_generate": {
      "type": "FunctionDef",
      "name": "first_generate",
      "md_content": [
        "**first_generate**: first_generate函数的作用是生成所有文档。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: 这个函数首先通过读取配置文件中的ignore_list来获取需要忽略的文档列表。然后通过调用need_to_generate函数判断是否需要生成文档。接下来，函数会获取文档生成的拓扑结构，并将拓扑结构传递给任务管理器task_manager。任务管理器会根据拓扑结构的顺序生成文档。然后，函数会设置同步函数为markdown_refresh，并创建多个线程来执行生成文档的任务。每个线程会调用generate_doc_for_a_single_item函数来生成单个文档。生成文档过程中，函数会更新文档版本号，并将生成的文档同步到文件系统中。最后，函数会检查生成的文档数量，并将生成的文档数量和错误信息记录到日志中。\n\n**注意**: 在生成文档的过程中，目标仓库的代码不能被修改，因为文档的生成过程必须绑定到特定的代码版本。"
      ],
      "code_start_line": 104,
      "code_end_line": 135,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(check_task_available_func) #将按照此顺序生成文档\n        # topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"成功生成了 {before_task_len - len(task_manager.task_dict)} 个文档\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {before_task_len - len(task_manager.task_dict)} docs are generated at this time\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "markdown_refresh": {
      "type": "FunctionDef",
      "name": "markdown_refresh",
      "md_content": [
        "**markdown_refresh**: markdown_refresh函数的功能是将目前最新的document信息写入到一个markdown格式的文件夹里。\n\n**parameters**: 该函数没有参数。\n\n**Code Description**: 该函数首先通过获取所有文件的元信息，然后遍历每个文件，检查文件内是否存在文档内容。如果文件内不存在文档内容，则跳过该文件。接下来，该函数定义了一个递归函数recursive_check，用于检查一个文件内是否存在文档内容。如果文件内存在文档内容，则将该文件的元信息转换为markdown格式的文本。函数to_markdown用于将DocItem对象转换为markdown格式的文本，包括标题、参数和文档内容。最后，将markdown内容写入到.md文件中。\n\n**Note**: \n- 在运行该函数之前，需要确保已经设置了CONFIG['repo_path']和CONFIG['Markdown_Docs_folder']的值。\n- 该函数使用了tqdm库来显示进度条。\n- 如果markdown内容为空，则会抛出异常。\n\n**Output Example**: \n```\n# 文件 file_name\n文档内容...\n\n## 函数 function_name(param1, param2)\n文档内容...\n```",
        "**markdown_refresh**: markdown_refresh函数的功能是将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)。\n\n**参数**: markdown_refresh函数没有接受任何参数。\n\n**代码描述**: markdown_refresh函数首先通过self.runner_lock获取一个锁，以确保在写入markdown文件时的线程安全。接下来，函数通过self.meta_info.get_all_files()获取所有的file_item列表。然后，函数遍历file_item_list中的每个file_item。在循环体内，函数定义了一个名为recursive_check的内部函数，用于检查一个file内是否存在doc。recursive_check函数首先判断doc_item的md_content是否为空列表。如果不为空，则说明该文件存在文档内容，返回True。如果为空，则继续遍历doc_item的children属性中的每个子项，并递归调用recursive_check函数。如果递归调用的结果为True，则说明该文件存在文档内容，返回True。如果遍历完所有子项后仍未返回True，则说明该文件不存在文档内容，返回False。接下来，markdown_refresh函数调用recursive_check函数检查当前file_item是否存在文档内容。如果不存在文档内容，则跳过该文件。如果存在文档内容，则获取file_item的完整名称，并定义一个名为to_markdown的内部函数，用于将doc_item转换为markdown格式的内容。to_markdown函数首先定义一个空字符串markdown_content，用于存储转换后的markdown内容。然后，to_markdown函数根据doc_item的item_type和obj_name生成markdown标题，并将其添加到markdown_content中。接下来，to_markdown函数判断doc_item的content属性中是否存在params键，并且params键对应的列表长度大于0。如果满足条件，则将params列表中的元素以逗号分隔的形式添加到markdown_content中。然后，to_markdown函数将doc_item的md_content属性中的最后一个元素添加到markdown_content中，如果md_content为空，则添加默认提示文本。最后，to_markdown函数遍历doc_item的children属性中的每个子项，并递归调用to_markdown函数，将返回的markdown内容添加到markdown_content中。最终，to_markdown函数返回markdown_content。接下来，markdown_refresh函数定义一个空字符串markdown，用于存储所有文件的markdown内容。然后，markdown_refresh函数遍历file_item的children属性中的每个子项，并调用to_markdown函数将子项转换为markdown格式的内容，并将转换后的内容添加到markdown中。接着，markdown_refresh函数判断markdown是否为空。如果为空，则抛出异常，提示markdown内容为空。如果不为空，则根据CONFIG['Markdown_Docs_folder']和file_item的文件名生成.md文件的路径，并创建该路径的文件夹。最后，markdown_refresh函数将markdown内容写入.md文件。\n\n**注意**: 在调用markdown_refresh函数之前，需要确保self.runner_lock、self.meta_info和CONFIG['Markdown_Docs_folder']等属性和变量的值正确设置。\n\n**输出示例**: markdown文档已经刷新，路径为CONFIG['Markdown_Docs_folder']。"
      ],
      "code_start_line": 137,
      "code_end_line": 180,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        with self.runner_lock:\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n                if recursive_check(file_item) == False:\n                    # logger.info(f\"不存在文档内容，跳过：{file_item.get_full_name()}\")\n                    continue\n                rel_file_path = file_item.get_full_name()\n                # file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n                    \n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n                # 写入markdown内容到.md文件\n                file_path = os.path.join(CONFIG['Markdown_Docs_folder'], file_item.get_file_name().replace('.py', '.md'))\n                if file_path.startswith('/'):\n                    # 移除开头的 '/'\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, 'w', encoding='utf-8') as file:\n                    file.write(markdown)\n\n            logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "git_commit": {
      "type": "FunctionDef",
      "name": "git_commit",
      "md_content": [
        "**git_commit**: git_commit函数的功能是将更改提交到Git仓库。\n**parameters**: 这个函数的参数是commit_message，表示提交的消息。\n**Code Description**: 这个函数使用了subprocess模块来执行git命令，将更改提交到Git仓库。在try块中，使用subprocess.check_call函数来执行git commit命令，传入的参数是['git', 'commit', '--no-verify', '-m', commit_message]，其中commit_message是传入的参数。如果执行命令出现异常，会捕获subprocess.CalledProcessError异常，并打印出错误信息。\n**Note**: 使用这个函数时需要注意以下几点：\n- commit_message参数是必需的，表示提交的消息。\n- 在执行git commit命令时，会使用'--no-verify'选项来跳过验证。\n- 如果执行git commit命令出现异常，会打印出错误信息。"
      ],
      "code_start_line": 182,
      "code_end_line": 186,
      "parent": "Runner",
      "params": [
        "self",
        "commit_message"
      ],
      "have_return": false,
      "code_content": "    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "run": {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: run函数的功能是运行文档更新流程。\n**参数**: 无\n**代码描述**: 这个函数会检测已更改的Python文件，处理每个文件，并相应地更新文档。\n首先，函数会检查self.meta_info.document_version是否为空。如果为空，说明文档还没有生成过，会调用self.first_generate()函数进行初次生成，并对目标目录路径进行检查点操作。然后函数会返回。\n接下来，如果self.meta_info.in_generation_process为False，表示不在生成文档的过程中，函数会打印\"Starting to detect changes.\"的日志信息。\n然后，函数会创建一个新的MetaInfo对象new_meta_info，并从旧的meta_info中加载文档信息。将new_meta_info赋值给self.meta_info，并将self.meta_info.in_generation_process设置为True。\n接下来，函数会根据配置文件中的ignore_list创建一个任务管理器task_manager，并设置任务可用性检查函数为need_to_generate。然后，函数会打印任务列表的信息。\n接下来，函数会设置task_manager的同步函数为self.markdown_refresh，并创建多个线程，每个线程都会调用worker函数，并传入task_manager、process_id和self.generate_doc_for_a_single_item函数作为参数。然后，启动每个线程并等待线程结束。\n接下来，函数会将self.meta_info.in_generation_process设置为False，并将self.meta_info.document_version设置为当前代码库的最新提交的commit的hexsha。\n最后，函数会对目标目录路径进行检查点操作，并打印\"Doc has been forwarded to the latest version\"的日志信息。然后，调用self.markdown_refresh()函数。\n**注意**: 这个函数会根据代码的变化来更新文档，并使用多线程进行处理。\n**输出示例**: 无"
      ],
      "code_start_line": 189,
      "code_end_line": 245,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(self.meta_info.target_repo_hierarchical_tree,task_available_func=check_task_available_func)\n        self.meta_info.print_task_list([cont.extra_info for cont in task_manager.task_dict.values()])\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "add_new_item": {
      "type": "FunctionDef",
      "name": "add_new_item",
      "md_content": [
        "**add_new_item**: add_new_item函数的作用是将新的项目添加到JSON文件中，并生成相应的文档。\n\n**参数**：该函数接受两个参数：\n- file_handler (FileHandler): 用于读写文件的文件处理器对象。\n- json_data (dict): 存储项目结构信息的JSON数据。\n\n**代码描述**：该函数首先创建一个空的文件字典file_dict。然后，通过调用file_handler的get_functions_and_classes方法，获取文件中的所有函数和类的结构信息。对于每个结构信息，函数会调用file_handler的get_obj_code_info方法，获取对象的代码信息。接着，函数会调用self.chat_engine的generate_doc方法，生成对象的文档，并将文档内容存储在md_content变量中。然后，将md_content添加到code_info字典中。接下来，函数会将新的对象添加到file_dict字典中。最后，函数将file_dict添加到json_data中，并将json_data写入json文件。接着，函数会调用file_handler的convert_to_markdown_file方法，将json文件的变更部分转换为markdown内容。最后，函数将markdown内容写入.md文件。\n\n**注意**：在调用add_new_item函数之前，需要确保已经创建了合适的file_handler和json_data对象，并将它们作为参数传递给add_new_item函数。此外，需要确保已经初始化了self.chat_engine、self.project_manager和logger对象。在函数执行完毕后，将会生成新的项目的结构信息，并将其写入json文件和.md文件中。"
      ],
      "code_start_line": 248,
      "code_end_line": 278,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler",
        "json_data"
      ],
      "have_return": false,
      "code_content": "    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "process_file_changes": {
      "type": "FunctionDef",
      "name": "process_file_changes",
      "md_content": [
        "**process_file_changes**: process_file_changes函数的作用是处理根据绝对文件路径处理更改的文件，包括新文件和现有文件。其中，changes_in_pyfile是一个包含更改结构信息的字典。示例格式为：{'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n**parameters**: \n- repo_path (str): 仓库路径。\n- file_path (str): 文件的相对路径。\n- is_new_file (bool): 表示文件是否为新文件。\n\n**Code Description**: \n该函数首先创建一个FileHandler对象，用于操作变更文件。然后，通过调用FileHandler对象的read_file方法获取整个py文件的代码。接下来，使用change_detector对象的parse_diffs方法解析文件的差异，并使用get_functions_and_classes方法获取文件中的函数和类。然后，调用change_detector对象的identify_changes_in_structure方法识别文件中的结构变更，并将结果保存在changes_in_pyfile变量中。之后，通过打开project_hierarchy.json文件，判断是否能够找到对应.py文件路径的项。如果找到了对应文件，将更新json文件中的内容，并将更新后的file写回到json文件中。然后，将变更部分的json文件内容转换成markdown内容，并将markdown内容写入.md文件。如果没有找到对应的文件，就添加一个新的项。最后，将run过程中更新的Markdown文件添加到暂存区。\n\n**Note**: \n- 在使用该函数时，需要传入正确的参数，包括仓库路径、文件相对路径和文件是否为新文件的标识。\n- 在调用该函数之前，需要确保已经创建了FileHandler对象和change_detector对象，并且已经正确设置了相关属性。\n- 在使用该函数之前，需要确保已经正确设置了logger对象、project_manager对象和CONFIG变量。\n- 在使用该函数之前，需要确保已经正确导入了FileHandler类、logger模块、json模块和os模块。"
      ],
      "code_start_line": 281,
      "code_end_line": 329,
      "parent": "Runner",
      "params": [
        "self",
        "repo_path",
        "file_path",
        "is_new_file"
      ],
      "have_return": false,
      "code_content": "    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "update_existing_item": {
      "type": "FunctionDef",
      "name": "update_existing_item",
      "md_content": [
        "**update_existing_item**: update_existing_item函数的功能是更新现有项目。\n**参数**：该函数接受三个参数：\n- file_dict (dict): 包含文件结构信息的字典。\n- file_handler (FileHandler): 文件处理器对象。\n- changes_in_pyfile (dict): 包含文件中发生变化的对象信息的字典。\n**代码描述**：该函数首先通过调用get_new_objects方法获取新对象和被删除的对象。然后，对于每个被删除的对象，如果该对象在file_dict中存在，则从file_dict中删除该对象，并记录日志。接下来，该函数通过调用file_handler的generate_file_structure方法生成文件的结构信息，得到当前文件中的所有对象，并将其存储在current_objects字典中。然后，该函数遍历current_info_dict字典，如果当前对象在file_dict中存在，则更新file_dict中该对象的信息，否则将该对象添加到file_dict中。接下来，该函数遍历changes_in_pyfile['added']列表中的每个对象，对于每个对象，遍历current_objects字典，找到与该对象名称匹配的current_object，并调用project_manager的find_all_referencer方法获取该对象的引用者列表，并将该对象的名称和引用者列表存储在referencer_list中。然后，该函数使用线程池并发执行update_object方法，该方法用于更新对象的文档。最后，该函数返回更新后的file_dict字典。\n**注意**：在更新对象的文档之前，该函数会先删除被删除的对象，并更新file_dict中已存在对象的信息。\n**输出示例**：返回更新后的file_dict字典。"
      ],
      "code_start_line": 335,
      "code_end_line": 406,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "changes_in_pyfile"
      ],
      "have_return": true,
      "code_content": "    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "update_object": {
      "type": "FunctionDef",
      "name": "update_object",
      "md_content": [
        "**update_object**: update_object函数的作用是生成文档内容并更新对象的相应字段信息。\n\n**参数**：这个函数的参数有：\n- file_dict (dict): 一个包含旧对象信息的字典。\n- file_handler: 文件处理器。\n- obj_name (str): 对象名称。\n- obj_referencer_list (list): 对象引用者列表。\n\n**代码描述**：这个函数首先检查file_dict中是否存在obj_name对应的对象。如果存在，就获取该对象并调用self.chat_engine.generate_doc函数生成文档内容。然后将生成的文档内容赋值给obj字典中的\"md_content\"字段。\n\n**注意**：在调用update_object函数时，需要传入正确的参数，并确保file_dict中存在obj_name对应的对象。另外，需要确保self.chat_engine.generate_doc函数能够正确生成文档内容，并将生成的内容赋值给obj字典中的\"md_content\"字段。"
      ],
      "code_start_line": 409,
      "code_end_line": 425,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "obj_name",
        "obj_referencer_list"
      ],
      "have_return": false,
      "code_content": "    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "get_new_objects": {
      "type": "FunctionDef",
      "name": "get_new_objects",
      "md_content": [
        "**get_new_objects**: get_new_objects函数的功能是通过比较.py文件的当前版本和先前版本来获取已添加和已删除的对象。\n\n**参数**：file_handler (FileHandler) - 文件处理器对象。\n\n**代码描述**：该函数首先通过调用file_handler.get_modified_file_versions()方法获取当前版本和先前版本的文件。然后，它调用file_handler.get_functions_and_classes()方法来解析当前版本和先前版本的.py文件，并将解析结果存储在parse_current_py和parse_previous_py变量中。如果先前版本不存在，则parse_previous_py为空列表。\n\n接下来，函数使用集合推导式将parse_current_py和parse_previous_py中的函数和类名分别存储在current_obj和previous_obj集合中。\n\n然后，函数通过计算current_obj和previous_obj的差集，得到new_obj和del_obj分别表示已添加和已删除的对象。最后，函数将new_obj和del_obj转换为列表，并作为元组返回。\n\n**注意**：在使用该函数时，需要确保传入正确的file_handler对象，并且该对象具有正确的方法和属性。\n\n**输出示例**：\nnew_obj: ['add_context_stack', '__init__']\ndel_obj: []"
      ],
      "code_start_line": 429,
      "code_end_line": 452,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "recursive_check": {
      "type": "FunctionDef",
      "name": "recursive_check",
      "md_content": [
        "**recursive_check**: recursive_check函数的功能是检查一个文件内是否存在文档。\n\n**参数**: \n- doc_item: DocItem类型的参数，表示要检查的文件或目录。\n- 返回值: 布尔类型，表示是否存在文档。\n\n**代码描述**: \nrecursive_check函数首先判断传入的doc_item的md_content属性是否为空列表，如果不为空，则说明该文件存在文档，直接返回True。接着，函数遍历doc_item的children属性，对每个子节点递归调用recursive_check函数。如果递归调用返回True，则说明子节点或其子节点存在文档，直接返回True。如果遍历完所有子节点后仍未返回True，则说明该文件及其子节点都不存在文档，返回False。\n\n**注意**: \n- 该函数只检查文件内是否存在文档，不会检查文件的父目录或整个项目的文档情况。\n- 该函数的参数doc_item必须是DocItem类型的对象。\n\n**输出示例**: \n假设传入的doc_item是一个文件对象，且该文件存在文档，则函数返回True。否则，函数返回False。"
      ],
      "code_start_line": 143,
      "code_end_line": 149,
      "parent": null,
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n",
      "name_column": 20,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "to_markdown": {
      "type": "FunctionDef",
      "name": "to_markdown",
      "md_content": [
        "**to_markdown**: to_markdown函数的功能是将DocItem对象转换为Markdown格式的字符串。\n**parameters**: to_markdown函数接受两个参数：\n- item: DocItem类型，表示要转换为Markdown的对象。\n- now_level: int类型，表示当前的层级。\n**Code Description**: to_markdown函数首先初始化一个空的markdown_content字符串。然后根据当前层级和对象的类型生成标题，并将其添加到markdown_content中。接下来，如果对象的content字典中存在params键且params列表不为空，则将参数列表添加到markdown_content中。然后，将对象的md_content列表的最后一个元素（如果列表不为空）或者字符串\"Doc has not been generated...\"添加到markdown_content中。最后，遍历对象的children字典，对每个子对象递归调用to_markdown函数，并将返回的字符串添加到markdown_content中。最终，返回markdown_content字符串。\n**Note**: 该函数用于将DocItem对象转换为Markdown格式的字符串，并递归处理子对象。\n**Output Example**: \n```\n## Function to_markdown(item: DocItem, now_level: int)\n将DocItem对象转换为Markdown格式的字符串。\n\n### 参数\n- item: DocItem类型，表示要转换为Markdown的对象。\n- now_level: int类型，表示当前的层级。\n\n### 返回值\n- str类型，表示转换后的Markdown字符串。\n\n注意：此示例仅为模拟，实际输出可能会根据输入的对象和层级而有所不同。\n```",
        "**to_markdown**: to_markdown函数的功能是将DocItem对象转换为Markdown格式的字符串。\n\n**参数**: to_markdown函数接受两个参数：\n- item: DocItem类型的参数，表示要转换为Markdown的对象。\n- now_level: int类型的参数，表示当前的层级。\n\n**代码描述**: to_markdown函数首先初始化一个空字符串markdown_content，用于存储转换后的Markdown内容。然后，函数根据当前层级now_level生成相应数量的\"#\"字符，并拼接上item的item_type和obj_name属性，形成标题。接下来，函数判断item的content属性中是否存在\"params\"键，并且params列表的长度是否大于0。如果满足条件，则将params列表中的参数用逗号连接起来，并拼接到标题后面。然后，函数在markdown_content中添加换行符。接着，函数判断item的md_content属性是否为空。如果不为空，则将md_content的最后一个元素添加到markdown_content中。如果为空，则在markdown_content中添加\"Doc has not been generated...\"。接下来，函数遍历item的children属性中的每个子对象，并递归调用to_markdown函数，将子对象的转换结果添加到markdown_content中。最后，函数返回markdown_content。\n\n**注意**: 在调用to_markdown函数时，需要传入正确的item对象和now_level参数。另外，需要确保item对象正确实现了相关属性和方法。\n\n**输出示例**: 假设item的item_type为DocItemType._file，obj_name为\"example.py\"，params为[\"param1\", \"param2\"]，md_content为[\"This is an example file.\"], children为空字典，则函数返回的字符串为：\n\"## _file example.py(param1, param2)\\nThis is an example file.\"\\"
      ],
      "code_start_line": 155,
      "code_end_line": 164,
      "parent": null,
      "params": [
        "item",
        "now_level"
      ],
      "have_return": true,
      "code_content": "                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n",
      "name_column": 20,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    }
  },
  "repo_agent/file_handler.py": {
    "FileHandler": {
      "type": "ClassDef",
      "name": "FileHandler",
      "md_content": [
        "**FileHandler**: FileHandler的功能是处理文件的读写操作。\n\n**attributes**: \n- file_path: 文件的相对路径\n- repo_path: 仓库的路径\n- project_hierarchy: 项目层级结构的路径\n\n**Code Description**: \nFileHandler类提供了一些方法来处理文件的读写操作。在初始化时，需要传入文件的相对路径和仓库的路径。其中，file_path是相对于仓库根目录的路径。在初始化过程中，还会根据配置文件中的project_hierarchy字段，生成项目层级结构的路径。\n\nread_file方法用于读取文件的内容。它会根据传入的相对路径和仓库路径，拼接出文件的绝对路径，然后使用utf-8编码打开文件，并读取文件内容。最后将文件内容作为字符串返回。\n\nget_obj_code_info方法用于获取给定对象的代码信息。它接受代码类型、代码名称、起始行号、结束行号、父级对象、参数和文件路径等参数。在方法内部，首先创建一个空的字典code_info，用于存储代码信息。然后根据传入的文件路径或者默认的文件路径，打开代码文件并读取所有行。根据起始行号和结束行号，截取出代码的内容。接着判断代码中是否包含return关键字，并将结果存储在have_return变量中。最后，将代码信息存储在code_info字典中，并返回该字典。\n\nwrite_file方法用于将内容写入文件。它接受文件路径和内容作为参数。在方法内部，首先判断文件路径是否以'/'开头，如果是，则移除开头的'/'。然后根据传入的文件路径和仓库路径，拼接出文件的绝对路径。接着创建文件的父文件夹（如果不存在），然后使用utf-8编码打开文件，并将内容写入文件。\n\nget_modified_file_versions方法用于获取修改文件的当前版本和上一个版本。它首先使用git库打开仓库，并读取当前工作目录下的文件内容作为当前版本。然后通过git库获取上一个提交的文件版本，并将其作为上一个版本。如果没有上一个版本，则将上一个版本设置为None。最后，返回当前版本和上一个版本。\n\nget_end_lineno方法用于获取给定节点的结束行号。它接受一个节点作为参数。在方法内部，首先判断节点是否具有行号属性，如果没有，则返回-1表示该节点没有行号。然后遍历节点的子节点，递归调用get_end_lineno方法，获取子节点的结束行号。如果子节点的结束行号大于-1，则更新结束行号为子节点的结束行号。最后返回结束行号。\n\nadd_parent_references方法用于为AST中的每个节点添加父节点引用。它接受当前节点和父节点作为参数。在方法内部，遍历当前节点的子节点，并将父节点设置为当前节点的父节点。然后递归调用add_parent_references方法，将当前节点作为子节点的父节点。\n\nget_functions_and_classes方法用于获取文件中的所有函数和类的信息。它接受文件的代码内容作为参数。在方法内部，首先使用ast库解析代码内容，生成AST树。然后调用add_parent_references方法，为AST树中的每个节点添加父节点引用。接着遍历AST树中的每个节点，如果节点是函数、类或异步函数定义节点，则获取节点的起始行号和结束行号，并获取父节点的名称和参数列表（如果有）。最后将函数和类的信息存储在一个列表中，并返回该列表。\n\ngenerate_file_structure方法用于生成给定文件路径的文件结构。它接受文件路径作为参数。在方法内部，首先打开文件，并读取文件内容。然后调用get_functions_and_classes方法，获取文件中的所有函数和类的信息。接着将函数和类的信息存储在一个字典中，并返回该字"
      ],
      "code_start_line": 14,
      "code_end_line": 292,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class FileHandler:\n    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n\n    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n\n    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n\n\n    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n        \n    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parent_name = node.parent.name if 'name' in dir(node.parent) else None\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                )\n        return functions_and_classes\n        \n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n    \n\n    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n    \n\n    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef'] and obj['params']:\n                params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化FileHandler对象。\n**参数**: 这个函数的参数如下：\n- repo_path (str): 仓库的路径。\n- file_path (str): 文件的路径。\n**代码描述**: 这个函数接受两个参数repo_path和file_path，并将它们分别赋值给self.repo_path和self.file_path。此外，它还使用os模块的join函数将repo_path、CONFIG['project_hierarchy']和\".project_hierarchy.json\"拼接起来，赋值给self.project_hierarchy。\n**注意**: 使用这个函数时需要注意以下几点：\n- file_path参数是相对于仓库根目录的路径。\n- 代码文件必须存在，并且需要有读取权限。\n**输出示例**: 这是一个可能的返回值的示例：\n```\n{\n    'file_path': 'file_path',\n    'repo_path': 'repo_path',\n    'project_hierarchy': 'repo_path/CONFIG['project_hierarchy']/.project_hierarchy.json'\n}\n```\nRaw code:```\n    def __init__(self, repo_path, file_path):\n        \"\"\"\n        Initialize the FileHandler object.\n\n        Args:\n            repo_path (str): The path of the repository.\n            file_path (str): The path of the file.\n\n        Returns:\n            None\n        \"\"\"\n        self.file_path = file_path\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n\n```"
      ],
      "code_start_line": 15,
      "code_end_line": 18,
      "parent": "FileHandler",
      "params": [
        "self",
        "repo_path",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/get_obj_code_info",
        "repo_agent/file_handler.py/FileHandler/get_end_lineno",
        "repo_agent/file_handler.py/FileHandler/add_parent_references",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes",
        "repo_agent/file_handler.py/FileHandler/generate_file_structure"
      ]
    },
    "read_file": {
      "type": "FunctionDef",
      "name": "read_file",
      "md_content": [
        "**read_file**: read_file函数的功能是读取文件内容。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先通过os模块的join方法将仓库路径和文件路径拼接成绝对路径，然后使用open函数以只读模式打开文件，并指定编码为utf-8。接着使用file.read()方法读取文件内容，并将内容赋值给变量content。最后返回content作为函数的返回值。\n**注意**: 在使用该函数之前，需要确保已经正确设置了仓库路径和文件路径。\n**输出示例**: 假设文件内容为\"Hello, World!\"，则函数的返回值为\"Hello, World!\"。"
      ],
      "code_start_line": 20,
      "code_end_line": 31,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_obj_code_info": {
      "type": "FunctionDef",
      "name": "get_obj_code_info",
      "md_content": [
        "**get_obj_code_info**: get_obj_code_info函数的功能是获取给定对象的代码信息。\n**parameters**: 这个函数的参数如下：\n- code_type (str): 代码的类型。\n- code_name (str): 代码的名称。\n- start_line (int): 代码的起始行号。\n- end_line (int): 代码的结束行号。\n- parent (str): 代码的父级。\n- file_path (str, optional): 文件路径。默认为None。\n\n**Code Description**: 这个函数的作用是根据给定的对象，获取其代码的相关信息。它首先创建一个空的字典code_info，然后将传入的参数和一些额外的信息添加到字典中。接下来，它打开代码文件，读取文件中的内容，并根据起始行号和结束行号提取出代码的内容。然后，它在第一行代码中查找对象名称的位置，并判断代码中是否包含'return'关键字。最后，它将所有的信息添加到code_info字典中，并将其作为返回值返回。\n\n**Note**: 使用这个函数时需要注意以下几点：\n- file_path参数是可选的，如果不传入file_path，则使用self.file_path作为默认值。\n- 代码文件必须存在，并且需要有读取权限。\n\n**Output Example**: 这是一个可能的返回值的示例：\n```\n{\n    'type': 'function',\n    'name': 'get_obj_code_info',\n    'md_content': [],\n    'code_start_line': 10,\n    'code_end_line': 20,\n    'parent': 'FileHandler',\n    'params': ['code_type', 'code_name', 'start_line', 'end_line', 'parent', 'params'],\n    'have_return': True,\n    'code_content': 'def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\\n    ...\\n',\n    'name_column': 4\n}\n```"
      ],
      "code_start_line": 33,
      "code_end_line": 75,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_type",
        "code_name",
        "start_line",
        "end_line",
        "parent",
        "params",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "write_file": {
      "type": "FunctionDef",
      "name": "write_file",
      "md_content": [
        "**write_file**: write_file函数的功能是将内容写入文件。\n**参数**：这个函数的参数。\n- file_path (str): 文件的相对路径。\n- content (str): 要写入文件的内容。\n\n**代码说明**：这个函数的作用是将content写入指定的文件中。首先，我们需要确保file_path是相对路径，如果以'/'开头，则将其移除。然后，通过os.path.join函数将repo_path和file_path拼接成绝对路径abs_file_path。接下来，使用os.makedirs函数创建abs_file_path的父目录，如果目录已存在则不做任何操作。最后，使用open函数以写入模式打开abs_file_path，并指定编码为utf-8。然后，使用file.write方法将content写入文件。\n\n**注意**：在使用这段代码时需要注意以下几点：\n- 确保传入的file_path是相对路径，否则可能会导致文件写入错误。\n- 确保传入的content是字符串类型，否则可能会导致写入失败或写入的内容不符合预期。"
      ],
      "code_start_line": 77,
      "code_end_line": 93,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path",
        "content"
      ],
      "have_return": false,
      "code_content": "    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_modified_file_versions": {
      "type": "FunctionDef",
      "name": "get_modified_file_versions",
      "md_content": [
        "**get_modified_file_versions**: get_modified_file_versions函数的作用是获取修改文件的当前版本和上一个版本。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先使用git.Repo方法创建一个repo对象，然后通过self.repo_path获取仓库路径。接下来，函数使用os.path.join方法将仓库路径和文件路径拼接成当前版本文件的路径。然后，函数使用open方法打开当前版本文件，并使用utf-8编码读取文件内容，将内容赋值给current_version变量。接着，函数使用repo.iter_commits方法获取最近一次提交的commit对象列表，限制只获取与文件路径相关的提交。如果存在commit对象，则取第一个commit对象，尝试通过commit.tree / self.file_path获取上一个版本文件的数据流，并使用utf-8解码，将解码后的内容赋值给previous_version变量。如果获取上一个版本文件的过程中出现KeyError异常，则将previous_version设置为None，表示文件可能是新添加的，不在之前的提交中。最后，函数返回一个包含当前版本和上一个版本的元组。\n**注意**: 使用该代码时需要注意以下几点：\n- 需要安装gitpython库。\n- 需要确保self.repo_path和self.file_path的值正确。\n**输出示例**: \ncurrent_version = \"This is the content of the current version of the file.\"\nprevious_version = \"This is the content of the previous version of the file.\""
      ],
      "code_start_line": 96,
      "code_end_line": 120,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_end_lineno": {
      "type": "FunctionDef",
      "name": "get_end_lineno",
      "md_content": [
        "**get_end_lineno**: get_end_lineno函数的作用是获取给定节点的结束行号。\n**参数**: 此函数的参数。\n**代码说明**: 此函数的说明。\nget_end_lineno函数接受一个节点作为参数，用于查找该节点的结束行号。\n\n如果节点没有lineno属性，则返回-1，表示该节点没有行号。\n\n如果节点有lineno属性，则将end_lineno初始化为节点的lineno。\n\n然后，遍历节点的所有子节点，对每个子节点递归调用get_end_lineno函数，获取子节点的结束行号。\n\n如果子节点的结束行号大于-1，则将end_lineno更新为子节点的结束行号。\n\n最后，返回end_lineno作为节点的结束行号。\n\n**注意**: 使用此代码的注意事项。\n- 此函数只适用于具有行号属性的节点。\n- 如果节点没有行号属性，则返回-1。\n\n**输出示例**: 模拟代码返回值的可能外观。\n```python\nnode = ast.parse(\"print('Hello, World!')\")\nend_lineno = get_end_lineno(node)\nprint(end_lineno)  # 输出结果为1\n```"
      ],
      "code_start_line": 122,
      "code_end_line": 140,
      "parent": "FileHandler",
      "params": [
        "self",
        "node"
      ],
      "have_return": true,
      "code_content": "    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "add_parent_references": {
      "type": "FunctionDef",
      "name": "add_parent_references",
      "md_content": [
        "**add_parent_references**: add_parent_references函数的功能是为AST中的每个节点添加一个父节点的引用。\n**参数**: 这个函数的参数。\n- node: AST中的当前节点。\n- parent: 父节点，默认为None。\n**代码描述**: 这个函数通过递归遍历AST的每个子节点，为每个子节点添加一个父节点的引用。\n- 首先，通过调用ast.iter_child_nodes(node)函数来获取当前节点的所有子节点。\n- 然后，通过将child.parent设置为当前节点，为每个子节点添加一个父节点的引用。\n- 最后，通过递归调用self.add_parent_references(child, node)函数，为每个子节点的子节点添加父节点的引用。\n**注意**: 使用这段代码时需要注意以下几点：\n- 这个函数是一个递归函数，会遍历AST的所有子节点，因此在处理大型AST时可能会导致递归深度过大的问题。\n- 在使用这个函数之前，需要确保AST已经被正确构建，否则可能会导致错误的结果。"
      ],
      "code_start_line": 142,
      "code_end_line": 154,
      "parent": "FileHandler",
      "params": [
        "self",
        "node",
        "parent"
      ],
      "have_return": false,
      "code_content": "    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "get_functions_and_classes": {
      "type": "FunctionDef",
      "name": "get_functions_and_classes",
      "md_content": [
        "**get_functions_and_classes**: get_functions_and_classes函数的功能是检索所有函数、类、它们的参数（如果有的话）以及它们的层级关系。\n**parameters**: get_functions_and_classes函数的参数如下：\n- code_content：要解析的整个文件的代码内容。\n**Code Description**: get_functions_and_classes函数的代码逻辑如下：\n1. 使用ast.parse函数解析code_content，生成抽象语法树（AST）。\n2. 调用add_parent_references函数为AST中的每个节点添加父节点的引用。\n3. 遍历AST中的每个节点，如果节点是FunctionDef、ClassDef或AsyncFunctionDef类型的实例，则进行以下操作：\n   - 获取节点的起始行号和结束行号。\n   - 如果节点的父节点有name属性，则获取父节点的名称，否则设置为None。\n   - 如果节点有args属性，则获取args中每个参数的名称，否则设置为空列表。\n   - 将节点的类型、名称、起始行号、结束行号、父节点名称和参数列表作为元组添加到functions_and_classes列表中。\n4. 返回functions_and_classes列表作为函数的输出结果。\n**Note**: 使用该函数时需要注意以下几点：\n- code_content参数应该是整个文件的代码内容。\n**Output Example**: 以下是函数返回值的示例：\n[('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]"
      ],
      "code_start_line": 156,
      "code_end_line": 181,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_content"
      ],
      "have_return": true,
      "code_content": "    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parent_name = node.parent.name if 'name' in dir(node.parent) else None\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                )\n        return functions_and_classes\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "generate_file_structure": {
      "type": "FunctionDef",
      "name": "generate_file_structure",
      "md_content": [
        "**generate_file_structure**: generate_file_structure函数的作用是生成给定文件路径的文件结构。\n**参数**: file_path (str): 文件的相对路径。\n**代码描述**: generate_file_structure函数首先通过打开文件并读取内容，获取文件中的函数和类的结构信息。然后，它遍历这些结构信息，并调用get_obj_code_info函数获取每个结构的代码信息。最后，将每个结构的代码信息存储在一个字典中，并返回该字典作为结果。\n**注意**: 该函数依赖于get_functions_and_classes和get_obj_code_info函数的实现。\n**输出示例**: \n{\n    \"function_name\": {\n        \"type\": \"function\",\n        \"start_line\": 10,\n        ··· ···\n        \"end_line\": 20,\n        \"parent\": \"class_name\"\n    },\n    \"class_name\": {\n        \"type\": \"class\",\n        \"start_line\": 5,\n        ··· ···\n        \"end_line\": 25,\n        \"parent\": None\n    }\n}"
      ],
      "code_start_line": 183,
      "code_end_line": 220,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/__init__"
      ],
      "reference_who": []
    },
    "generate_overall_structure": {
      "type": "FunctionDef",
      "name": "generate_overall_structure",
      "md_content": [
        "**generate_overall_structure**: generate_overall_structure函数的功能是生成整个代码库的结构。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先创建一个空的字典repo_structure，然后创建一个GitignoreChecker对象gitignore_checker，该对象用于检查.gitignore文件中指定的文件和文件夹。接下来，函数使用tqdm库创建一个进度条bar，用于显示检查进度。然后，函数通过遍历bar中的not_ignored_files来生成代码库的结构。在每次遍历中，函数调用generate_file_structure函数生成指定文件的结构，并将结果存储在repo_structure字典中。如果在生成文件结构的过程中出现异常，函数会打印错误信息并继续处理下一个文件。最后，函数返回repo_structure字典，表示整个代码库的结构。\n**注意**: 使用该函数前需要确保已经设置了repo_path属性，并且.gitignore文件存在于repo_path目录下。\n**输出示例**: \n{\n    \"file1.py\": {\n        \"function1\": {},\n        \"function2\": {},\n        ...\n    },\n    \"file2.py\": {\n        \"function3\": {},\n        \"function4\": {},\n        ...\n    },\n    ...\n}"
      ],
      "code_start_line": 223,
      "code_end_line": 241,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "convert_to_markdown_file": {
      "type": "FunctionDef",
      "name": "convert_to_markdown_file",
      "md_content": [
        "**convert_to_markdown_file**: convert_to_markdown_file函数的功能是将文件的内容转换为markdown格式。\n**参数**: 这个函数的参数。\n- file_path (str, 可选): 要转换的文件的相对路径。如果未提供，则使用默认文件路径None。\n\n**代码说明**: 这个函数首先使用utf-8编码打开self.project_hierarchy文件，并将其加载为json数据。然后，它检查file_path是否为None，如果是，则使用self.file_path作为文件路径。接下来，它在json_data中查找与file_path匹配的文件对象。如果找不到文件对象，则会引发ValueError异常。然后，它初始化一个空的markdown字符串和一个空的parent_dict字典。接下来，它对file_dict中的对象进行排序，并根据对象的code_start_line属性进行排序。然后，它遍历排序后的对象列表，并根据对象的层级和父对象的关系生成markdown字符串。最后，它返回markdown字符串。\n\n**注意**: 使用这段代码时需要注意以下几点：\n- 需要确保self.project_hierarchy文件存在且包含正确的json数据。\n- 需要确保file_path参数指定的文件在project_hierarchy.json中存在对应的文件对象。\n\n**输出示例**: 假设project_hierarchy.json中包含以下文件对象：\n{\n    \"file1.py\": {\n        \"obj1\": {\n            \"type\": \"FunctionDef\",\n            \"name\": \"function1\",\n            \"parent\": null,\n            \"code_start_line\": 1,\n            \"params\": [],\n            \"md_content\": [\"This is function1.\"]\n        },\n        \"obj2\": {\n            \"type\": \"ClassDef\",\n            \"name\": \"class1\",\n            \"parent\": null,\n            \"code_start_line\": 5,\n            \"params\": [],\n            \"md_content\": [\"This is class1.\"]\n        },\n        \"obj3\": {\n            \"type\": \"FunctionDef\",\n            \"name\": \"function2\",\n            \"parent\": \"class1\",\n            \"code_start_line\": 10,\n            \"params\": [],\n            \"md_content\": [\"This is function2.\"]\n        }\n    }\n}\n\n调用convert_to_markdown_file函数，假设file_path参数为\"file1.py\"，则返回的markdown字符串如下：\n\"# 1 FunctionDef function1:\\nThis is function1.\\n\\n***\\n# 1 ClassDef class1:\\nThis is class1.\\n\\n***\\n## 2 FunctionDef function2:\\nThis is function2.\\n\\n***\\n\""
      ],
      "code_start_line": 244,
      "code_end_line": 292,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef'] and obj['params']:\n                params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "repo_agent/config.py": {},
  "repo_agent/multi_task_dispatch.py": {
    "Task": {
      "type": "ClassDef",
      "name": "Task",
      "md_content": [
        "**Task**: Task类的功能是表示一个任务。\n\n**属性**: 该类具有以下属性：\n- task_id: 一个整数，表示任务的ID。\n- extra_info: 任意类型，表示额外的任务信息，默认为None。\n- dependencies: 一个Task对象列表，表示任务的依赖任务。\n- status: 一个整数，表示任务的状态，0表示未开始，1表示正在进行，2表示已经完成，3表示出错了。\n\n**代码描述**: Task类是一个表示任务的类，用于存储任务的相关信息。它具有以下方法：\n\n- \\_\\_init\\_\\_方法：初始化Task对象，设置初始值。\n\n**\\_\\_init\\_\\_方法**: 初始化Task对象，设置初始值。\n- 参数:\n  - task_id: 一个整数，表示任务的ID。\n  - dependencies: 一个Task对象列表，表示任务的依赖任务。\n  - extra_info: 任意类型，表示额外的任务信息，默认为None。\n- 返回值: 无\n\n**注意**: 在使用Task类时，需要先创建一个Task对象，并设置相应的属性值。\n\n**输出示例**: 以下是Task类的使用示例：\n\n```python\n# 创建Task对象\ntask = Task(task_id=1, dependencies=[], extra_info=\"example\")\n\n# 打印任务信息\nprint(f\"任务ID：{task.task_id}\")\nprint(f\"任务依赖：{task.dependencies}\")\nprint(f\"任务状态：{task.status}\")\n```\n\n输出结果：\n```\n任务ID：1\n任务依赖：[]\n任务状态：0\n```\n\n请根据代码生成的内容，为该对象生成详细的解释文档。"
      ],
      "code_start_line": 9,
      "code_end_line": 14,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class Task:\n    def __init__(self, task_id: int, dependencies: List[Task],extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0 #任务状态：0未开始，1正在进行，2已经完成，3出错了\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/multi_task_dispatch.py/TaskManager",
        "repo_agent/multi_task_dispatch.py/TaskManager/add_task",
        "repo_agent/multi_task_dispatch.py/worker",
        "repo_agent/multi_task_dispatch.py/some_function"
      ]
    },
    "TaskManager": {
      "type": "ClassDef",
      "name": "TaskManager",
      "md_content": [
        "**TaskManager**: TaskManager的功能是管理任务的类。\n\n**属性**：该类具有以下属性：\n- task_dict: 一个字典，用于存储任务的字典，键为任务ID，值为Task对象。\n- task_lock: 一个线程锁，用于保证任务的并发安全性。\n- now_id: 一个整数，表示当前任务的ID。\n- query_id: 一个整数，表示查询任务的ID。\n- sync_func: 一个函数，用于同步任务。\n\n**代码描述**：TaskManager类是一个任务管理器，用于管理任务的添加、获取、标记完成等操作。它具有以下方法：\n\n- \\_\\_init\\_\\_方法：初始化TaskManager对象，设置初始值。\n- all_success属性：判断是否所有任务都已完成。\n- add_task方法：添加任务到任务字典中，并返回任务ID。\n- get_next_task方法：获取下一个可执行的任务。\n- mark_completed方法：标记任务为已完成。\n\n**\\_\\_init\\_\\_方法**：初始化TaskManager对象，设置初始值。\n- 参数：无\n- 返回值：无\n\n**all_success属性**：判断是否所有任务都已完成。\n- 参数：无\n- 返回值：布尔值，表示是否所有任务都已完成。\n\n**add_task方法**：添加任务到任务字典中，并返回任务ID。\n- 参数：\n  - dependency_task_id: 一个整数列表，表示任务的依赖任务ID。\n  - extra: 任意类型，表示额外的任务信息，默认为None。\n- 返回值：整数，表示添加的任务ID。\n\n**get_next_task方法**：获取下一个可执行的任务。\n- 参数：\n  - process_id: 一个整数，表示进程ID。\n- 返回值：一个元组，包含两个值，第一个值为下一个可执行的任务(Task对象)，第二个值为任务ID。\n\n**mark_completed方法**：标记任务为已完成。\n- 参数：\n  - task_id: 一个整数，表示任务ID。\n- 返回值：无\n\n**注意**：在使用TaskManager类时，需要先创建一个TaskManager对象，并调用相应的方法来管理任务。\n\n**输出示例**：以下是TaskManager类的使用示例：\n\n```python\n# 创建TaskManager对象\ntask_manager = TaskManager()\n\n# 添加任务\ntask_id1 = task_manager.add_task([])\ntask_id2 = task_manager.add_task([task_id1])\ntask_id3 = task_manager.add_task([task_id1, task_id2])\n\n# 获取下一个任务\ntask, task_id = task_manager.get_next_task(1)\nprint(f\"获取到任务：{task}, 任务ID：{task_id}\")\n\n# 标记任务为已完成\ntask_manager.mark_completed(task_id)\n\n# 判断是否所有任务都已完成\nprint(f\"是否所有任务都已完成：{task_manager.all_success}\")\n```\n\n输出结果：\n```\n获取到任务：Task对象, 任务ID：1\n是否所有任务都已完成：False\n```"
      ],
      "code_start_line": 17,
      "code_end_line": 55,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class TaskManager:\n    def __init__(self):\n        self.task_dict: Dict[int, Task]  = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n\n    @property\n    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n\n    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        with self.task_lock:\n            denp_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(task_id=self.now_id, dependencies=denp_tasks, extra_info=extra)\n            self.now_id += 1\n            return self.now_id - 1\n\n    def get_next_task(self, process_id: int):\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (len(self.task_dict[task_id].dependencies) == 0) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    logger.info(f\"[{process_id}] get task_id {task_id}, remain task: {len(self.task_dict)}\")\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n        \n    def mark_completed(self, task_id: int):\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/multi_task_dispatch.py/Task"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化TaskManager对象。\n**参数**: 该函数没有参数。\n**代码描述**: 在这个函数中，我们对TaskManager对象进行了初始化。首先，我们创建了一个空的字典task_dict，用于存储任务的信息。字典的键是任务的ID，值是Task对象。接下来，我们创建了一个线程锁task_lock，用于在多线程环境下保护任务字典的操作。然后，我们初始化了now_id和query_id两个变量，分别用于生成任务的ID和查询任务的ID。最后，我们将sync_func变量设置为None，用于存储同步函数的引用。\n**注意**: 在使用TaskManager对象之前，需要先调用__init__函数进行初始化。"
      ],
      "code_start_line": 18,
      "code_end_line": 23,
      "parent": "TaskManager",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.task_dict: Dict[int, Task]  = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "all_success": {
      "type": "FunctionDef",
      "name": "all_success",
      "md_content": [
        "**all_success**: all_success函数的功能是检查任务字典是否为空。\n**参数**: 该函数没有参数。\n**代码描述**: 这个函数通过检查任务字典的长度是否为0来判断任务字典是否为空。如果任务字典为空，函数返回True；否则，返回False。\n**注意**: 使用这段代码时需要注意任务字典的数据类型，应该是一个字典对象。\n**输出示例**: 假设任务字典为空，函数将返回True。"
      ],
      "code_start_line": 26,
      "code_end_line": 27,
      "parent": "TaskManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "add_task": {
      "type": "FunctionDef",
      "name": "add_task",
      "md_content": [
        "**add_task**: add_task函数的功能是将任务添加到任务管理器中。\n**参数**: \n- dependency_task_id: 依赖任务的ID列表，类型为List[int]。\n- extra: 额外信息，类型为任意类型，默认值为None。\n**代码描述**: \nadd_task函数首先使用self.task_lock对任务管理器进行加锁，以确保在多线程环境下的安全操作。然后，根据传入的dependency_task_id列表，通过遍历获取对应的依赖任务对象，并将其存储在denp_tasks列表中。接下来，将新任务的信息以Task对象的形式存储在任务管理器的task_dict字典中，其中任务ID为self.now_id，依赖任务为denp_tasks，额外信息为extra。最后，将self.now_id的值加1，并返回self.now_id减1作为新任务的ID。\n**注意**: \n- 在调用add_task函数之前，需要确保任务管理器的实例已经创建。\n- 在多线程环境下使用add_task函数时，需要注意加锁操作以保证数据的一致性。\n**输出示例**: \n假设当前self.now_id的值为10，调用add_task([1, 2], extra=\"example\")的结果为9，表示成功将任务添加到任务管理器中，并返回新任务的ID为9。"
      ],
      "code_start_line": 29,
      "code_end_line": 34,
      "parent": "TaskManager",
      "params": [
        "self",
        "dependency_task_id",
        "extra"
      ],
      "have_return": true,
      "code_content": "    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        with self.task_lock:\n            denp_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(task_id=self.now_id, dependencies=denp_tasks, extra_info=extra)\n            self.now_id += 1\n            return self.now_id - 1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/multi_task_dispatch.py/Task"
      ],
      "reference_who": []
    },
    "get_next_task": {
      "type": "FunctionDef",
      "name": "get_next_task",
      "md_content": [
        "**get_next_task**: get_next_task函数的功能是获取下一个任务。\n**参数**: 这个函数的参数是process_id，表示进程的ID。\n**代码描述**: 这个函数首先使用self.task_lock进行线程同步，然后递增self.query_id的值。接着，它遍历self.task_dict中的所有任务ID。对于每个任务ID，它判断任务的依赖数量是否为0，并且任务的状态是否为0。如果满足这两个条件，说明任务已经准备好可以执行，它将任务的状态设置为1，并且使用logger记录日志信息。如果self.query_id能被10整除，它会调用self.sync_func()函数。最后，它返回找到的任务以及任务的ID。如果没有找到满足条件的任务，它返回None和-1。\n**注意**: 在使用这段代码时需要注意以下几点：\n- 这段代码使用了self.task_lock进行线程同步，确保多个线程之间的安全访问。\n- 这段代码依赖self.task_dict字典来存储任务信息，确保在获取下一个任务时能够正确地遍历和更新任务状态。\n**输出示例**: 这里给出一个可能的返回值的示例：\n(task, task_id)"
      ],
      "code_start_line": 36,
      "code_end_line": 47,
      "parent": "TaskManager",
      "params": [
        "self",
        "process_id"
      ],
      "have_return": true,
      "code_content": "    def get_next_task(self, process_id: int):\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (len(self.task_dict[task_id].dependencies) == 0) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    logger.info(f\"[{process_id}] get task_id {task_id}, remain task: {len(self.task_dict)}\")\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "mark_completed": {
      "type": "FunctionDef",
      "name": "mark_completed",
      "md_content": [
        "**mark_completed**: mark_completed函数的功能是将指定的任务标记为已完成。\n**parameters**: mark_completed函数的参数有一个task_id，表示要标记为已完成的任务的ID。\n**Code Description**: mark_completed函数首先使用self.task_lock对任务字典进行加锁，以确保在修改任务字典时不会发生竞争条件。然后，它根据给定的task_id从任务字典中获取目标任务。接下来，它遍历任务字典中的每个任务，并检查目标任务是否是当前任务的依赖之一。如果是，则将目标任务从当前任务的依赖列表中移除。最后，它使用task_id从任务字典中删除目标任务。\n**Note**: 在使用mark_completed函数之前，需要确保已经获取了TaskManager对象的实例，并且任务字典已经被正确初始化。此外，由于mark_completed函数涉及到对任务字典的修改，建议在调用该函数时使用适当的同步机制，以避免多线程环境下的竞争条件。"
      ],
      "code_start_line": 49,
      "code_end_line": 55,
      "parent": "TaskManager",
      "params": [
        "self",
        "task_id"
      ],
      "have_return": false,
      "code_content": "    def mark_completed(self, task_id: int):\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "worker": {
      "type": "FunctionDef",
      "name": "worker",
      "md_content": [
        "**worker**: worker函数的作用是处理任务的执行和完成标记。\n**参数**: \n- task_manager: 任务管理器对象，用于获取下一个任务和标记任务完成。\n- process_id: 进程ID，用于获取特定进程的任务。\n- handler: 任务处理函数，用于执行任务的具体操作。\n\n**代码描述**: \nworker函数是一个无限循环的函数，它会不断地从任务管理器中获取下一个任务，并执行任务的处理函数。如果任务管理器中的所有任务都已经完成，函数会立即返回。函数的主要逻辑如下：\n1. 判断任务管理器的all_success属性是否为True，如果是则直接返回。\n2. 调用任务管理器的get_next_task方法，传入进程ID，获取下一个任务和任务ID。\n3. 如果获取到的任务为None，说明当前没有可执行的任务，函数会休眠0.5秒后继续下一次循环。\n4. 调用任务处理函数handler，传入任务的额外信息extra_info。\n5. 调用任务管理器的mark_completed方法，标记任务为已完成。\n\n**注意**: \n- worker函数是一个无限循环的函数，只有当任务管理器的all_success属性为True时才会退出循环。\n- 任务处理函数handler需要根据实际需求自行实现，它负责执行任务的具体操作。\n\n**输出示例**: \n以下是worker函数的一个可能的返回值示例：\n```\nNone\n```"
      ],
      "code_start_line": 59,
      "code_end_line": 69,
      "parent": null,
      "params": [
        "task_manager",
        "process_id",
        "handler"
      ],
      "have_return": true,
      "code_content": "def worker(task_manager, process_id: int, handler: Callable):\n    while True:\n        if task_manager.all_success:\n            return\n        task, task_id = task_manager.get_next_task(process_id)\n        if task is None: \n            time.sleep(0.5)\n            continue\n        # print(f\"will perform task: {task_id}\")\n        handler(task.extra_info)\n        task_manager.mark_completed(task.task_id)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/multi_task_dispatch.py/Task"
      ],
      "reference_who": []
    },
    "some_function": {
      "type": "FunctionDef",
      "name": "some_function",
      "md_content": [
        "**some_function**: some_function函数的功能是随机睡眠一段时间。\n\n**参数**: 该函数没有任何参数。\n\n**代码描述**: 这个函数使用了time模块的sleep方法来实现睡眠功能。sleep方法接受一个参数，表示要睡眠的秒数。在这个函数中，使用了random模块的random方法生成一个0到1之间的随机数，并将其乘以3作为睡眠的秒数。这样就实现了随机睡眠一段时间的功能。\n\n**注意**: 这个函数没有任何参数，所以在调用时不需要传入任何参数。调用这个函数后，程序会暂停执行一段随机的时间。"
      ],
      "code_start_line": 76,
      "code_end_line": 77,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "    def some_function(): #随机睡一会\n        time.sleep(random.random()*3)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/multi_task_dispatch.py/Task"
      ],
      "reference_who": []
    }
  },
  "repo_agent/doc_meta_info.py": {
    "EdgeType": {
      "type": "ClassDef",
      "name": "EdgeType",
      "md_content": [
        "**EdgeType**: EdgeType的功能是定义了边的类型。\n\n**attributes**: 这个类没有属性。\n\n**Code Description**: EdgeType是一个枚举类，它定义了三种不同的边类型：reference_edge、subfile_edge和file_item_edge。每种边类型都有一个自动生成的唯一值。\n\n- reference_edge：表示一个对象引用另一个对象。\n- subfile_edge：表示一个文件或文件夹属于另一个文件夹。\n- file_item_edge：表示一个对象属于一个文件。\n\n**Note**: \n- EdgeType是一个枚举类，它提供了一种方便的方式来定义和使用不同类型的边。\n- 每个边类型都有一个自动生成的唯一值，可以用于标识和区分不同的边。\n- 开发者可以使用EdgeType来指定边的类型，以便在程序中进行相应的处理和判断。"
      ],
      "code_start_line": 21,
      "code_end_line": 24,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class EdgeType(Enum):\n    reference_edge = auto() #一个obj引用另一个obj\n    subfile_edge = auto() # 一个 文件/文件夹 属于一个文件夹\n    file_item_edge = auto() #一个 obj 属于一个文件\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemType/to_str",
        "repo_agent/doc_meta_info.py/DocItemType/print_self",
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/has_ans_relation",
        "repo_agent/doc_meta_info.py/DocItem/get_travel_list",
        "repo_agent/doc_meta_info.py/DocItem/check_depth",
        "repo_agent/doc_meta_info.py/DocItem/find_min_ances",
        "repo_agent/doc_meta_info.py/DocItem/parse_tree_path",
        "repo_agent/doc_meta_info.py/DocItem/get_file_name",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/DocItem/find",
        "repo_agent/doc_meta_info.py/DocItem/print_recursive",
        "repo_agent/doc_meta_info.py/DocItem/print_recursive/print_indent",
        "repo_agent/doc_meta_info.py/find_all_referencer",
        "repo_agent/doc_meta_info.py/MetaInfo",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files/walk_tree",
        "repo_agent/doc_meta_info.py/MetaInfo/find_obj_with_lineno",
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager",
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/find_item",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel2",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json",
        "repo_agent/doc_meta_info.py/walk_file",
        "repo_agent/doc_meta_info.py/in_white_list",
        "repo_agent/doc_meta_info.py/parse_one_item"
      ]
    },
    "DocItemType": {
      "type": "ClassDef",
      "name": "DocItemType",
      "md_content": [
        "**DocItemType**: DocItemType的功能是XXX\n**属性**: 这个类的属性。\n**代码描述**: 这个类的描述。\nDocItemType是一个枚举类，用于表示文档中的不同类型的项。它包含了根节点、目录、文件、类、类函数、函数、子函数和全局变量等不同的类型。每个类型都有一个对应的字符串表示，可以通过调用to_str()方法来获取。此外，还有一个print_self()方法用于打印类型的名称，并根据类型的不同设置不同的颜色。\n\n**to_str()方法**:\n这个方法用于将DocItemType转换为字符串表示。如果类型是_class、_function、_class_function或_sub_function，它们都会被转换为\"FunctionDef\"。否则，直接返回类型的名称。\n\n**print_self()方法**:\n这个方法用于打印类型的名称，并根据类型的不同设置不同的颜色。如果类型是_dir，颜色为绿色；如果类型是_file，颜色为黄色；如果类型是_class，颜色为蓝色；如果类型是_function，颜色为红色。最后返回带有颜色的类型名称。\n\n**get_edge_type()方法**:\n这个方法是一个静态方法，用于获取从一个DocItemType到另一个DocItemType的边的类型。它接受两个参数from_item_type和to_item_type，分别表示起始类型和目标类型。具体的实现需要根据具体的业务逻辑来完成。\n\n**注意**:\n- DocItemType是一个枚举类，用于表示文档中的不同类型的项。\n- to_str()方法用于将DocItemType转换为字符串表示。\n- print_self()方法用于打印类型的名称，并根据类型的不同设置不同的颜色。\n- get_edge_type()方法是一个静态方法，用于获取从一个DocItemType到另一个DocItemType的边的类型。\n\n**输出示例**:\n```\nClassDef\n```"
      ],
      "code_start_line": 28,
      "code_end_line": 63,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItemType(Enum):\n    _repo = auto() #根节点，需要生成readme\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_function = auto()\n    _function = auto() #文件内的常规function\n    _sub_function = auto() #function内的定义的subfunction\n    _global_var = auto()\n\n    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n\n    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n\n    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "to_str": {
      "type": "FunctionDef",
      "name": "to_str",
      "md_content": [
        "**to_str**: to_str函数的功能是将DocItemType枚举类型的值转换为字符串表示。\n**参数**: 该函数没有参数。\n**代码说明**: to_str函数根据不同的DocItemType枚举值返回相应的字符串表示。如果self等于DocItemType._class，则返回\"ClassDef\"；如果self等于DocItemType._function、DocItemType._class_function或DocItemType._sub_function，则返回\"FunctionDef\"；否则返回self.name。\n**注意**: 使用该函数时需要确保self是DocItemType枚举类型的值。\n**输出示例**: 假设self等于DocItemType._class，则返回\"ClassDef\"。"
      ],
      "code_start_line": 38,
      "code_end_line": 48,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "print_self": {
      "type": "FunctionDef",
      "name": "print_self",
      "md_content": [
        "**print_self**: print_self函数的功能是根据DocItemType的类型返回相应的颜色和名称。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先定义了一个变量color，并将其初始化为Fore.WHITE。然后通过判断self的值来确定color的值。如果self等于DocItemType._dir，则将color赋值为Fore.GREEN；如果self等于DocItemType._file，则将color赋值为Fore.YELLOW；如果self等于DocItemType._class，则将color赋值为Fore.BLUE；如果self等于DocItemType._function，则将color赋值为Fore.RED。最后，函数返回color加上self.name和Style.RESET_ALL。\n**注意**: 使用该代码时需要注意self的值必须是DocItemType的类型之一。\n**输出示例**: 假设self等于DocItemType._dir，那么函数的返回值将是Fore.GREEN + self.name + Style.RESET_ALL。"
      ],
      "code_start_line": 50,
      "code_end_line": 60,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_edge_type": {
      "type": "FunctionDef",
      "name": "get_edge_type",
      "md_content": [
        "**get_edge_type**: get_edge_type函数的作用是获取边的类型。\n**parameters**: 这个函数的参数有两个，分别是from_item_type和to_item_type，它们的类型都是DocItemType。\n**Code Description**: 这个函数没有具体的代码实现，只有一个pass语句，表示函数体为空。\n**Note**: 这个函数的作用是获取边的类型，但是具体的实现逻辑需要在pass语句后面进行补充。"
      ],
      "code_start_line": 62,
      "code_end_line": 63,
      "parent": "DocItemType",
      "params": [
        "from_item_type",
        "to_item_type"
      ],
      "have_return": false,
      "code_content": "    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "DocItemStatus": {
      "type": "ClassDef",
      "name": "DocItemStatus",
      "md_content": [
        "**DocItemStatus**: DocItemStatus的功能是表示文档项的状态。\n\n**属性**: 这个类的属性有：\n- doc_up_to_date: 表示文档是最新的，无需生成新的文档。\n- doc_has_not_been_generated: 表示文档还未生成，需要生成新的文档。\n- code_changed: 表示源码被修改了，需要改变文档。\n- add_new_referencer: 表示添加了新的引用者。\n- referencer_not_exist: 表示曾经引用该对象的对象被删除了，或者不再引用该对象了。\n\n**代码描述**: 这个类定义了一个枚举类型，用于表示文档项的不同状态。每个状态都有一个对应的属性，用于表示该状态的含义。\n\n**注意**: 在使用这个类时需要注意以下几点：\n- 可以通过调用`DocItemStatus.doc_up_to_date`、`DocItemStatus.doc_has_not_been_generated`等属性来获取对应的状态。\n- 这些状态可以用于判断文档项的状态，根据不同的状态来执行相应的操作。"
      ],
      "code_start_line": 66,
      "code_end_line": 71,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class DocItemStatus(Enum):\n    doc_up_to_date = auto() #无需生成文档\n    doc_has_not_been_generated = auto() #文档还未生成，需要生成\n    code_changed = auto() #源码被修改了，需要改文档\n    add_new_referencer = auto() #添加了新的引用者\n    referencer_not_exist = auto() #曾经引用他的obj被删除了，或者不再引用他了\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "DocItem": {
      "type": "ClassDef",
      "name": "DocItem",
      "md_content": [
        "**DocItem**: DocItem的功能是XXX\n**属性**: 这个类的属性。\n**代码描述**: 这个类的描述。\n(Detailed code analysis and description...)\n**注意**: 使用该代码时需要注意的事项。\n**输出示例**: 模拟代码返回值的可能外观。\n\n**DocItem**: DocItem的功能是存储文档信息的类。\n\n**属性**:\n- item_type: DocItemType = DocItemType._class_function，表示文档项的类型。\n- item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated，表示文档的状态。\n- obj_name: str = \"\"，表示对象的名称。\n- md_content: List[str] = field(default_factory=list)，存储不同版本的文档内容。\n- content: Dict[Any,Any] = field(default_factory=dict)，存储原始信息。\n- children: Dict[str, DocItem] = field(default_factory=dict)，存储子对象。\n- father: Any[DocItem] = None，表示父对象。\n- depth: int = 0，表示对象在树中的深度。\n- tree_path: List[DocItem] = field(default_factory=list)，表示对象在树中的路径。\n- max_reference_ansce: Any[DocItem] = None，表示最早的引用对象。\n- reference_who: List[DocItem] = field(default_factory=list)，表示引用了该对象的对象列表。\n- who_reference_me: List[DocItem] = field(default_factory=list)，表示被该对象引用的对象列表。\n- reference_who_name_list: List[str] = field(default_factory=list)，表示引用了该对象的对象名称列表。\n- who_reference_me_name_list: List[str] = field(default_factory=list)，表示被该对象引用的对象名称列表。\n- multithread_task_id: int = -1，表示在多线程中的任务ID。\n\n**代码描述**:\n- `__eq__(self, other) -> bool`：重载了等于运算符，判断两个DocItem对象是否相等。\n- `has_ans_relation(now_a: DocItem, now_b: DocItem)`：判断两个节点之间是否存在祖先关系。\n- `get_travel_list(self)`：获取以当前节点为根的树的遍历列表。\n- `check_depth(self)`：计算当前节点在树中的深度。\n- `find_min_ances(node_a: DocItem, node_b: DocItem)`：找到两个节点的最小公共祖先。\n- `parse_tree_path(self, now_path)`：解析树中节点的路径。\n- `get_file_name(self)`：获取当前节点所在文件的文件名。\n- `get_full_name(self)`：获取从下到上所有的对象名称。\n- `find(self, recursive_file_path: list) -> Optional[DocItem]`：根据路径列表从根节点开始查找对应的文件。\n- `print_recursive(self, indent=0, print_content = False)`：递归打印树中的节点。\n\n**注意**:\n- 在使用`__eq__`方法进行对象比较时，需要确保比较的对象是`DocItem`的实例。\n- 在使用`has_ans_relation`方法判断两个节点之间是否存在祖先关系时，如果存在祖先关系，会返回更早的节点。\n- `get_travel_list`方法返回以当前节点为根的树的遍历列表。\n- `check_depth`方法计算当前节点在树中的深度。\n- `find_min_ances`方法找到两个节点的最小公共祖先。\n- `parse_tree_path`方法解析树中节点的路径。\n- `get_file_name`方法获取当前节点所在文件的文件名。\n- `get_full_name`方法获取从下到上所有的对象名称。\n- `find`方法根据路径列表从根节点开始查找对应的文件。\n- `print_recursive`方法递归打印树中的节点。\n\n**输出示例**:\n```\nDocItem: 存储文档信息的类\n属性: \n- item_type: DocItemType = DocItemType._class_function\n- item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n- obj_name: str = \"\"\n- md_content: List[str] = []\n- content: Dict[Any,Any] = {}\n- children: Dict[str, DocItem]"
      ],
      "code_start_line": 75,
      "code_end_line": 194,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItem():\n    item_type: DocItemType = DocItemType._class_function\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n\n    obj_name: str = \"\" #对象的名字\n    md_content: List[str] = field(default_factory=list) #存储不同版本的doc\n    content: Dict[Any,Any] = field(default_factory=dict) #原本存储的信息\n\n    children: Dict[str, DocItem] = field(default_factory=dict) #子对象\n    father: Any[DocItem] = None\n\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list) #一整条链路，从root开始\n    max_reference_ansce: Any[DocItem] = None\n\n    reference_who: List[DocItem] = field(default_factory=list) #他引用了谁\n    who_reference_me: List[DocItem] = field(default_factory=list) #谁引用了他\n\n    reference_who_name_list: List[str] = field(default_factory=list) #他引用了谁，这个可能是老版本的\n    who_reference_me_name_list: List[str] = field(default_factory=list) #谁引用了他，这个可能是老版本的\n\n    multithread_task_id: int = -1 #在多线程中的task_id\n\n    def __eq__(self, other) -> bool:\n        # 检查other是否是MyCustomClass的实例\n        if not isinstance(other, DocItem):\n            return False\n        if self.item_type != other.item_type:\n            return False\n        if self.obj_name != other.obj_name:\n            return False\n        return self.get_full_name() == other.get_full_name()\n\n\n    @staticmethod\n    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n    \n    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n    \n    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n\n\n    \n    @staticmethod\n    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n\n    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n\n    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n    \n    \n    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "__eq__": {
      "type": "FunctionDef",
      "name": "__eq__",
      "md_content": [
        "**__eq__**: __eq__函数的功能是比较两个对象是否相等。\n**参数**: 这个函数的参数。\n**代码描述**: 这个函数的描述。\n__eq__函数用于比较两个对象是否相等。首先，它会检查other是否是DocItem类的实例，如果不是，则返回False。然后，它会逐个比较self和other的属性值，如果item_type属性值不相等，则返回False；如果obj_name属性值不相等，则返回False。最后，它会调用get_full_name()方法来比较两个对象的完整名称是否相等。如果完整名称相等，则返回True，否则返回False。\n\n**注意**: 使用这段代码时需要注意的事项。\n这个函数只能用于比较两个DocItem类的实例是否相等，如果用于其他类的实例比较可能会得到错误的结果。\n\n**输出示例**: 模拟代码返回值的可能外观。\nTrue"
      ],
      "code_start_line": 98,
      "code_end_line": 106,
      "parent": "DocItem",
      "params": [
        "self",
        "other"
      ],
      "have_return": true,
      "code_content": "    def __eq__(self, other) -> bool:\n        # 检查other是否是MyCustomClass的实例\n        if not isinstance(other, DocItem):\n            return False\n        if self.item_type != other.item_type:\n            return False\n        if self.obj_name != other.obj_name:\n            return False\n        return self.get_full_name() == other.get_full_name()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "has_ans_relation": {
      "type": "FunctionDef",
      "name": "has_ans_relation",
      "md_content": [
        "**has_ans_relation**: has_ans_relation函数的功能是判断两个节点是否存在祖先关系，并返回更早的节点。\n**参数**: 这个函数的参数有两个，分别是now_a和now_b，它们都是DocItem类型的对象。\n**代码描述**: 这个函数首先判断now_b是否在now_a的树路径(tree_path)中，如果是，则返回now_b。接着判断now_a是否在now_b的树路径(tree_path)中，如果是，则返回now_a。如果以上两个条件都不满足，则返回None。\n**注意**: 使用这段代码时需要注意以下几点：\n- 参数now_a和now_b必须是DocItem类型的对象。\n- 函数返回的结果可能是一个DocItem对象或者None。\n**输出示例**: 假设now_a和now_b都是DocItem对象，并且now_b在now_a的树路径中，则函数的返回值为now_b。"
      ],
      "code_start_line": 110,
      "code_end_line": 116,
      "parent": "DocItem",
      "params": [
        "now_a",
        "now_b"
      ],
      "have_return": true,
      "code_content": "    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_travel_list": {
      "type": "FunctionDef",
      "name": "get_travel_list",
      "md_content": [
        "**get_travel_list**: get_travel_list函数的功能是获取当前对象及其所有子对象的列表。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先创建一个包含当前对象的列表now_list。然后遍历当前对象的所有子对象，将子对象的get_travel_list函数返回的列表与now_list相加，得到更新后的now_list。最后返回now_list作为函数的返回值。\n**注意**: 该函数是一个递归函数，会不断调用子对象的get_travel_list函数，直到所有子对象都被遍历完毕。\n**输出示例**: 假设当前对象有两个子对象child1和child2，它们分别有子对象child1_1和child2_1，那么函数的返回值可能是[now_list, child1, child1_1, child2, child2_1]。"
      ],
      "code_start_line": 118,
      "code_end_line": 122,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "check_depth": {
      "type": "FunctionDef",
      "name": "check_depth",
      "md_content": [
        "**check_depth**: check_depth函数的作用是计算当前节点的深度。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先判断当前节点是否有子节点，如果没有子节点，则将当前节点的深度设置为0，并返回深度值。如果有子节点，则遍历所有子节点，并递归调用子节点的check_depth函数，获取子节点的深度值。然后将子节点的最大深度值加1，作为当前节点的深度值，并返回深度值。\n**注意**: 使用该代码时需要注意以下几点：\n- 该函数是一个递归函数，会不断调用子节点的check_depth函数，直到遍历到叶子节点。\n- 该函数依赖于节点的children属性，需要保证children属性正确设置。\n**输出示例**: 假设当前节点有两个子节点，子节点的深度分别为2和3，则该函数的返回值为4。"
      ],
      "code_start_line": 124,
      "code_end_line": 133,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_min_ances": {
      "type": "FunctionDef",
      "name": "find_min_ances",
      "md_content": [
        "**find_min_ances**: find_min_ances函数的功能是找到两个节点的最小公共祖先。\n**参数**: 这个函数的参数是node_a和node_b，它们都是DocItem类型的对象。\n**代码描述**: 这个函数首先初始化一个变量pos为0，然后使用断言来判断node_a和node_b的tree_path的第一个元素是否相等。接下来进入一个无限循环，每次循环pos加1。在循环中，如果node_a和node_b的tree_path在pos位置的元素不相等，就返回node_a的tree_path在pos-1位置的元素作为最小公共祖先。\n**注意**: 使用这段代码时需要注意以下几点：\n- 参数node_a和node_b必须是DocItem类型的对象。\n- node_a和node_b的tree_path必须有相同的长度。\n**输出示例**: 假设node_a的tree_path为[1, 2, 3, 4]，node_b的tree_path为[1, 2, 5, 6]，那么函数的返回值将是2，即最小公共祖先是2。"
      ],
      "code_start_line": 138,
      "code_end_line": 144,
      "parent": "DocItem",
      "params": [
        "node_a",
        "node_b"
      ],
      "have_return": true,
      "code_content": "    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "parse_tree_path": {
      "type": "FunctionDef",
      "name": "parse_tree_path",
      "md_content": [
        "**parse_tree_path**: parse_tree_path函数的作用是将当前路径添加到now_path中，并遍历子节点，递归调用parse_tree_path函数。\n\n**参数**: \n- now_path: 当前路径，是一个列表。\n\n**代码描述**: \nparse_tree_path函数接受一个参数now_path，将当前节点self添加到now_path列表中，形成新的路径self.tree_path。然后遍历子节点，对每个子节点递归调用parse_tree_path函数，传入新的路径self.tree_path。\n\n**代码分析与描述**:\nparse_tree_path函数的目的是为了构建树的路径。首先，将当前节点self添加到now_path列表中，形成新的路径self.tree_path。然后，使用for循环遍历子节点，对每个子节点进行递归调用parse_tree_path函数，传入新的路径self.tree_path。这样就可以逐层构建树的路径。\n\n**注意**: \n- parse_tree_path函数需要传入一个列表作为参数now_path。\n- parse_tree_path函数会修改对象的属性tree_path。"
      ],
      "code_start_line": 146,
      "code_end_line": 149,
      "parent": "DocItem",
      "params": [
        "self",
        "now_path"
      ],
      "have_return": false,
      "code_content": "    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_file_name": {
      "type": "FunctionDef",
      "name": "get_file_name",
      "md_content": [
        "**get_file_name**: get_file_name函数的功能是获取文件名。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先调用了get_full_name函数获取完整文件名，然后通过split函数将文件名以\".py\"为分隔符进行拆分，取拆分后的第一个元素，再加上\".py\"后缀，最后返回拼接后的文件名。\n**注意**: 使用该代码时需要注意文件名的格式，确保文件名以\".py\"结尾。\n**输出示例**: 假设完整文件名为\"example.py\"，则该函数返回\"example.py\"。"
      ],
      "code_start_line": 151,
      "code_end_line": 153,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_full_name": {
      "type": "FunctionDef",
      "name": "get_full_name",
      "md_content": [
        "**get_full_name**: get_full_name函数的功能是获取从下到上所有的obj名字\n**参数**: 无参数\n**代码描述**: 这个函数通过遍历对象的父对象链，获取从下到上所有的obj名字，并将它们以斜杠分隔的形式返回。\n**代码分析**: \n- 首先，函数判断当前对象的父对象是否为空，如果为空，则直接返回当前对象的名字。\n- 然后，定义一个空的列表name_list用于存储所有的obj名字。\n- 接着，定义一个变量now，初始值为当前对象。\n- 使用while循环，当now不为空时，将当前对象的名字添加到name_list列表的开头，并将当前对象的父对象赋值给now。\n- 最后，将name_list列表的第一个元素（即当前对象的名字）删除，并使用斜杠将name_list列表中的所有元素连接起来，返回结果。\n**注意**: \n- 这个函数只能在DocItem对象中使用，不能在其他对象中使用。\n- 当前对象的父对象链中的每个对象都必须有一个名字。\n**输出示例**: \n假设当前对象的名字为\"obj3\"，父对象的名字分别为\"obj2\"和\"obj1\"，则函数的返回值为\"obj1/obj2/obj3\"。"
      ],
      "code_start_line": 154,
      "code_end_line": 165,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find": {
      "type": "FunctionDef",
      "name": "find",
      "md_content": [
        "**find**: find函数的功能是根据给定的路径列表从repo根节点找到对应的文件，如果找不到则返回None。\n**参数**: 这个函数的参数是recursive_file_path，它是一个路径列表，用于指定要查找的文件的路径。\n**代码描述**: 这个函数首先会检查当前对象的item_type属性是否为DocItemType._repo，如果不是，则会抛出一个断言错误。然后，函数会使用一个while循环来遍历路径列表。在每次循环中，函数会检查当前路径是否存在于当前对象的子节点中。如果不存在，则直接返回None。如果存在，则将当前对象更新为对应的子节点，并继续遍历下一个路径。当遍历完整个路径列表后，函数会返回最后一个节点对象。\n**注意**: 这个函数只能在DocItem对象中调用，且只能用于查找文件对象。\n**输出示例**: 假设给定的路径列表为['folder1', 'folder2', 'file1']，并且在repo根节点下存在一个名为'folder1'的子节点，它的子节点中存在一个名为'folder2'的子节点，最后一个节点是一个名为'file1'的文件对象。那么函数的返回值将是'file1'文件对象。"
      ],
      "code_start_line": 168,
      "code_end_line": 179,
      "parent": "DocItem",
      "params": [
        "self",
        "recursive_file_path"
      ],
      "have_return": true,
      "code_content": "    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "print_recursive": {
      "type": "FunctionDef",
      "name": "print_recursive",
      "md_content": [
        "**print_recursive**: print_recursive函数的功能是递归打印repo对象。\n**参数**: 这个函数的参数有indent和print_content。\n**代码描述**: 这个函数首先定义了一个内部函数print_indent，用于打印缩进。然后，它打印出当前对象的类型和名称，并根据子对象的数量打印出相应的信息。接下来，它遍历子对象，并对每个子对象调用print_recursive函数，实现递归打印。\n**注意**: 使用这段代码时需要注意以下几点：\n- indent参数用于控制打印时的缩进层级。\n- print_content参数用于控制是否打印对象的内容。\n**输出示例**: 假设有一个repo对象，它包含一个文件夹和两个文件，其中一个文件夹下还有一个文件。调用print_recursive函数后，输出可能如下所示：\n```\n|-文件夹: folder, 2 children\n  |-文件: file1\n  |-文件: file2\n    |-文件: file3\n```"
      ],
      "code_start_line": 181,
      "code_end_line": 194,
      "parent": "DocItem",
      "params": [
        "self",
        "indent",
        "print_content"
      ],
      "have_return": true,
      "code_content": "    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "print_indent": {
      "type": "FunctionDef",
      "name": "print_indent",
      "md_content": [
        "**print_indent**: print_indent函数的功能是根据给定的缩进级别打印相应的缩进字符串。\n**参数**: 这个函数的参数是indent，表示缩进级别，默认值为0。\n**代码描述**: 这个函数首先判断缩进级别是否为0，如果是0则返回空字符串。否则，根据缩进级别生成相应的缩进字符串，然后在字符串末尾添加一个\"|- \"的标记，并返回该字符串。\n**注意**: 使用该代码时需要注意缩进级别的取值范围，以及返回值的格式。\n**输出示例**: 假设缩进级别为2，则返回的字符串为\"    |- \"。"
      ],
      "code_start_line": 184,
      "code_end_line": 187,
      "parent": "print_recursive",
      "params": [
        "indent"
      ],
      "have_return": true,
      "code_content": "        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_all_referencer": {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [
        "**find_all_referencer**: find_all_referencer函数的功能是在给定的代码路径中查找所有引用了指定变量的位置。\n\n**parameters**: \n- repo_path: 代码仓库的路径\n- variable_name: 要查找引用的变量名\n- file_path: 文件路径\n- line_number: 变量所在行号\n- column_number: 变量所在列号\n- in_file_only: 是否只在当前文件中查找引用，默认为False\n\n**Code Description**: \n该函数通过使用jedi库来解析代码，并查找引用了指定变量的位置。首先，它将代码路径和文件路径合并为完整的路径，并创建一个jedi.Script对象。然后，它使用get_references方法获取所有引用了指定变量的位置。如果in_file_only参数为True，则只在当前文件中查找引用，否则在整个代码路径中查找引用。接下来，它通过过滤出变量名为variable_name的引用，并返回它们的位置。最后，它将引用的位置转换为相对于代码仓库路径的相对路径，并返回结果。\n\n**Note**: \n- 该函数依赖于jedi库来解析代码和查找引用位置，因此需要确保已安装该库。\n- 如果发生任何异常，函数将打印错误信息和相关参数，并返回一个空列表。\n\n**Output Example**: \n假设代码仓库路径为\"/path/to/repo\"，变量名为\"my_variable\"，文件路径为\"src/main.py\"，变量所在行号为10，列号为5。如果在代码中存在引用了\"my_variable\"的位置，函数可能返回类似以下的结果：\n[(\"src/main.py\", 15, 8), (\"src/utils.py\", 20, 12)]"
      ],
      "code_start_line": 198,
      "code_end_line": 214,
      "parent": null,
      "params": [
        "repo_path",
        "variable_name",
        "file_path",
        "line_number",
        "column_number",
        "in_file_only"
      ],
      "have_return": true,
      "code_content": "def find_all_referencer(repo_path, variable_name, file_path, line_number, column_number, in_file_only=False):\n    \"\"\"复制过来的之前的实现\"\"\"\n    script = jedi.Script(path=os.path.join(repo_path, file_path))\n\n    try:\n        if in_file_only:\n            references = script.get_references(line=line_number, column=column_number, scope=\"file\")\n        else:\n            references = script.get_references(line=line_number, column=column_number)\n        # 过滤出变量名为 variable_name 的引用，并返回它们的位置\n        variable_references = [ref for ref in references if ref.name == variable_name]\n        return [(os.path.relpath(ref.module_path, repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n    except Exception as e:\n        # 打印错误信息和相关参数\n        print(f\"Error occurred: {e}\")\n        print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n        return []\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "MetaInfo": {
      "type": "ClassDef",
      "name": "MetaInfo",
      "md_content": [
        "**MetaInfo**: MetaInfo的功能是管理项目的元信息，包括仓库路径、文档版本、目标仓库的文件结构、白名单等。\n\n**属性**：\n- repo_path: str类型，表示仓库路径。\n- document_version: str类型，表示文档版本，随时间变化。如果为空字符串，则表示文档尚未完成；否则，对应一个目标仓库的commit hash。\n- target_repo_hierarchical_tree: DocItem类型，表示整个仓库的文件结构。\n- white_list: List类型，表示白名单，存储一些特定的对象。\n- in_generation_process: bool类型，表示是否正在生成文档。\n- checkpoint_lock: threading.Lock类型，用于多线程操作时的锁。\n\n**代码描述**：\n- init_from_project_path(project_abs_path: str) -> MetaInfo: 从一个仓库路径中初始化MetaInfo对象。该方法会根据仓库路径生成整个仓库的文件结构，并返回一个新的MetaInfo对象。\n- from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo: 从已有的metainfo目录中读取metainfo。该方法会读取.meta-info.json文件和.project_hierarchy.json文件，并返回一个新的MetaInfo对象。\n- checkpoint(self, target_dir_path: str, flash_reference_relation=False): 将MetaInfo对象保存到指定目录。该方法会将MetaInfo对象的信息保存到.project_hierarchy.json和meta-info.json文件中。\n- print_task_list(self, item_list): 打印剩余待完成的任务列表。该方法会打印出剩余待完成任务的task_id、文档生成原因和路径。\n- get_all_files(self) -> List[DocItem]: 获取所有的文件节点。该方法会返回一个列表，包含所有的文件节点。\n- find_obj_with_lineno(self, file_node, start_line_num) -> DocItem: 根据文件节点和起始行号查找对应的对象。该方法会遍历文件节点及其子节点，找到对应的对象并返回。\n- parse_reference(self): 双向提取所有引用关系。该方法会解析仓库中所有对象之间的引用关系，并更新到MetaInfo对象中。\n- get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager: 获取任务管理器。该方法会根据对象之间的引用关系，生成任务管理器，并返回一个TaskManager对象。\n- get_topology(self, task_available_func = None) -> TaskManager: 计算仓库中所有对象的拓扑顺序。该方法会解析仓库中所有对象之间的引用关系，并根据拓扑排序生成任务管理器，并返回一个TaskManager对象。\n- _map(self, deal_func: Callable): 对所有节点进行相同的操作。该方法会对MetaInfo对象中的所有节点进行相同的操作，通过传入的deal_func函数实现。\n- load_doc_from_older_meta(self, older_meta: MetaInfo): 从旧版本的metainfo中加载文档。该方法会将旧版本的metainfo中的文档信息合并到当前的MetaInfo对象中。\n- from_project_hierarchy_path(repo_path: str) -> MetaInfo: 从仓库路径中加载MetaInfo对象。该方法会根据仓库路径读取.project_hierarchy.json文件，并返回一个新的MetaInfo对象。\n- to_hierarchy_json(self, flash_reference_relation = False): 将MetaInfo对象转换为.project_hierarchy.json文件的格式。该方法会将MetaInfo对象转换为.project_hierarchy.json文件的格式，并返回一个字典。\n- from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo: 从.project_hierarchy.json文件的内容中加载MetaInfo对象。该方法会根据.project_hierarchy.json文件的内容生成MetaInfo对象，并返回一个新的MetaInfo对象。\n\n**注意**：\n- MetaInfo对象用于管理项目的元信息，包括仓库路径、文档版本、目标仓库的文件结构等。\n- 可以通过init_from_project_path方法从仓库路径中初始化MetaInfo对象。\n- 可以通过from_checkpoint_path方法从已有的metainfo目录中读取MetaInfo对象。\n- 可以通过checkpoint方法将MetaInfo对象保存到指定目录。\n- 可以通过print"
      ],
      "code_start_line": 218,
      "code_end_line": 640,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class MetaInfo():\n    repo_path: str = \"\"\n    document_version: str = \"\" #随时间变化，\"\"代表没完成，否则对应一个目标仓库的commit hash\n    target_repo_hierarchical_tree: DocItem = field(default_factory=\"Docitem\") #整个repo的文件结构\n    white_list: Any[List] = None\n\n    in_generation_process: bool = False\n\n    checkpoint_lock: threading.Lock = threading.Lock()\n\n    @staticmethod\n    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n    \n    @staticmethod\n    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n\n    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        with self.checkpoint_lock:\n            logger.info(f\"will save MetaInfo at {target_dir_path}\")\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n            with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            \n            with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n                meta = {\n                    \"repo_path\": self.repo_path,\n                    \"doc_version\": self.document_version,\n                    \"in_generation_process\": self.in_generation_process,\n                }\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n    \n    \n    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n\n    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n\n    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\"\"\"\n        now_node = file_node\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n\n            \n\n    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n        white_list_file_names = []\n        obj_names = []\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_file_names = [cont[\"id_text\"] for cont in self.white_list]\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            if white_list_file_names != [] and (file_node.get_file_name() not in white_list_file_names): #如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count\n                in_file_only = False\n                if white_list_file_names != [] and (now_obj.obj_name not in white_list_file_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=True,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    # if now_obj.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n                    #     print(reference_list)\n                    #     print(referencer_node.get_full_name())\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n            # logger.info(f\"find {ref_count} refer-relation in {file_node.get_full_name()}\")\n    \n\n    def get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n            doc_items = list(filter(in_white_list, doc_items))\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total = len(items_by_depth),desc=\"sorting topology order\")\n        while items_by_depth:\n            for item in items_by_depth:\n                if all(referenced in deal_items for referenced in item.reference_who):\n                    \"\"\"一个任务依赖于所有引用者和他的子节点\"\"\"\n                    item_denp_task_ids = []\n                    for _, child in item.children.items():\n                        if child.multithread_task_id in task_manager.task_dict.keys():\n                            item_denp_task_ids.append(child.multithread_task_id)\n                    for referenced_item in item.reference_who:\n                        if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                            item_denp_task_ids.append(referenced_item.multithread_task_id)\n                    item_denp_task_ids = list(set(item_denp_task_ids)) #去重\n                    if task_available_func == None or task_available_func(item):\n                        task_id = task_manager.add_task(dependency_task_id=item_denp_task_ids,extra=item)\n                        item.multithread_task_id = task_id\n                    deal_items.append(item)\n                    items_by_depth.remove(item)\n                    bar.update(1)\n                    break\n\n                    # #将尾递归转化为while的形式来解决最大深度的问题\n                    # while item.father is not None:\n                    #     father_node = item.father\n                    #     all_children_processed = True\n                    #     for _, node in father_node.children.items():\n                    #         if node not in sorted_items:\n                    #             all_children_processed = False\n                    #             break\n                    #     if not all_children_processed:\n                    #         break\n                    #     sorted_items.append(father_node)\n                    #     if father_node in items_by_depth:\n                    #         items_by_depth.remove(father_node)\n                    #     bar.update(1)\n                    #     item = father_node  # 更新item为父节点，继续循环\n                    # break\n\n        # Further optimization for minimizing tree distance could be added here\n        return task_manager\n\n    def get_topology(self, task_available_func = None) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n        return task_manager\n    \n    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n\n    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference() \n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n    \n    def to_hierarchy_json(self, flash_reference_relation = False):\n        \"\"\"\n        如果flash_reference_relation=True,则会将最新的双向引用关系写回到meta文件中\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n\n    @staticmethod\n    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "init_from_project_path": {
      "type": "FunctionDef",
      "name": "init_from_project_path",
      "md_content": [
        "**init_from_project_path**: init_from_project_path函数的作用是从一个仓库路径中初始化MetaInfo对象。\n**parameters**: 这个函数的参数是一个字符串类型的project_abs_path，表示仓库的绝对路径。\n**Code Description**: 这个函数首先将传入的project_abs_path赋值给CONFIG['repo_path']，然后使用日志记录器logger记录初始化MetaInfo的操作。接着，创建一个FileHandler对象file_handler，传入project_abs_path和None作为参数。然后调用file_handler的generate_overall_structure方法生成整个仓库的结构。接下来，使用MetaInfo类的from_project_hierarchy_json方法，将仓库结构作为参数，生成一个新的MetaInfo对象metainfo。最后，将project_abs_path赋值给metainfo的repo_path属性，并返回metainfo对象。\n**Note**: 使用这个函数时需要传入一个有效的仓库路径作为参数。\n**Output Example**: \n```\n{\n    \"repo_path\": \"/path/to/repo\",\n    \"other_info\": \"...\"\n}\n```"
      ],
      "code_start_line": 229,
      "code_end_line": 237,
      "parent": "MetaInfo",
      "params": [
        "project_abs_path"
      ],
      "have_return": true,
      "code_content": "    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "from_checkpoint_path": {
      "type": "FunctionDef",
      "name": "from_checkpoint_path",
      "md_content": [
        "**from_checkpoint_path**: from_checkpoint_path函数的功能是从已有的metainfo目录中读取metainfo信息。\n**parameters**: 这个函数的参数是checkpoint_dir_path，表示metainfo目录的路径。\n**Code Description**: 这个函数首先根据checkpoint_dir_path和\".project_hierarchy.json\"拼接出project_hierarchy_json_path，然后使用json.load()函数读取该文件的内容，将结果保存在project_hierarchy_json变量中。接着调用MetaInfo类的from_project_hierarchy_json()方法，将project_hierarchy_json作为参数传入，得到metainfo对象。然后再根据checkpoint_dir_path和\"meta-info.json\"拼接出meta_info_json_path，使用json.load()函数读取该文件的内容，将结果保存在meta_data变量中。最后，将meta_data中的\"repo_path\"、\"doc_version\"和\"in_generation_process\"分别赋值给metainfo对象的repo_path、document_version和in_generation_process属性。最后，使用logger.info()函数打印加载的meta-info信息，并返回metainfo对象。\n**Note**: 使用该函数前需要确保metainfo目录中存在\".project_hierarchy.json\"和\"meta-info.json\"文件。\n**Output Example**: \n```\nloading meta-info from /path/to/checkpoint_dir, document-version=\"1.0.0\"\n```"
      ],
      "code_start_line": 240,
      "code_end_line": 256,
      "parent": "MetaInfo",
      "params": [
        "checkpoint_dir_path"
      ],
      "have_return": true,
      "code_content": "    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "checkpoint": {
      "type": "FunctionDef",
      "name": "checkpoint",
      "md_content": [
        "**checkpoint**: checkpoint函数的功能是将MetaInfo保存到指定目录。\n**parameters**: 这个函数的参数有：\n- target_dir_path: 保存MetaInfo的目标目录路径，类型为字符串。\n- flash_reference_relation: 是否刷新引用关系，默认为False，类型为布尔值。\n**Code Description**: 这个函数的代码描述如下：\n- 首先，使用checkpoint_lock进行线程同步。\n- 然后，使用logger记录日志，表示将在target_dir_path处保存MetaInfo。\n- 如果target_dir_path不存在，则创建该目录。\n- 调用to_hierarchy_json函数生成当前的层次结构JSON，并根据flash_reference_relation参数刷新引用关系。\n- 将当前层次结构JSON以美观的格式写入.target_dir_path目录下的.project_hierarchy.json文件中。\n- 创建一个meta字典，包含repo_path、doc_version和in_generation_process等信息。\n- 将meta以美观的格式写入.target_dir_path目录下的meta-info.json文件中。\n**Note**: 使用该函数时需要注意以下几点：\n- target_dir_path参数需要传入一个有效的目录路径。\n- 如果目录不存在，函数会自动创建该目录。\n- 函数会将MetaInfo保存到目标目录下的.project_hierarchy.json和meta-info.json文件中。"
      ],
      "code_start_line": 258,
      "code_end_line": 273,
      "parent": "MetaInfo",
      "params": [
        "self",
        "target_dir_path",
        "flash_reference_relation"
      ],
      "have_return": false,
      "code_content": "    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        with self.checkpoint_lock:\n            logger.info(f\"will save MetaInfo at {target_dir_path}\")\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n            with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            \n            with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n                meta = {\n                    \"repo_path\": self.repo_path,\n                    \"doc_version\": self.document_version,\n                    \"in_generation_process\": self.in_generation_process,\n                }\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "print_task_list": {
      "type": "FunctionDef",
      "name": "print_task_list",
      "md_content": [
        "**print_task_list**: print_task_list函数的功能是打印任务列表。\n**parameters**: 这个函数的参数是item_list，表示任务列表。\n**Code Description**: 这个函数首先导入了prettytable模块，然后创建了一个名为task_table的表格对象，表格的列名分别是\"task_id\"、\"Doc Generation Reason\"和\"Path\"。接着定义了一个变量task_count并初始化为0。然后使用enumerate函数遍历item_list中的每个元素，对于每个元素，将其item_status的name属性、以及item的get_full_name()方法的返回值作为一行数据添加到task_table中，并将task_count的值作为task_id。最后打印\"Remain tasks to be done\"和task_table。\n**Note**: 这段代码使用了prettytable模块来创建一个漂亮的表格，并将任务列表中的任务信息添加到表格中进行展示。在使用这段代码之前，需要确保已经安装了prettytable模块。"
      ],
      "code_start_line": 276,
      "code_end_line": 284,
      "parent": "MetaInfo",
      "params": [
        "self",
        "item_list"
      ],
      "have_return": false,
      "code_content": "    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_all_files": {
      "type": "FunctionDef",
      "name": "get_all_files",
      "md_content": [
        "**get_all_files**: get_all_files函数的功能是获取所有的file节点\n**parameters**: 该函数没有参数\n**Code Description**: 该函数通过遍历目标仓库的层级树，获取所有的file节点，并将其存储在一个列表中返回。\n首先，函数创建了一个空列表files用于存储file节点。然后，定义了一个内部函数walk_tree，用于递归遍历树的节点。在walk_tree函数中，如果当前节点的item_type为_file，即为file节点，则将该节点添加到files列表中。然后，遍历当前节点的所有子节点，并对每个子节点调用walk_tree函数。最后，调用walk_tree函数，传入目标仓库的层级树的根节点self.target_repo_hierarchical_tree。最终，返回存储了所有file节点的列表files。\n**Note**: 该函数的返回值是一个列表，列表中的每个元素都是一个file节点。\n**Output Example**: \n返回值示例：\n[<DocItem object at 0x000001>, <DocItem object at 0x000002>, <DocItem object at 0x000003>]"
      ],
      "code_start_line": 286,
      "code_end_line": 295,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "walk_tree": {
      "type": "FunctionDef",
      "name": "walk_tree",
      "md_content": [
        "**walk_tree**: walk_tree函数的功能是遍历树形结构。\n**参数**: 这个函数的参数是now_node，表示当前节点。\n**代码描述**: 这个函数的作用是遍历树形结构，将文件节点添加到files列表中。首先判断当前节点的类型是否为文件，如果是文件，则将当前节点添加到files列表中。然后遍历当前节点的所有子节点，对每个子节点递归调用walk_tree函数。\n**注意**: 使用这段代码时需要注意以下几点：\n- 确保传入的参数now_node是一个有效的节点对象。\n- 确保在调用walk_tree函数之前，已经初始化了files列表。"
      ],
      "code_start_line": 289,
      "code_end_line": 293,
      "parent": "get_all_files",
      "params": [
        "now_node"
      ],
      "have_return": false,
      "code_content": "        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_obj_with_lineno": {
      "type": "FunctionDef",
      "name": "find_obj_with_lineno",
      "md_content": [
        "**find_obj_with_lineno**: find_obj_with_lineno函数的功能是在给定的文件节点中查找与指定起始行号对应的对象。\n**参数**: \n- self: 当前对象的实例\n- file_node: 文件节点，表示要在其中查找对象的文件节点\n- start_line_num: 起始行号，表示要查找的对象所在的起始行号\n**代码描述**: \n该函数通过遍历文件节点及其子节点的方式，在给定的文件节点中查找与指定起始行号对应的对象。函数首先将当前节点设置为文件节点，然后进入循环，直到当前节点没有子节点为止。在循环中，函数遍历当前节点的所有子节点，如果子节点的代码起始行号小于等于指定的起始行号，则将当前节点更新为该子节点，并标记找到了符合条件的子节点。如果没有找到符合条件的子节点，则返回当前节点。最终，函数返回当前节点作为查找结果。\n**注意**: \n- 函数假设文件节点及其子节点的content属性不为None。\n**输出示例**: \n假设给定的文件节点为file_node，起始行号为start_line_num，且在文件节点中存在与起始行号对应的对象，则函数返回与起始行号对应的对象的DocItem。"
      ],
      "code_start_line": 298,
      "code_end_line": 311,
      "parent": "MetaInfo",
      "params": [
        "self",
        "file_node",
        "start_line_num"
      ],
      "have_return": true,
      "code_content": "    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\"\"\"\n        now_node = file_node\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "parse_reference": {
      "type": "FunctionDef",
      "name": "parse_reference",
      "md_content": [
        "**parse_reference**: parse_reference函数的功能是双向提取所有引用关系。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: 该函数首先获取所有文件节点，然后根据白名单筛选出需要解析的文件名。接下来，对于每个文件节点，函数会遍历文件内的所有变量。在遍历过程中，函数会调用find_all_referencer函数找到所有引用了当前变量的位置，并将引用关系添加到相应的节点中。同时，函数还会判断当前变量和引用变量之间是否存在祖先节点关系，如果不存在，则将它们添加到彼此的引用列表中，并更新最大引用祖先节点。最后，函数会递归遍历当前变量的子节点，以处理嵌套的变量。\n\n**注意**: 在使用该函数时，需要注意以下几点：\n- 该函数依赖于get_all_files函数和find_all_referencer函数，因此在调用parse_reference函数之前，需要确保这两个函数已经正确实现。\n- 如果存在白名单，函数只会解析白名单中的对象，其他对象将被忽略。\n- 函数会根据引用关系更新节点的引用列表和最大引用祖先节点，这可能会对内存和性能产生一定的影响。因此，在处理大量数据时，需要注意性能问题。"
      ],
      "code_start_line": 315,
      "code_end_line": 373,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n        white_list_file_names = []\n        obj_names = []\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_file_names = [cont[\"id_text\"] for cont in self.white_list]\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            if white_list_file_names != [] and (file_node.get_file_name() not in white_list_file_names): #如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count\n                in_file_only = False\n                if white_list_file_names != [] and (now_obj.obj_name not in white_list_file_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=True,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    # if now_obj.get_full_name() == \"experiment2_gpt4_pdb.py/main\":\n                    #     print(reference_list)\n                    #     print(referencer_node.get_full_name())\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_task_manager": {
      "type": "FunctionDef",
      "name": "get_task_manager",
      "md_content": [
        "**get_task_manager**: get_task_manager函数的功能是根据拓扑引用关系获取任务管理器。\n\n**参数**: \n- now_node: 当前节点的文档项对象。\n- task_available_func: 可调用对象，用于判断任务是否可用。\n\n**代码描述**: \n该函数首先获取当前节点的所有文档项对象，并根据拓扑引用关系进行筛选。如果存在白名单，则通过in_white_list函数对文档项对象进行过滤。然后，根据深度对文档项对象进行排序，并初始化一些变量。接下来，使用循环遍历文档项对象，判断每个文档项对象的所有引用者和子节点是否都已处理。如果是，则获取这些引用者和子节点的多线程任务ID，并将其添加到任务管理器中。如果任务可用且满足任务可用函数的条件，则为文档项对象分配一个多线程任务ID，并将其添加到任务管理器中。最后，将已处理的文档项对象从列表中移除，并更新进度条。循环结束后，返回任务管理器。\n\n**注意**: \n- 该函数依赖于now_node对象的get_travel_list()方法和DocItem对象的get_file_name()和obj_name属性。\n- 如果存在白名单，需要提供一个包含文件路径和ID文本的字典列表。\n- 任务可用函数用于判断任务是否可用，如果不提供，则默认所有任务都可用。\n\n**输出示例**: \n以下是可能的返回值示例:\n```python\ntask_manager = {\n    \"task_dict\": {\n        \"task_id_1\": {\n            \"dependency_task_id\": [\"task_id_2\", \"task_id_3\"],\n            \"extra\": doc_item_1\n        },\n        \"task_id_2\": {\n            \"dependency_task_id\": [],\n            \"extra\": doc_item_2\n        },\n        \"task_id_3\": {\n            \"dependency_task_id\": [\"task_id_2\"],\n            \"extra\": doc_item_3\n        }\n    }\n}\n```"
      ],
      "code_start_line": 377,
      "code_end_line": 430,
      "parent": "MetaInfo",
      "params": [
        "self",
        "now_node",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n            doc_items = list(filter(in_white_list, doc_items))\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total = len(items_by_depth),desc=\"sorting topology order\")\n        while items_by_depth:\n            for item in items_by_depth:\n                if all(referenced in deal_items for referenced in item.reference_who):\n                    \"\"\"一个任务依赖于所有引用者和他的子节点\"\"\"\n                    item_denp_task_ids = []\n                    for _, child in item.children.items():\n                        if child.multithread_task_id in task_manager.task_dict.keys():\n                            item_denp_task_ids.append(child.multithread_task_id)\n                    for referenced_item in item.reference_who:\n                        if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                            item_denp_task_ids.append(referenced_item.multithread_task_id)\n                    item_denp_task_ids = list(set(item_denp_task_ids)) #去重\n                    if task_available_func == None or task_available_func(item):\n                        task_id = task_manager.add_task(dependency_task_id=item_denp_task_ids,extra=item)\n                        item.multithread_task_id = task_id\n                    deal_items.append(item)\n                    items_by_depth.remove(item)\n                    bar.update(1)\n                    break\n\n                    # #将尾递归转化为while的形式来解决最大深度的问题\n                    # while item.father is not None:\n                    #     father_node = item.father\n                    #     all_children_processed = True\n                    #     for _, node in father_node.children.items():\n                    #         if node not in sorted_items:\n                    #             all_children_processed = False\n                    #             break\n                    #     if not all_children_processed:\n                    #         break\n                    #     sorted_items.append(father_node)\n                    #     if father_node in items_by_depth:\n                    #         items_by_depth.remove(father_node)\n                    #     bar.update(1)\n                    #     item = father_node  # 更新item为父节点，继续循环\n                    # break\n\n        # Further optimization for minimizing tree distance could be added here\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "get_topology": {
      "type": "FunctionDef",
      "name": "get_topology",
      "md_content": [
        "**get_topology**: get_topology函数的功能是计算repo中所有对象的拓扑顺序。\n\n**参数**: get_topology函数有一个可选参数task_available_func。\n\n**代码描述**: get_topology函数首先调用self.parse_reference()方法解析引用关系。然后调用self.get_task_manager方法，传入self.target_repo_hierarchical_tree和task_available_func作为参数，获取任务管理器task_manager。最后，函数返回task_manager作为结果。\n\n**注意**: 在使用get_topology函数时，可以选择传入task_available_func参数来指定任务可用性的函数。\n\n**输出示例**: 假设函数的返回值如下所示：\n```\ntask_manager = {\n    'task1': ['task2', 'task3'],\n    'task2': ['task4'],\n    'task3': [],\n    'task4': []\n}\n```"
      ],
      "code_start_line": 432,
      "code_end_line": 437,
      "parent": "MetaInfo",
      "params": [
        "self",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_topology(self, task_available_func = None) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "_map": {
      "type": "FunctionDef",
      "name": "_map",
      "md_content": [
        "**_map**: _map函数的功能是将所有节点进行同一个操作。\n**参数**: 这个函数的参数是deal_func，它是一个可调用对象。\n**代码描述**: 这个函数定义了一个内部函数travel，它用来遍历所有节点并执行deal_func操作。travel函数接受一个参数now_item，表示当前节点。首先，它会调用deal_func函数来对当前节点进行操作。然后，它会遍历当前节点的所有子节点，并递归调用travel函数来对每个子节点执行相同的操作。最后，函数调用travel函数来对目标仓库的层次树进行遍历操作。\n**注意**: 使用这段代码时需要注意以下几点：\n- deal_func参数必须是一个可调用对象，可以是函数、方法或者其他可调用的对象。\n- _map函数会对目标仓库的层次树进行遍历操作，所以在使用时需要确保目标仓库的层次树已经构建完成。"
      ],
      "code_start_line": 439,
      "code_end_line": 445,
      "parent": "MetaInfo",
      "params": [
        "self",
        "deal_func"
      ],
      "have_return": false,
      "code_content": "    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "load_doc_from_older_meta": {
      "type": "FunctionDef",
      "name": "load_doc_from_older_meta",
      "md_content": [
        "**load_doc_from_older_meta**: load_doc_from_older_meta函数的作用是从旧版本的meta info中加载文档。\n\n**参数**: 这个函数的参数是older_meta，它是一个MetaInfo对象，表示旧版本的meta info。\n\n**代码描述**: 这个函数首先使用logger记录一条信息，表示正在从旧版本的metainfo中合并文档。然后，它获取目标仓库的层次树的根节点，并定义了一个名为find_item的内部函数，用于在新版本的meta中查找原来的某个项。接下来，它定义了一个名为travel的内部函数，用于遍历旧版本的meta info，并将相应的文档信息合并到新版本的meta info中。在遍历过程中，如果发现源码被修改了，它会将相应的项的状态设置为\"code_changed\"。然后，它调用self.parse_reference()函数来解析现在的双向引用，观察引用者是否有改变。最后，它定义了一个名为travel2的内部函数，用于遍历旧版本的meta info，并检查引用者是否有变化。如果引用者有变化，并且项的状态为\"doc_up_to_date\"，则根据情况将项的状态设置为\"referencer_not_exist\"或\"add_new_referencer\"。\n\n**注意**: 使用这段代码时需要注意以下几点：\n- older_meta参数必须是一个已经生成了文档的旧版本的MetaInfo对象。\n- 函数内部使用了logger来记录日志信息，需要确保logger已经正确配置。\n- 函数中使用了一些自定义的类和枚举类型，需要确保这些类和枚举类型已经正确定义。\n\n**输出示例**: 无法提供具体的输出示例，因为它依赖于具体的输入和环境。"
      ],
      "code_start_line": 447,
      "code_end_line": 501,
      "parent": "MetaInfo",
      "params": [
        "self",
        "older_meta"
      ],
      "have_return": true,
      "code_content": "    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference() \n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "travel": {
      "type": "FunctionDef",
      "name": "travel",
      "md_content": [
        "**travel**: travel函数的功能是寻找源码是否被修改的信息。\n\n**parameters**: travel函数接受一个参数now_older_item，该参数是一个DocItem对象，表示当前的旧版本item。\n\n**Code Description**: travel函数首先通过调用find_item函数来查找当前旧版本item在新版本文件中的对应item。如果找不到对应item，则表示新版本文件中删除了该item，此时函数会直接返回。\n\n如果找到了对应item，则将当前旧版本item的md_content和item_status属性赋值给对应item。如果当前旧版本item的content字典中包含\"code_content\"键，并且对应item的content字典中也包含\"code_content\"键，则比较两者的值是否相等。如果不相等，则表示源码被修改了，此时将对应item的item_status属性设置为DocItemStatus.code_changed。\n\n接下来，函数会遍历当前旧版本item的所有子item，并递归调用travel函数来处理每个子item。\n\n**Note**: travel函数用于判断源码是否被修改，并更新对应item的属性。在调用travel函数之前，需要确保now_older_item参数是一个有效的DocItem对象。\n\n**Output Example**: \n假设当前旧版本item的md_content为\"旧版本内容\"，item_status为DocItemStatus.unchanged，content为{\"code_content\": \"旧版本源码\"}。假设在新版本文件中找到了对应item，并且对应item的md_content为\"新版本内容\"，item_status为DocItemStatus.unchanged，content为{\"code_content\": \"新版本源码\"}。由于旧版本源码和新版本源码不相等，所以对应item的item_status被设置为DocItemStatus.code_changed。"
      ],
      "code_start_line": 465,
      "code_end_line": 480,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "find_item": {
      "type": "FunctionDef",
      "name": "find_item",
      "md_content": [
        "**find_item**: find_item函数的功能是在新版的meta中查找原来的某个东西。\n**parameters**: find_item函数接受一个参数now_item，类型为DocItem，表示要查找的当前项。\n**Code Description**: find_item函数通过递归的方式在新版的meta中查找原来的某个东西。首先判断当前项是否为根节点，如果是，则直接返回根节点。如果不是根节点，则通过递归调用find_item函数查找当前项的父节点。如果父节点不存在，则返回None。如果父节点存在，则判断当前项的名称是否在父节点的子节点中，如果在，则返回该子节点，否则返回None。\n**Note**: 使用该代码时需要注意以下几点：\n- find_item函数需要传入一个DocItem类型的参数now_item。\n- find_item函数返回一个Optional[DocItem]类型的值，表示可能找到的原来的某个东西。\n**Output Example**: \n假设当前项的父节点存在且当前项的名称在父节点的子节点中，则返回该子节点。\n假设当前项的父节点不存在，则返回None。"
      ],
      "code_start_line": 452,
      "code_end_line": 462,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_item"
      ],
      "have_return": true,
      "code_content": "        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "travel2": {
      "type": "FunctionDef",
      "name": "travel2",
      "md_content": [
        "**travel2**: travel2函数的功能是遍历now_older_item的子项，并对每个子项进行一系列操作。\n**参数**: travel2函数接受一个参数now_older_item，该参数是一个DocItem对象，表示当前的旧版本项。\n**代码描述**: travel2函数首先通过调用find_item函数查找now_older_item在新版本中的对应项result_item。如果找不到对应项，则直接返回。接下来，travel2函数会比较result_item引用的人是否发生了变化。它会将result_item引用的人的全名存储在new_reference_names列表中，将now_older_item引用的人的全名存储在old_reference_names列表中。如果new_reference_names和old_reference_names不相等，并且result_item的状态为DocItemStatus.doc_up_to_date，那么travel2函数会根据不同的情况更新result_item的状态。如果new_reference_names是old_reference_names的子集，说明旧的引用者包含了新的引用者，此时将result_item的状态更新为DocItemStatus.referencer_not_exist。否则，将result_item的状态更新为DocItemStatus.add_new_referencer。最后，travel2函数会递归调用自身，对now_older_item的每个子项进行相同的操作。\n**注意**: travel2函数依赖于find_item函数和DocItem类的定义。在调用travel2函数之前，需要确保now_older_item是一个有效的DocItem对象。\n**输出示例**: travel2函数没有明确的返回值，它的操作是在result_item上进行的，可能会更新result_item的状态。"
      ],
      "code_start_line": 486,
      "code_end_line": 500,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "from_project_hierarchy_path": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_path",
      "md_content": [
        "**from_project_hierarchy_path**: from_project_hierarchy_path函数的作用是将project_hierarchy_json文件转换为MetaInfo对象。\n**parameters**: 这个函数的参数是repo_path，表示仓库路径。\n**Code Description**: 这个函数首先通过os模块的join方法将.repo_path和\".project_hierarchy.json\"拼接成project_hierarchy_json_path，然后使用logger模块记录日志，输出parsing from和project_hierarchy_json_path的值。接下来，通过os模块的exists方法判断project_hierarchy_json_path是否存在，如果不存在则抛出NotImplementedError异常。然后使用open函数打开project_hierarchy_json_path文件，并以utf-8编码方式读取文件内容，将内容加载为project_hierarchy_json对象。最后，调用MetaInfo类的from_project_hierarchy_json方法，将project_hierarchy_json作为参数传入，返回MetaInfo对象。\n**Note**: 使用该函数前需要确保.repo_path目录下存在.project_hierarchy.json文件。\n**Output Example**: \n```\n{\n    \"name\": \"project_name\",\n    \"path\": \"project_path\",\n    \"files\": [\n        {\n            \"name\": \"file1\",\n            \"path\": \"file1_path\"\n        },\n        {\n            \"name\": \"file2\",\n            \"path\": \"file2_path\"\n        }\n    ]\n}\n```"
      ],
      "code_start_line": 505,
      "code_end_line": 515,
      "parent": "MetaInfo",
      "params": [
        "repo_path"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "to_hierarchy_json": {
      "type": "FunctionDef",
      "name": "to_hierarchy_json",
      "md_content": [
        "**to_hierarchy_json**: to_hierarchy_json函数的功能是将项目中的文件层级结构转换为JSON格式的数据。\n\n**参数**: to_hierarchy_json函数有一个可选参数flash_reference_relation，默认值为False。如果设置为True，则会将最新的双向引用关系写回到meta文件中。\n\n**代码描述**: to_hierarchy_json函数首先创建一个空的层级结构的JSON对象hierachy_json。然后通过调用get_all_files函数获取所有的文件对象列表file_item_list。接下来，对于每个文件对象file_item，都会创建一个空的文件层级结构的JSON对象file_hierarchy_content。\n\n在函数内部定义了一个名为walk_file的递归函数，用于遍历文件对象及其子对象。在walk_file函数中，首先将当前对象的相关信息添加到file_hierarchy_content中，包括对象的名称、类型、Markdown内容、状态等。如果flash_reference_relation参数为True，则还会将对象的引用关系添加到file_hierarchy_content中。然后判断当前对象的父对象是否为文件对象，如果不是，则将父对象的名称作为当前对象的父节点。接着，遍历当前对象的子对象，递归调用walk_file函数。\n\n最后，将file_hierarchy_content添加到hierachy_json中，以文件对象的全名作为键。\n\n最后，函数返回hierachy_json，即转换后的文件层级结构的JSON数据。\n\n**注意**: 使用该代码时需要注意以下几点：\n- 可以通过设置flash_reference_relation参数为True来获取文件对象的引用关系。\n- 函数返回的是一个JSON对象，表示项目中的文件层级结构。\n\n**输出示例**:\n```python\n{\n    \"repo_agent.doc_meta_info.MetaInfo\": {\n        \"file1\": {\n            \"name\": \"file1\",\n            \"type\": \"file\",\n            \"md_content\": \"This is the content of file1\",\n            \"item_status\": \"active\",\n            \"parent\": \"repo_agent.doc_meta_info.MetaInfo\"\n        },\n        \"file2\": {\n            \"name\": \"file2\",\n            \"type\": \"file\",\n            \"md_content\": \"This is the content of file2\",\n            \"item_status\": \"active\",\n            \"parent\": \"repo_agent.doc_meta_info.MetaInfo\"\n        },\n        \"folder1\": {\n            \"name\": \"folder1\",\n            \"type\": \"folder\",\n            \"md_content\": \"This is the content of folder1\",\n            \"item_status\": \"active\",\n            \"parent\": \"repo_agent.doc_meta_info.MetaInfo\"\n        },\n        \"folder2\": {\n            \"name\": \"folder2\",\n            \"type\": \"folder\",\n            \"md_content\": \"This is the content of folder2\",\n            \"item_status\": \"active\",\n            \"parent\": \"repo_agent.doc_meta_info.MetaInfo\"\n        }\n    }\n}\n```"
      ],
      "code_start_line": 517,
      "code_end_line": 548,
      "parent": "MetaInfo",
      "params": [
        "self",
        "flash_reference_relation"
      ],
      "have_return": true,
      "code_content": "    def to_hierarchy_json(self, flash_reference_relation = False):\n        \"\"\"\n        如果flash_reference_relation=True,则会将最新的双向引用关系写回到meta文件中\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "from_project_hierarchy_json": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_json",
      "md_content": [
        "**from_project_hierarchy_json**: from_project_hierarchy_json函数的作用是根据项目层级结构的JSON数据创建MetaInfo对象。\n**parameters**: from_project_hierarchy_json函数接受一个project_hierarchy_json参数，该参数是一个包含项目层级结构的JSON数据。\n**Code Description**: from_project_hierarchy_json函数的代码逻辑如下：\n\n1. 首先，创建一个名为target_meta_info的MetaInfo对象，作为返回结果。该对象的属性target_repo_hierarchical_tree表示整个项目的层级结构，初始时只有一个根节点。\n\n2. 遍历project_hierarchy_json中的每个文件，进行文件层级结构的解析和内容的解析。\n\n3. 对于每个文件，首先判断文件是否存在和是否为空，如果不存在或为空，则跳过该文件。\n\n4. 将文件路径按\"/\"分割为多个层级，然后根据层级逐级解析文件的层级结构。如果当前层级在层级结构中不存在，则创建一个对应的DocItem对象，并设置其类型为目录（DocItemType._dir），同时设置其父节点为当前层级的父节点。如果当前层级在层级结构中已存在，则直接将当前层级的节点设置为当前节点，并继续解析下一个层级。\n\n5. 解析完文件的层级结构后，创建一个对应的DocItem对象，并设置其类型为文件（DocItemType._file），同时设置其父节点为当前节点。\n\n6. 接下来，解析文件的内容。首先判断文件内容的类型是否为字典，如果不是，则抛出异常。\n\n7. 定义一个名为parse_one_item的内部函数，用于递归解析文件内容中的每个项。该函数接受三个参数：key表示项的名称，value表示项的内容，item_reflection表示已解析的项的映射关系。\n\n8. 在parse_one_item函数中，首先判断项是否已经解析过，如果已解析过，则直接返回。如果项有父节点，则先解析父节点。\n\n9. 创建一个对应的DocItem对象，并设置其属性。根据项的类型设置DocItem的类型，如果项的父节点不为空，则将当前项添加到父节点的子节点中，否则将当前项添加到文件节点的子节点中。\n\n10. 遍历文件内容中的每个项，调用parse_one_item函数进行解析。\n\n11. 解析完文件内容后，调用target_meta_info对象的parse_tree_path方法，解析整个层级结构的路径。\n\n12. 调用target_meta_info对象的check_depth方法，检查层级结构的深度。\n\n13. 返回target_meta_info对象作为函数的结果。\n\n**Note**: 使用该代码时需要注意以下几点：\n- 传入的project_hierarchy_json参数必须是一个包含项目层级结构的JSON数据。\n- 文件路径中的层级使用\"/\"进行分隔。\n- 文件内容的类型必须为字典，且包含特定的字段。\n\n**Output Example**: \n```\n{\n    \"target_repo_hierarchical_tree\": {\n        \"item_type\": \"_repo\",\n        \"obj_name\": \"full_repo\",\n        \"children\": {\n            \"folder1\": {\n                \"item_type\": \"_dir\",\n                \"md_content\": \"\",\n                \"obj_name\": \"folder1\",\n                \"father\": {\n                    \"item_type\": \"_repo\",\n                    \"obj_name\": \"full_repo\",\n                    \"children\": {\n                        \"folder1\": {\n                            \"item_type\": \"_dir\",\n                            \"md_content\": \"\",\n                            \"obj_name\": \"folder1\",\n                            \"father\": {\n                                \"item_type\": \"_repo\",\n                                \"obj_name\": \"full_repo\",\n                                \"children\": {\n                                    \"folder1\": {\n                                        \"item_type\": \"_dir\",\n                                        \"md_content\": \"\",\n                                        \"obj_name\": \"folder1\",\n                                        \"father\": {\n                                            \"item_type\": \"_repo\",\n                                            \"obj_name\": \"full_repo\",\n                                            \"children\": {\n                                                \"folder1\": {\n                                                    \"item_type\": \"_dir\",\n                                                    \"md_content\": \"\",\n                                                    \"obj_name\": \"folder1\",\n                                                    \"father\": {\n                                                        \"item_type\": \"_repo\",\n                                                        \"obj_name\": \"full_repo\",\n                                                        \"children\":"
      ],
      "code_start_line": 551,
      "code_end_line": 640,
      "parent": "MetaInfo",
      "params": [
        "project_hierarchy_json"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "walk_file": {
      "type": "FunctionDef",
      "name": "walk_file",
      "md_content": [
        "**walk_file**: walk_file函数的功能是将当前对象及其子对象的信息逐层添加到文件层次结构中。\n**参数**: now_obj: DocItem类型，表示当前对象。\n**代码说明**: walk_file函数首先将当前对象的信息添加到文件层次结构中，包括对象名称、类型、Markdown内容和状态。然后，如果存在引用关系，将当前对象引用和被引用的对象添加到文件层次结构中。接下来，将当前对象的父对象添加到文件层次结构中，如果父对象不是文件类型。最后，对当前对象的每个子对象递归调用walk_file函数。\n**注意**: 在使用该代码时需要注意以下几点：\n- walk_file函数需要传入一个DocItem类型的参数now_obj，表示当前对象。\n- walk_file函数会将当前对象及其子对象的信息逐层添加到文件层次结构中，因此需要确保文件层次结构的正确性。\n- 如果存在引用关系，需要确保引用和被引用的对象在文件层次结构中都正确添加。"
      ],
      "code_start_line": 526,
      "code_end_line": 543,
      "parent": null,
      "params": [
        "now_obj"
      ],
      "have_return": false,
      "code_content": "            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "in_white_list": {
      "type": "FunctionDef",
      "name": "in_white_list",
      "md_content": [
        "**in_white_list**: in_white_list函数的作用是判断给定的DocItem对象是否在白名单中。\n**参数**: 这个函数的参数是一个DocItem对象。\n**代码描述**: 这个函数通过遍历白名单中的每个元素，判断给定的DocItem对象是否与白名单中的某个元素匹配。匹配的条件是给定的DocItem对象的文件名与白名单元素的file_path属性相等，并且给定的DocItem对象的obj_name属性与白名单元素的id_text属性相等。如果找到匹配的元素，则返回True，否则返回False。\n**注意**: 使用这段代码需要注意以下几点：\n- 这个函数依赖于self.white_list属性，调用这个函数之前需要确保self.white_list已经被正确初始化。\n- 给定的DocItem对象需要正确设置文件名和对象名，否则无法正确判断是否在白名单中。\n**输出示例**: 假设白名单中有一个元素，其file_path属性为\"doc_meta_info.py\"，id_text属性为\"EdgeType\"，给定的DocItem对象的文件名为\"doc_meta_info.py\"，对象名为\"EdgeType\"，则调用in_white_list函数会返回True。"
      ],
      "code_start_line": 382,
      "code_end_line": 386,
      "parent": null,
      "params": [
        "item"
      ],
      "have_return": true,
      "code_content": "            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    },
    "parse_one_item": {
      "type": "FunctionDef",
      "name": "parse_one_item",
      "md_content": [
        "**parse_one_item**: parse_one_item函数的功能是解析一个项目。\n**参数**: 这个函数的参数。\n- key: 项目的键值\n- value: 项目的值\n- item_reflection: 项目的反射信息\n\n**代码描述**: 这个函数用于解析一个项目，并将解析结果存储在item_reflection中。首先，函数会检查项目的键值是否已经存在于item_reflection中，如果存在则直接返回。接下来，函数会检查项目是否有父项目，如果有，则先解析父项目。然后，函数会创建一个DocItem对象，并将项目的相关信息存储在该对象中。如果项目有item_status字段，则将其转换为对应的DocItemStatus枚举值，并赋值给DocItem对象的item_status属性。如果项目有reference_who字段，则将其赋值给DocItem对象的reference_who_name_list属性。如果项目有who_reference_me字段，则将其赋值给DocItem对象的who_reference_me_name_list属性。接着，函数会将当前项目添加到父项目的children属性中，并将父项目赋值给当前项目的father属性。如果项目没有父项目，则将当前项目添加到file_item的children属性中，并将file_item赋值给当前项目的father属性。最后，根据项目的type字段，将DocItem对象的item_type属性设置为对应的DocItemType枚举值。\n\n**注意**: 使用该代码时需要注意以下几点：\n- 该函数会递归解析项目，如果项目已经解析过则会跳过。\n- 如果项目有父项目，则会先解析父项目。\n- 项目的信息会存储在item_reflection中，可以通过该对象获取解析结果。\n\n**输出示例**:\n假设有以下输入：\n- key: \"item1\"\n- value: {\"parent\": \"item2\", \"md_content\": \"这是一个示例项目\", \"type\": \"FunctionDef\"}\n- item_reflection: {}\n\n则函数的调用结果为：\n- item_reflection: {\"item1\": DocItem(obj_name=\"item1\", content={\"parent\": \"item2\", \"md_content\": \"这是一个示例项目\", \"type\": \"FunctionDef\"}, md_content=\"这是一个示例项目\", item_type=DocItemType._function, father=DocItem(obj_name=\"item2\", ...), children={}), \"item2\": DocItem(obj_name=\"item2\", ...)}"
      ],
      "code_start_line": 595,
      "code_end_line": 631,
      "parent": null,
      "params": [
        "key",
        "value",
        "item_reflection"
      ],
      "have_return": true,
      "code_content": "            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ],
      "reference_who": []
    }
  },
  "repo_agent/chat_engine.py": {
    "get_import_statements": {
      "type": "FunctionDef",
      "name": "get_import_statements",
      "md_content": [
        "**get_import_statements**: get_import_statements函数的功能是获取当前对象的导入语句列表。\n**参数**: 这个函数没有参数。\n**代码描述**: 这个函数首先使用inspect模块的getsourcelines函数获取当前对象的源代码行列表。然后，它使用列表推导式从源代码行列表中筛选出以'import'或'from'开头的行，将它们保存在import_lines列表中。最后，函数返回import_lines列表，即当前对象的导入语句列表。\n**注意**: 使用这段代码时需要注意以下几点：\n- 该函数没有参数，直接调用即可。\n- 当前对象的导入语句列表是根据源代码动态生成的，因此在不同的环境中可能会有所不同。\n**输出示例**: 下面是一个可能的返回值的示例：\n```\n['import sys\\n', 'import inspect\\n', 'import os\\n']\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 19,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/chat_engine.py/build_path_tree",
        "repo_agent/chat_engine.py/build_path_tree/tree",
        "repo_agent/chat_engine.py/build_path_tree/tree_to_string",
        "repo_agent/chat_engine.py/ChatEngine/num_tokens_from_string",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referenced_prompt",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referencer_prompt"
      ]
    },
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [
        "**build_path_tree**: build_path_tree函数的功能是构建路径树。\n**参数**: build_path_tree函数接受三个参数:\n- who_reference_me: 一个包含字符串路径的列表，表示引用当前对象的对象的路径列表。\n- reference_who: 一个包含字符串路径的列表，表示当前对象引用的对象的路径列表。\n- doc_item_path: 一个字符串，表示当前对象的文档路径。\n\n**代码描述**: build_path_tree函数首先定义了一个内部函数tree，用于创建一个默认字典的树结构。然后，它创建了一个空的路径树path_tree。接下来，它遍历who_reference_me和reference_who两个路径列表，将每个路径按照分隔符(os.sep)分割成多个部分，然后将这些部分逐级添加到路径树中。接着，它处理doc_item_path，将其按照分隔符分割成多个部分，并在最后一个对象前面加上星号，然后将这些部分逐级添加到路径树中。最后，它定义了一个内部函数tree_to_string，用于将路径树转换为字符串表示。最后，它返回了路径树的字符串表示。\n\n**注意**: 在使用build_path_tree函数时，需要确保传入正确的参数，包括who_reference_me、reference_who和doc_item_path。此外，需要注意路径的分隔符应该与操作系统相匹配。\n\n**输出示例**:\n```\n├─ who_reference_me\n│   ├─ path1\n│   └─ path2\n├─ reference_who\n│   ├─ path3\n│   └─ path4\n└─ doc_item_path\n    └─ ✳️path5\n```"
      ],
      "code_start_line": 21,
      "code_end_line": 48,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [
        "**tree**: tree函数的功能是返回一个defaultdict(tree)对象。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数的代码非常简单，只有一行代码。它使用了collections模块中的defaultdict类来创建一个默认值为tree的字典对象，并将其作为返回值返回。\n**注意**: 使用该函数时需要先导入collections模块。\n**输出示例**: 以下是该函数可能返回的结果示例:\n```\ndefaultdict(<function tree at 0x00000123456789>, {})\n```"
      ],
      "code_start_line": 22,
      "code_end_line": 23,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [
        "**tree_to_string**: tree_to_string函数的功能是将树结构转换为字符串表示。\n**参数**: 这个函数的参数。\n- tree: 一个字典类型的树结构。\n- indent: 可选参数，表示缩进的级别，默认为0。\n**代码描述**: 这个函数通过递归的方式遍历树结构，并将每个节点的键值对转换为字符串表示。首先，函数会对树结构按键进行排序。然后，对于每个键值对，函数会根据缩进级别生成相应数量的空格，并将键添加到字符串s中。如果值是一个字典类型，则递归调用tree_to_string函数，并将缩进级别加1。最后，函数返回字符串s。\n**注意**: 使用这段代码时需要注意以下几点：\n- tree参数必须是一个字典类型的树结构。\n- indent参数表示缩进的级别，可以根据需要进行调整。\n**输出示例**: 对于给定的树结构，函数返回的字符串可能如下所示：\n```\nroot\n    node1\n        leaf1\n        leaf2\n    node2\n        leaf3\n    node3\n        leaf4\n        leaf5\n```\n这个示例中，树结构包含一个根节点root，根节点下有三个子节点node1、node2和node3。其中，node1下有两个叶子节点leaf1和leaf2，node2下有一个叶子节点leaf3，node3下有两个叶子节点leaf4和leaf5。函数将树结构转换为字符串表示时，根节点和子节点之间使用不同级别的缩进进行区分，叶子节点不再缩进。"
      ],
      "code_start_line": 40,
      "code_end_line": 46,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "ChatEngine": {
      "type": "ClassDef",
      "name": "ChatEngine",
      "md_content": [
        "**ChatEngine**: ChatEngine的功能是生成函数或类的文档。\n**attributes**: 这个类的属性。\n**Code Description**: 这个类的描述。\nChatEngine是一个用于生成函数或类文档的类。它具有以下方法和属性：\n\n- `__init__(self, CONFIG)`: 这是ChatEngine类的构造函数。它接受一个CONFIG参数，并将其赋值给self.config属性。\n\n- `num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int`: 这个方法接受一个字符串参数和一个编码名称参数，并返回文本字符串中的标记数。\n\n- `generate_doc(self, doc_item: DocItem, file_handler)`: 这个方法接受一个DocItem对象和一个文件处理器参数，并生成文档。\n\n**Note**: 在使用这个类的代码中，需要注意以下几点：\n\n- 在实例化ChatEngine对象时，需要传入一个CONFIG参数。\n- 在调用num_tokens_from_string方法时，需要传入一个字符串参数和一个可选的编码名称参数。\n- 在调用generate_doc方法时，需要传入一个DocItem对象和一个文件处理器参数。\n\n**Output Example**: 这是代码返回值的一个示例。"
      ],
      "code_start_line": 50,
      "code_end_line": 278,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChatEngine:\n    \"\"\"\n    ChatEngine is used to generate the doc of functions or classes.\n    \"\"\"\n    def __init__(self, CONFIG):\n        self.config = CONFIG\n\n    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                # print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=project_structure_prefix,\n                        project_structure=project_structure, \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=combine_ref_situation, \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            reference_letter=reference_letter, \n                            referencer_content=referencer_content,\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的作用是初始化ChatEngine对象。\n**参数**: 这个函数的参数。\n- CONFIG: 配置参数，用于初始化ChatEngine对象。\n**代码描述**: 这个函数用于初始化ChatEngine对象，并将传入的配置参数赋值给对象的config属性。\n**详细代码分析和描述**: \n在这个函数中，我们首先接收一个名为CONFIG的参数，该参数用于初始化ChatEngine对象。然后，我们将传入的CONFIG参数赋值给对象的config属性。这样，我们就可以在ChatEngine对象的其他方法中使用这个配置参数了。\n\n**注意**: \n- 在创建ChatEngine对象时，需要传入一个有效的配置参数。\n- 通过调用__init__函数，可以创建一个已经初始化的ChatEngine对象。"
      ],
      "code_start_line": 54,
      "code_end_line": 55,
      "parent": "ChatEngine",
      "params": [
        "self",
        "CONFIG"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, CONFIG):\n        self.config = CONFIG\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "num_tokens_from_string": {
      "type": "FunctionDef",
      "name": "num_tokens_from_string",
      "md_content": [
        "**num_tokens_from_string**: num_tokens_from_string函数的功能是返回文本字符串中的标记数量。\n**参数**: 这个函数的参数。\n- string: str类型，表示要计算标记数量的文本字符串。\n- encoding_name: str类型，表示要使用的编码名称，默认为\"cl100k_base\"。\n**代码描述**: 这个函数通过使用指定的编码将文本字符串编码为标记列表，并返回标记列表的长度，即标记数量。\n- 首先，函数使用tiktoken.get_encoding(encoding_name)获取指定名称的编码。\n- 然后，函数使用获取到的编码对文本字符串进行编码，并将结果保存在encoding变量中。\n- 最后，函数返回编码后的标记列表的长度，即标记数量。\n**注意**: 使用这段代码时需要注意以下几点：\n- 参数string必须是一个有效的文本字符串。\n- 参数encoding_name必须是一个有效的编码名称，否则会引发异常。\n**输出示例**: 模拟代码返回值的可能外观。\n例如，如果输入字符串为\"Hello, world!\"，编码名称为\"cl100k_base\"，则代码的返回值可能为7。"
      ],
      "code_start_line": 57,
      "code_end_line": 61,
      "parent": "ChatEngine",
      "params": [
        "self",
        "string",
        "encoding_name"
      ],
      "have_return": true,
      "code_content": "    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "generate_doc": {
      "type": "FunctionDef",
      "name": "generate_doc",
      "md_content": [
        "**generate_doc**: generate_doc函数的功能是生成代码文档。\n**parameters**: generate_doc函数的参数包括doc_item和file_handler。\n**Code Description**: generate_doc函数用于生成代码文档。首先，它从doc_item中获取代码信息，包括代码类型、代码名称、代码内容、是否有返回值等。然后，它判断代码是否被其他对象引用。接下来，它根据代码的引用情况和项目的层级结构生成项目结构信息。之后，它根据配置文件中的语言设置，确定代码的语言类型。然后，它根据代码的类型和是否有返回值，生成相应的提示信息。接着，它获取代码的引用和被引用情况，并生成相应的提示信息。最后，它使用OpenAI的API将系统提示和用户提示发送给模型，获取生成的文档内容。\n**Note**: 使用时需要注意代码的长度限制，如果超过限制，需要使用更大的模型进行处理。"
      ],
      "code_start_line": 63,
      "code_end_line": 278,
      "parent": "ChatEngine",
      "params": [
        "self",
        "doc_item",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                # print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=project_structure_prefix,\n                        project_structure=project_structure, \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=combine_ref_situation, \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            reference_letter=reference_letter, \n                            referencer_content=referencer_content,\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_referenced_prompt": {
      "type": "FunctionDef",
      "name": "get_referenced_prompt",
      "md_content": [
        "**get_referenced_prompt**: get_referenced_prompt函数的功能是获取引用了该代码的对象的相关信息。\n**参数**: 这个函数的参数是一个DocItem对象，表示文档项。\n**代码描述**: 这个函数首先判断文档项的reference_who属性的长度是否为0，如果是0则返回空字符串。接着，函数会遍历文档项的reference_who属性，对于每一个引用了该代码的对象，函数会生成一个包含对象的全名、文档和原始代码的字符串，并将其添加到prompt列表中。最后，函数将prompt列表中的字符串用换行符连接起来并返回。\n**注意**: 该函数依赖于DocItem对象的reference_who属性，如果该属性为空，则函数会返回空字符串。\n**输出示例**: 假设有两个引用了该代码的对象，其相关信息如下：\nobj: repo_agent/chat_engine.py/get_import_statements\nDocument: None\nRaw code:```\ndef get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n\n```==========\nobj: repo_agent/chat_engine.py/ChatEngine/generate_doc\nDocument: None\nRaw code:```\ndef generate_doc():\n    doc_item = DocItem()\n    doc_item.reference_who = [get_referenced_prompt]\n    return doc_item\n\n```==========\n则函数的返回值为：\nAs you can see, the code calls the following objects, their code and docs are as following:\nobj: repo_agent/chat_engine.py/get_import_statements\nDocument: None\nRaw code:```\ndef get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n\n```==========\nobj: repo_agent/chat_engine.py/ChatEngine/generate_doc\nDocument: None\nRaw code:```\ndef generate_doc():\n    doc_item = DocItem()\n    doc_item.reference_who = [get_referenced_prompt]\n    return doc_item\n\n```=========="
      ],
      "code_start_line": 110,
      "code_end_line": 117,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    },
    "get_referencer_prompt": {
      "type": "FunctionDef",
      "name": "get_referencer_prompt",
      "md_content": [
        "**get_referencer_prompt**: get_referencer_prompt函数的功能是获取引用了某个对象的所有对象的代码和文档信息。\n**参数**: 这个函数的参数是一个DocItem对象，表示待查询的对象。\n**代码描述**: 这个函数首先判断待查询对象是否被其他对象引用，如果没有被引用则返回空字符串。然后，函数会遍历所有引用了待查询对象的对象，获取它们的全名、文档信息和原始代码，并将它们拼接成一个字符串列表。最后，函数将字符串列表用换行符连接起来并返回。\n**注意**: 使用这段代码时需要注意以下几点：\n- 参数doc_item必须是一个有效的DocItem对象。\n- 如果待查询对象没有被其他对象引用，则返回的字符串为空。\n**输出示例**: 下面是一个可能的返回值的示例：\n```\nAlso, the code has been referenced by the following objects, their code and docs are as following:\nobj: repo_agent/chat_engine.py/get_import_statements\nDocument: None\nRaw code:```\ndef get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n\n```==========\n```"
      ],
      "code_start_line": 120,
      "code_end_line": 127,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/get_import_statements"
      ],
      "reference_who": []
    }
  },
  "repo_agent/prompt.py": {},
  "repo_agent/change_detector.py": {
    "ChangeDetector": {
      "type": "ClassDef",
      "name": "ChangeDetector",
      "md_content": [
        "**ChangeDetector**: ChangeDetector的功能是检测代码变更。\n\n**属性**: \n- repo_path (str): 代码仓库的路径\n- repo (git.Repo): Git仓库对象\n\n**代码描述**: \nChangeDetector类是一个用于检测代码变更的工具类。它提供了一些方法来获取已经暂存的Python文件以及它们的变更内容。\n\n- `__init__(self, repo_path)`: 初始化ChangeDetector对象。参数repo_path是代码仓库的路径。该方法会根据repo_path创建一个git.Repo对象，并将其赋值给self.repo属性。\n\n- `get_staged_pys(self)`: 获取已经暂存的Python文件。该方法会遍历暂存区中的文件变更，筛选出Python文件，并返回一个字典，其中键是文件路径，值是一个布尔值，表示该文件是否是新创建的。\n\n- `get_file_diff(self, file_path, is_new_file)`: 获取指定文件的变更内容。参数file_path是文件的相对路径，is_new_file是一个布尔值，表示该文件是否是新创建的。该方法会根据is_new_file的值，使用不同的方式获取文件的变更内容，并返回一个列表，列表中的每个元素表示文件的一行变更内容。\n\n- `parse_diffs(self, diffs)`: 解析变更内容，提取添加和删除的对象信息。该方法会遍历变更内容的每一行，根据行的前缀判断该行是添加还是删除的内容，并将其添加到相应的列表中。最后，返回一个字典，其中键是变更类型（添加或删除），值是一个列表，列表中的每个元素表示变更的一行内容。\n\n- `identify_changes_in_structure(self, changed_lines, structures)`: 识别发生变更的函数或类的结构。该方法会遍历所有变更的行，对于每一行，它会检查该行是否在某个结构（函数或类）的起始行和结束行之间。如果是，则认为该结构发生了变更，并将其名称和父结构名称添加到结果字典中。\n\n- `get_to_be_staged_files(self)`: 获取所有满足条件的未暂存文件。该方法会获取所有满足以下条件的未暂存文件：\n    1. 将文件的扩展名更改为.md后，与已经暂存的文件对应。\n    2. 文件的路径与CONFIG中的'project_hierarchy'字段相同。\n    返回一个列表，列表中的每个元素是满足条件的文件的路径。\n\n- `add_unstaged_files(self)`: 将满足条件的未暂存文件添加到暂存区。该方法会获取满足条件的未暂存文件，并使用git命令将其添加到暂存区。\n\n**注意**: \n- 该类依赖于GitPython库和git命令行工具。\n- 在使用get_file_diff方法之前，需要确保文件已经被添加到暂存区。\n- 在使用add_unstaged_files方法之前，需要确保GitPython库和git命令行工具已经安装并配置正确。\n\n**输出示例**:\n{\n    'added': {\n        ('PipelineAutoMatNode', None),\n        ('to_json_new', 'PipelineAutoMatNode')\n    },\n    'removed': set()\n}"
      ],
      "code_start_line": 12,
      "code_end_line": 229,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChangeDetector:\n    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n\n\n    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n\n    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n    \n    \n    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.\n    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.\n    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.\n    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n    \n    # TODO:可能有错，需要单元测试覆盖； 可能有更好的实现方式\n    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n\n    \n    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化一个ChangeDetector对象。\n**参数**: \n- repo_path (str): 代码库的路径。\n**代码描述**: \n该函数接受一个repo_path参数，并将其赋值给self.repo_path属性。然后，它使用git.Repo(repo_path)创建一个git.Repo对象，并将其赋值给self.repo属性。\n**注意**: \n请注意，使用该代码需要安装git和gitpython库。\n**返回值**: \nNone"
      ],
      "code_start_line": 13,
      "code_end_line": 24,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "repo_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/change_detector.py/ChangeDetector/get_staged_pys",
        "repo_agent/change_detector.py/ChangeDetector/get_file_diff",
        "repo_agent/change_detector.py/ChangeDetector/parse_diffs",
        "repo_agent/change_detector.py/ChangeDetector/identify_changes_in_structure",
        "repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files"
      ]
    },
    "get_staged_pys": {
      "type": "FunctionDef",
      "name": "get_staged_pys",
      "md_content": [
        "**get_staged_pys**: get_staged_pys函数的功能是获取已暂存的代码库中的新增Python文件。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: 该函数通过Git获取已暂存的代码库中的新增Python文件。它首先使用GitPython库的`repo.index.diff(\"HEAD\", R=True)`方法来比较暂存区（index）与最近一次提交（HEAD）之间的差异。通过将R参数设置为True，可以正确地将最近一次提交（HEAD）作为旧状态与当前暂存区（新状态）进行比较。这样，如果暂存区中有新增文件，它将正确地显示为已添加，因为它在最近一次提交中不存在。然后，通过遍历差异列表，筛选出变更类型为\"A\"（新增）或\"M\"（修改）且文件后缀为\".py\"的文件，并将其添加到一个字典中。字典的键为文件路径，值为布尔值，表示该文件是否为新创建的文件。\n\n**注意**: 请注意，GitPython库中的逻辑与git命令行工具的逻辑不同。在GitPython库中，`repo.index.diff('HEAD')`方法将暂存区（index）作为新状态与原始的最近一次提交（HEAD）作为旧状态进行比较。这意味着，如果当前暂存区中有新增文件，它将显示为在最近一次提交（HEAD）中不存在，即\"deleted\"。通过将R参数设置为True，可以翻转这种逻辑，将最近一次提交（HEAD）正确地作为旧状态，并将其与当前暂存区（新状态）进行比较。这样，如果暂存区中有新增文件，它将正确地显示为已添加，因为它在最近一次提交中不存在。\n\n**输出示例**: \n```python\n{\n    \"path/to/file1.py\": True,\n    \"path/to/file2.py\": False,\n    \"path/to/file3.py\": True\n}\n```\n上述示例表示在已暂存的代码库中，有三个新增的Python文件，其中\"path/to/file1.py\"和\"path/to/file3.py\"是新创建的文件，而\"path/to/file2.py\"是已存在的文件。"
      ],
      "code_start_line": 26,
      "code_end_line": 50,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__"
      ],
      "reference_who": []
    },
    "get_file_diff": {
      "type": "FunctionDef",
      "name": "get_file_diff",
      "md_content": [
        "**get_file_diff**: get_file_diff函数的功能是检索特定文件的更改。对于新文件，它使用git diff --staged命令获取差异。\n**参数**：该函数的参数。\n- file_path (str): 文件的相对路径。\n- is_new_file (bool): 表示文件是否为新文件。\n**代码说明**：该函数首先获取repo对象，然后根据is_new_file参数的值执行不同的操作。如果is_new_file为True，则将文件添加到暂存区，并使用git diff --staged命令获取差异。如果is_new_file为False，则使用git diff命令获取文件与HEAD的差异。\n**注意**：使用该代码需要注意以下几点：\n- 需要安装git和gitpython库。\n- file_path参数应为文件的相对路径。\n**输出示例**：假设文件的差异为[\"+ line 1\", \"- line 2\"]，则返回值为[\"+ line 1\", \"- line 2\"]。"
      ],
      "code_start_line": 53,
      "code_end_line": 75,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "file_path",
        "is_new_file"
      ],
      "have_return": true,
      "code_content": "    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__"
      ],
      "reference_who": []
    },
    "parse_diffs": {
      "type": "FunctionDef",
      "name": "parse_diffs",
      "md_content": [
        "**parse_diffs**: parse_diffs函数的功能是解析差异内容，提取添加和删除的对象信息，这些对象可以是类或函数。\n**参数**: 这个函数的参数。\n- diffs (list): 包含差异内容的列表。通过类内的get_file_diff()函数获取。\n\n**代码描述**: 这个函数的描述。\nparse_diffs函数用于解析差异内容，提取添加和删除的对象信息。它接受一个包含差异内容的列表作为参数，并返回一个字典，其中包含添加和删除的行信息。\n\n函数首先初始化了一个字典changed_lines，用于存储添加和删除的行信息。然后通过遍历差异内容列表，逐行解析差异内容。\n\n在遍历过程中，首先检测行号信息，通过正则表达式匹配获取当前行和变化行的行号。如果匹配成功，则更新当前行号和变化行号，并继续下一次循环。\n\n接下来，判断当前行是否以\"+\"开头且不以\"+++\"开头，如果是，则将该行添加到changed_lines字典的\"added\"键对应的列表中，并将变化行号加1。\n\n如果当前行以\"-\"开头且不以\"---\"开头，那么将该行添加到changed_lines字典的\"removed\"键对应的列表中，并将当前行号加1。\n\n对于没有变化的行，将同时增加当前行号和变化行号。\n\n最后，函数返回changed_lines字典，其中包含添加和删除的行信息。\n\n**注意**: 使用该代码时需要注意以下几点：\n- 该函数依赖于传入的差异内容列表，需要确保传入正确的差异内容。\n- 函数返回的changed_lines字典中，\"added\"键对应的列表包含了添加的行信息，\"removed\"键对应的列表包含了删除的行信息。\n\n**输出示例**: 一个可能的返回值示例。\n{'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}"
      ],
      "code_start_line": 77,
      "code_end_line": 115,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "diffs"
      ],
      "have_return": true,
      "code_content": "    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__"
      ],
      "reference_who": []
    },
    "identify_changes_in_structure": {
      "type": "FunctionDef",
      "name": "identify_changes_in_structure",
      "md_content": [
        "**identify_changes_in_structure**: identify_changes_in_structure函数的功能是识别发生更改的结构：遍历所有更改的行，对于每一行，它检查该行是否在一个结构（函数或类）的起始行和结束行之间。如果是，则认为该结构发生了更改，并将其名称和父结构的名称添加到结果字典changes_in_structures的相应集合中（取决于该行是添加还是删除）。\n\n**参数**：changed_lines（dict）：一个包含发生更改的行号的字典，{'added': [(行号，更改内容)], 'removed': [(行号，更改内容)]}；structures（list）：从get_functions_and_classes接收到的函数或类结构的列表，每个结构由结构类型、名称、起始行号、结束行号和父结构名称组成。\n\n**代码描述**：该函数通过遍历changed_lines中的行号，然后遍历structures中的每个结构，检查行号是否在结构的起始行和结束行之间。如果是，则将该结构的名称和父结构的名称添加到changes_in_structures字典的相应集合中。最后返回changes_in_structures字典，其中包含发生更改的结构，键是更改类型，值是结构名称和父结构名称的集合。\n\n**注意**：在使用该代码时需要注意以下几点：\n- 该函数依赖于get_functions_and_classes函数提供的结构信息，因此在调用该函数之前，需要确保已经调用了get_functions_and_classes函数并将其返回值作为参数传递给identify_changes_in_structure函数。\n- 该函数假设changed_lines和structures参数的格式是正确的，如果参数格式不正确，可能会导致函数无法正常工作。\n\n**输出示例**：{'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}"
      ],
      "code_start_line": 121,
      "code_end_line": 148,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "changed_lines",
        "structures"
      ],
      "have_return": true,
      "code_content": "    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__"
      ],
      "reference_who": []
    },
    "get_to_be_staged_files": {
      "type": "FunctionDef",
      "name": "get_to_be_staged_files",
      "md_content": [
        "**get_to_be_staged_files**: get_to_be_staged_files函数的功能是检索仓库中所有未暂存的文件，这些文件满足以下条件之一：\n1. 将文件的扩展名更改为.md后，与已暂存的文件对应。\n2. 文件的路径与CONFIG中的'project_hierarchy'字段相同。\n\n它返回这些文件的路径列表。\n\n**参数**: 无参数。\n\n**代码描述**: 该函数首先获取已暂存的文件列表，然后获取未暂存的文件列表和项目层次结构。接下来，它遍历未暂存的文件列表和已暂存的文件列表，根据条件判断是否将文件加入到待暂存文件列表中。最后，返回待暂存文件列表。\n\n**注意**: \n- 该函数依赖于CONFIG中的'project_hierarchy'字段和'Markdown_Docs_folder'字段。\n- 函数中的路径处理使用了os.path模块的相关方法。\n\n**输出示例**: \n```\n['repo_agent/change_detector.py', 'repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files']\n```"
      ],
      "code_start_line": 151,
      "code_end_line": 218,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/__init__"
      ],
      "reference_who": []
    },
    "add_unstaged_files": {
      "type": "FunctionDef",
      "name": "add_unstaged_files",
      "md_content": [
        "**add_unstaged_files**: add_unstaged_files函数的功能是将满足条件的未暂存文件添加到暂存区。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数首先调用get_to_be_staged_files()函数获取满足条件的未暂存文件列表，然后使用git命令将这些文件添加到暂存区。最后，函数返回满足条件的未暂存文件列表。\n**注意**: 使用该代码时需要确保已经安装了git，并且当前工作目录是git仓库的根目录。\n**输出示例**: 返回满足条件的未暂存文件列表的示例。\n\n该函数的主要功能是将满足条件的未暂存文件添加到git仓库的暂存区。在版本控制系统中，暂存区是用来存放待提交的修改的区域。通过将文件添加到暂存区，可以将这些文件纳入下一次提交的范围内。\n\n在函数内部，首先调用了self.get_to_be_staged_files()函数，该函数用于获取满足条件的未暂存文件列表。然后，使用subprocess模块执行git命令将这些文件添加到暂存区。具体而言，通过构建add_command字符串，使用subprocess.run()函数执行该命令。最后，函数返回满足条件的未暂存文件列表。\n\n需要注意的是，在使用该代码之前，需要确保已经安装了git，并且当前工作目录是git仓库的根目录。否则，可能会导致命令执行失败。\n\n以下是该函数返回值的示例：\n```\n['file1.py', 'file2.py', 'file3.py']\n```"
      ],
      "code_start_line": 221,
      "code_end_line": 229,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "repo_agent/project_manager.py": {
    "ProjectManager": {
      "type": "ClassDef",
      "name": "ProjectManager",
      "md_content": [
        "**ProjectManager**: ProjectManager的功能是管理项目的类。\n\n**attributes**: 这个类的属性有：\n- repo_path: 项目的存储路径\n- project: 项目对象\n- project_hierarchy: 项目层次结构的路径\n\n**Code Description**: 这个类用于管理项目。在初始化时，需要传入项目的存储路径和项目层次结构的路径。初始化时，会创建一个项目对象，并将项目层次结构的路径保存在project_hierarchy属性中。\n\nget_project_structure方法用于获取项目的结构。它会遍历项目路径下的所有文件和文件夹，并将它们的名称保存在一个列表中。最后，将列表中的元素用换行符连接起来，并返回结果。\n\nfind_all_referencer方法用于查找给定文件中变量的所有引用。它接受变量名、文件路径、行号和列号作为参数。首先，它使用jedi库创建一个Script对象，并传入文件路径。然后，使用Script对象的get_references方法获取变量的所有引用。接下来，它会过滤掉引用中不包含变量名的部分，并返回每个引用的文件路径、行号和列号。\n\n**Note**: \n- 在使用find_all_referencer方法时，需要确保传入的文件路径是相对于项目存储路径的相对路径。\n- 如果在find_all_referencer方法中发生错误，会打印错误消息和相关参数，并返回一个空列表作为结果。\n\n**Output Example**: \n```\n[\n    ('project_manager.py', 10, 5),\n    ('project_manager.py', 15, 10),\n    ('utils.py', 20, 15)\n]\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 52,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ProjectManager:\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n\n    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n    \n    def find_all_referencer(self, variable_name, file_path, line_number, column_number):\n        \"\"\"\n        Find all references of a variable in a given file.\n\n        Args:\n            variable_name (str): The name of the variable to search for.\n            file_path (str): The path of the file to search in.\n            line_number (int): The line number where the variable is located.\n            column_number (int): The column number where the variable is located.\n\n        Returns:\n            list: A list of tuples containing the file path, line number, and column number of each reference.\n        \n        \"\"\"\n        script = jedi.Script(path=os.path.join(self.repo_path, file_path))\n        references = script.get_references(line=line_number, column=column_number)\n\n        try:\n            # Filter out references with variable_name and return their positions\n            variable_references = [ref for ref in references if ref.name == variable_name]\n            return [(os.path.relpath(ref.module_path, self.repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n        except Exception as e:\n            # Print error message and related parameters\n            print(f\"Error occurred: {e}\")\n            print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n            return []\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化ProjectManager对象。\n**参数**: \n- repo_path: 项目的根目录路径。\n- project_hierarchy: 项目层级结构的路径。\n**代码描述**: \n__init__函数接受两个参数，repo_path和project_hierarchy，分别表示项目的根目录路径和项目层级结构的路径。\n在函数内部，首先将传入的repo_path赋值给self.repo_path，以便后续使用。\n然后，通过调用jedi.Project(self.repo_path)创建一个jedi.Project对象，并将其赋值给self.project，以便后续使用。jedi.Project是一个用于分析Python代码的工具，可以用于获取代码的结构和提供代码补全功能。\n接下来，将传入的repo_path、project_hierarchy和\".project_hierarchy.json\"拼接起来，得到项目层级结构文件的完整路径，并将其赋值给self.project_hierarchy，以便后续使用。\n**注意**: \n- 在使用该函数之前，需要确保os、jedi模块已经安装并导入。\n**示例代码**:\n```python\npm = ProjectManager('/path/to/repo', 'project_hierarchy')\n```\n**注意**: \n- 该函数在创建ProjectManager对象时会自动调用，无需手动调用。\n- 传入的repo_path和project_hierarchy参数应该是有效的路径。\n- 通过self.repo_path可以获取项目的根目录路径。\n- 通过self.project可以获取jedi.Project对象，用于分析项目的代码。\n- 通过self.project_hierarchy可以获取项目层级结构文件的路径。"
      ],
      "code_start_line": 5,
      "code_end_line": 8,
      "parent": "ProjectManager",
      "params": [
        "self",
        "repo_path",
        "project_hierarchy"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/project_manager.py/ProjectManager/get_project_structure",
        "repo_agent/project_manager.py/ProjectManager/get_project_structure/walk_dir"
      ]
    },
    "get_project_structure": {
      "type": "FunctionDef",
      "name": "get_project_structure",
      "md_content": [
        "**get_project_structure**: get_project_structure函数的功能是获取项目的结构。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数通过遍历项目目录，获取项目的结构信息，并将结果以字符串形式返回。\n首先，函数内部定义了一个名为walk_dir的嵌套函数，用于遍历目录并获取项目结构信息。walk_dir函数接受两个参数，root表示当前遍历的根目录，prefix表示当前目录的前缀，用于标识目录的层级关系。\n在walk_dir函数内部，首先将当前目录的名称添加到结构列表structure中，通过os.path.basename(root)获取目录的名称，并添加到structure列表中。然后，根据当前目录的名称更新前缀new_prefix，用于标识下一级目录的层级关系。\n接下来，通过sorted函数对当前目录下的文件和目录进行排序，并遍历每一个文件和目录。如果遍历到的名称以'.'开头，表示该文件或目录是隐藏文件或目录，忽略不处理。如果遍历到的是目录，递归调用walk_dir函数，传入该目录的路径和新的前缀new_prefix，继续遍历该目录下的文件和目录。如果遍历到的是文件，并且文件名以'.py'结尾，将文件名添加到结构列表structure中，通过new_prefix + name的形式添加到structure列表中。\n最后，定义一个空列表structure，用于存储项目的结构信息。调用walk_dir函数，传入self.repo_path作为根目录，开始遍历项目目录并获取结构信息。最后，通过'\\n'.join(structure)将结构列表转换为字符串，并作为函数的返回值返回。\n**注意**: 该函数依赖于os和jedi模块，需要确保这两个模块已经安装并导入。\n**输出示例**: 假设项目目录结构如下：\n- project\n  - module1\n    - file1.py\n    - file2.py\n  - module2\n    - file3.py\n  - file4.py\n调用get_project_structure函数后，返回的结果为：\n- project\n  - module1\n    - file1.py\n    - file2.py\n  - module2\n    - file3.py\n  - file4.py"
      ],
      "code_start_line": 10,
      "code_end_line": 25,
      "parent": "ProjectManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/project_manager.py/ProjectManager/__init__"
      ],
      "reference_who": []
    },
    "walk_dir": {
      "type": "FunctionDef",
      "name": "walk_dir",
      "md_content": [
        "**walk_dir**: walk_dir函数的功能是遍历指定目录下的所有文件和子目录，并将它们的结构保存到一个列表中。\n**parameters**: walk_dir函数有两个参数：\n- root：要遍历的根目录的路径。\n- prefix：文件和目录的前缀，用于标识它们在目录结构中的层级关系。\n**Code Description**: walk_dir函数的代码逻辑如下：\n1. 将根目录的基本名称添加到结构列表中，前缀为空。\n2. 创建一个新的前缀，将原前缀加上两个空格，用于标识子目录和文件的层级关系。\n3. 遍历根目录下的所有文件和子目录，按名称进行排序。\n4. 如果名称以'.'开头，表示是隐藏文件或目录，忽略它们。\n5. 构建文件或目录的完整路径。\n6. 如果是目录，递归调用walk_dir函数，传入子目录的路径和新的前缀。\n7. 如果是文件且以'.py'结尾，将文件名添加到结构列表中，前缀为新的前缀。\n**Note**: 使用该代码时需要注意以下几点：\n- 确保传入的根目录路径存在且有效。\n- 该函数会遍历根目录下的所有文件和子目录，可能会消耗较长的时间和资源，特别是在大型项目中使用时。请谨慎使用。\n- 结构列表中保存的是文件和目录的相对路径，可以根据需要进行进一步处理和使用。"
      ],
      "code_start_line": 11,
      "code_end_line": 21,
      "parent": "get_project_structure",
      "params": [
        "root",
        "prefix"
      ],
      "have_return": false,
      "code_content": "        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/project_manager.py/ProjectManager/__init__"
      ],
      "reference_who": []
    },
    "find_all_referencer": {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [
        "**find_all_referencer**: find_all_referencer函数的功能是在给定的文件中查找变量的所有引用。\n**parameters**: 这个函数的参数有：\n- variable_name (str): 要搜索的变量的名称。\n- file_path (str): 要搜索的文件的路径。\n- line_number (int): 变量所在的行号。\n- column_number (int): 变量所在的列号。\n**Code Description**: 这个函数的作用是在给定的文件中查找变量的所有引用。它使用jedi库来解析代码，并通过指定的行号和列号来获取变量的引用。然后，它过滤出与变量名称匹配的引用，并返回每个引用的文件路径、行号和列号。\n**Note**: 使用这段代码时需要注意以下几点：\n- 需要安装jedi库。\n- 需要传入正确的变量名称、文件路径、行号和列号。\n**Output Example**: 以下是这段代码返回值的一个示例：\n[('path/to/file.py', 10, 5), ('path/to/file.py', 15, 3), ('path/to/file.py', 20, 8)]"
      ],
      "code_start_line": 27,
      "code_end_line": 52,
      "parent": "ProjectManager",
      "params": [
        "self",
        "variable_name",
        "file_path",
        "line_number",
        "column_number"
      ],
      "have_return": true,
      "code_content": "    def find_all_referencer(self, variable_name, file_path, line_number, column_number):\n        \"\"\"\n        Find all references of a variable in a given file.\n\n        Args:\n            variable_name (str): The name of the variable to search for.\n            file_path (str): The path of the file to search in.\n            line_number (int): The line number where the variable is located.\n            column_number (int): The column number where the variable is located.\n\n        Returns:\n            list: A list of tuples containing the file path, line number, and column number of each reference.\n        \n        \"\"\"\n        script = jedi.Script(path=os.path.join(self.repo_path, file_path))\n        references = script.get_references(line=line_number, column=column_number)\n\n        try:\n            # Filter out references with variable_name and return their positions\n            variable_references = [ref for ref in references if ref.name == variable_name]\n            return [(os.path.relpath(ref.module_path, self.repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n        except Exception as e:\n            # Print error message and related parameters\n            print(f\"Error occurred: {e}\")\n            print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n            return []\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "repo_agent/utils/gitignore_checker.py": {
    "GitignoreChecker": {
      "type": "ClassDef",
      "name": "GitignoreChecker",
      "md_content": [
        "**GitignoreChecker**: GitignoreChecker的功能是检查给定目录下的文件和文件夹是否被.gitignore文件忽略，并返回未被忽略且具有'.py'扩展名的文件路径列表。\n\n**attributes**: \n- directory (str): 要检查的目录路径。\n- gitignore_path (str): .gitignore文件的路径。\n- folder_patterns (list): 从.gitignore文件中提取的文件夹模式列表。\n- file_patterns (list): 从.gitignore文件中提取的文件模式列表。\n\n**Code Description**: \nGitignoreChecker类用于检查给定目录下的文件和文件夹是否被.gitignore文件忽略。在初始化时，需要指定要检查的目录路径和.gitignore文件的路径。如果指定的.gitignore文件不存在，则会回退到默认路径。类中定义了私有方法_load_gitignore_patterns()，用于加载和解析.gitignore文件，并将模式分为文件夹模式和文件模式。还定义了静态方法_parse_gitignore()，用于解析.gitignore文件的内容并返回模式列表。静态方法_split_gitignore_patterns()用于将.gitignore模式分为文件夹模式和文件模式。私有方法_is_ignored()用于检查给定路径是否与模式匹配。check_files_and_folders()方法用于检查给定目录下的所有文件和文件夹，并根据分割的.gitignore模式返回未被忽略且具有'.py'扩展名的文件路径列表。\n\n**Note**: \n- 在初始化GitignoreChecker对象时，需要指定要检查的目录路径和.gitignore文件的路径。\n- 如果指定的.gitignore文件不存在，则会回退到默认路径。\n- check_files_and_folders()方法返回的文件路径是相对于self.directory的路径。\n\n**Output Example**: \n假设给定目录下有以下文件和文件夹：\n- 文件夹1/\n- 文件夹2/\n- 文件1.py\n- 文件2.py\n- 文件3.txt\n\n如果.gitignore文件的内容为：\n```\n文件夹1/\n*.txt\n```\n\n则check_files_and_folders()方法将返回以下文件路径列表：\n```\n['文件2.py']\n```"
      ],
      "code_start_line": 5,
      "code_end_line": 116,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class GitignoreChecker:\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是使用特定的目录和.gitignore文件的路径初始化GitignoreChecker。\n\n**参数**: \n- directory (str): 要检查的目录。\n- gitignore_path (str): .gitignore文件的路径。\n\n**代码描述**: \n该函数首先将传入的目录和.gitignore文件的路径保存到self.directory和self.gitignore_path属性中。然后，函数调用_load_gitignore_patterns函数加载和解析.gitignore文件的模式，并将结果保存到self.folder_patterns和self.file_patterns属性中。\n\n**注意**: \n在使用该代码时需要注意以下几点：\n- 确保.gitignore文件存在于指定的路径或默认路径中。\n- 确保.gitignore文件的编码为utf-8。\n\n**输出示例**: \n以下是该函数可能返回的结果示例：\n```python\n(['folder_pattern1', 'folder_pattern2'], ['file_pattern1', 'file_pattern2'])\n```"
      ],
      "code_start_line": 6,
      "code_end_line": 16,
      "parent": "GitignoreChecker",
      "params": [
        "self",
        "directory",
        "gitignore_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_load_gitignore_patterns",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_parse_gitignore",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_split_gitignore_patterns",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_is_ignored"
      ]
    },
    "_load_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_load_gitignore_patterns",
      "md_content": [
        "**_load_gitignore_patterns**: _load_gitignore_patterns函数的功能是加载和解析.gitignore文件，然后将模式分割为文件夹模式和文件模式。\n\n**参数**: 该函数没有接受任何参数。\n\n**代码描述**: 该函数首先尝试打开指定的.gitignore文件，并读取文件内容。如果文件不存在，则会回退到默认路径。然后，函数会调用_parse_gitignore函数解析.gitignore文件的内容，得到模式列表。最后，函数会调用_split_gitignore_patterns函数将模式列表分割为文件夹模式和文件模式，并将结果以元组的形式返回。\n\n**注意**: 在使用该代码时需要注意以下几点：\n- 确保.gitignore文件存在于指定的路径或默认路径中。\n- 确保.gitignore文件的编码为utf-8。\n\n**输出示例**: 以下是该函数可能返回的结果示例：\n```python\n(['folder_pattern1', 'folder_pattern2'], ['file_pattern1', 'file_pattern2'])\n```"
      ],
      "code_start_line": 18,
      "code_end_line": 37,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "_parse_gitignore": {
      "type": "FunctionDef",
      "name": "_parse_gitignore",
      "md_content": [
        "**_parse_gitignore**: _parse_gitignore函数的功能是解析.gitignore文件的内容，并将模式作为列表返回。\n**参数**: 这个函数的参数是gitignore_content，表示.gitignore文件的内容，类型为str。\n**代码描述**: 这个函数通过遍历gitignore_content的每一行，将不以'#'开头且不为空的行添加到patterns列表中，并返回该列表。\n**注意**: 使用这段代码时需要注意以下几点：\n- gitignore_content参数必须是一个字符串。\n- 返回值是一个列表，其中包含从.gitignore内容中提取出的模式。\n**输出示例**: 假设.gitignore文件的内容为：\n```\n# This is a comment\n*.txt\n!important.txt\n```\n则函数的返回值为：['*.txt', '!important.txt']"
      ],
      "code_start_line": 40,
      "code_end_line": 55,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_content"
      ],
      "have_return": true,
      "code_content": "    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "_split_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_split_gitignore_patterns",
      "md_content": [
        "**_split_gitignore_patterns**: _split_gitignore_patterns函数的功能是将.gitignore模式分割为文件夹模式和文件模式。\n**参数**: 这个函数的参数。\n- gitignore_patterns (list): 一个包含.gitignore文件中模式的列表。\n**代码说明**: 这个函数的功能是将.gitignore模式分割为文件夹模式和文件模式。它接受一个包含.gitignore模式的列表作为输入，并返回两个列表，一个用于文件夹模式，一个用于文件模式。\n函数首先创建两个空列表，用于存储文件夹模式和文件模式。然后，它遍历输入的模式列表，并根据模式的结尾字符来判断是文件夹模式还是文件模式。如果模式以'/'结尾，则将其去除'/'后添加到文件夹模式列表中；否则，将其添加到文件模式列表中。最后，函数返回文件夹模式列表和文件模式列表。\n**注意**: 使用这段代码时需要注意以下几点：\n- 输入的gitignore_patterns参数必须是一个列表。\n- 返回值是一个包含两个列表的元组，第一个列表是文件夹模式，第二个列表是文件模式。\n**输出示例**: 假设输入的gitignore_patterns为['/folder/', 'file.txt']，则函数的返回值为(['folder'], ['file.txt'])。"
      ],
      "code_start_line": 58,
      "code_end_line": 75,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_patterns"
      ],
      "have_return": true,
      "code_content": "    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "_is_ignored": {
      "type": "FunctionDef",
      "name": "_is_ignored",
      "md_content": [
        "**_is_ignored**: _is_ignored函数的功能是检查给定的路径是否与任何模式匹配。\n**参数**: 这个函数的参数。\n- path (str): 要检查的路径。\n- patterns (list): 要检查的模式列表。\n- is_dir (bool): 如果路径是目录，则为True；否则为False。\n**代码说明**: 这个函数通过遍历模式列表，检查给定的路径是否与任何模式匹配。如果匹配到了任何模式，则返回True；否则返回False。\n- 首先，对于每个模式，使用fnmatch模块的fnmatch函数来检查给定的路径是否与模式匹配。\n- 如果is_dir为True，并且模式以'/'结尾，并且去掉结尾的'/'后的模式与给定的路径匹配，则返回True。\n- 如果没有匹配到任何模式，则返回False。\n**注意**: 使用该代码时需要注意以下几点：\n- patterns参数应该是一个包含模式字符串的列表。\n- path参数应该是一个字符串，表示要检查的路径。\n- is_dir参数是可选的，默认为False。如果要检查的路径是一个目录，则应将is_dir参数设置为True。\n**输出示例**: 模拟代码返回值的可能外观。\n- 对于给定的路径和模式列表，如果路径与任何模式匹配，则返回True；否则返回False。"
      ],
      "code_start_line": 78,
      "code_end_line": 95,
      "parent": "GitignoreChecker",
      "params": [
        "path",
        "patterns",
        "is_dir"
      ],
      "have_return": true,
      "code_content": "    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": []
    },
    "check_files_and_folders": {
      "type": "FunctionDef",
      "name": "check_files_and_folders",
      "md_content": [
        "**check_files_and_folders**: check_files_and_folders函数的功能是检查给定目录中的所有文件和文件夹是否与拆分的gitignore模式匹配。返回一个文件列表，这些文件既不被忽略，又具有'.py'扩展名。返回的文件路径是相对于self.directory的。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: 该函数使用os.walk遍历给定目录下的所有文件和文件夹。首先，通过调用_is_ignored函数，将与文件夹模式匹配的文件夹从dirs列表中移除。然后，对于每个文件，将其路径与self.directory的相对路径进行比较，并检查文件是否与文件模式匹配且具有'.py'扩展名。如果满足条件，将文件的相对路径添加到not_ignored_files列表中。\n\n**注意**: 使用该代码时需要注意以下几点：\n- 确保已经设置了self.directory的值，以指定要检查的目录。\n- 确保已经设置了self.folder_patterns和self.file_patterns的值，以指定要匹配的文件夹和文件模式。\n\n**输出示例**: \n```\n['folder1/file1.py', 'folder2/file2.py', 'file3.py']\n```"
      ],
      "code_start_line": 97,
      "code_end_line": 116,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "display/book_tools/generate_summary_from_book.py": {
    "create_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_readme_if_not_exist",
      "md_content": [
        "**create_readme_if_not_exist**: create_readme_if_not_exist函数的功能是在指定目录下创建README.md文件，如果该文件不存在的话。\n**参数**: 这个函数的参数是dire，表示指定的目录路径。\n**代码描述**: 这个函数首先使用os.path.join函数将dire和'README.md'拼接成一个路径字符串，表示README.md文件的路径。然后，它使用os.path.exists函数检查该路径是否存在，如果不存在则使用open函数创建一个名为readme_file的文件对象，并以写入模式打开README.md文件。接着，它使用os.path.basename函数获取dire的基本名称，并将该名称作为标题写入README.md文件中。\n**注意**: 使用这段代码时需要注意以下几点：\n- 需要提供正确的目录路径作为参数dire。\n- 如果指定目录下已经存在README.md文件，则不会再次创建该文件。\n- 创建README.md文件时，需要确保指定的目录路径是可写的。\n- README.md文件的内容将以Markdown格式写入文件中，以便于阅读和展示。"
      ],
      "code_start_line": 6,
      "code_end_line": 12,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "def create_readme_if_not_exist(dire):\n    readme_path = os.path.join(dire, 'README.md')\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, 'w') as readme_file:\n            dirname = os.path.basename(dire)\n            readme_file.write('# {}\\n'.format(dirname))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "display/book_tools/generate_summary_from_book.py/output_markdown",
        "display/book_tools/generate_summary_from_book.py/is_markdown_file",
        "display/book_tools/generate_summary_from_book.py/main"
      ]
    },
    "output_markdown": {
      "type": "FunctionDef",
      "name": "output_markdown",
      "md_content": [
        "**output_markdown**: output_markdown函数的功能是将目录结构生成为Markdown格式的文档。\n**参数**: 这个函数的参数有dire、base_dir、output_file和iter_depth。\n**代码描述**: 这个函数首先遍历指定目录下的所有文件和文件夹，如果是文件夹则调用create_readme_if_not_exist函数创建README.md文件。然后再次遍历目录，如果是文件夹则检查是否存在README.md文件，如果存在则在输出文件中创建一个指向该文件的Markdown链接。然后递归调用output_markdown函数处理嵌套的文件夹。如果是文件且是Markdown文件，则在输出文件中创建一个指向该文件的Markdown链接。\n**注意**: 使用这段代码时需要注意以下几点：\n- 需要提供正确的目录路径作为参数dire和base_dir。\n- 需要提供一个可写的输出文件对象作为参数output_file。\n- 可以选择性地提供一个整数值作为参数iter_depth，用于控制Markdown链接的缩进层级。"
      ],
      "code_start_line": 42,
      "code_end_line": 65,
      "parent": null,
      "params": [
        "dire",
        "base_dir",
        "output_file",
        "iter_depth"
      ],
      "have_return": false,
      "code_content": "def output_markdown(dire, base_dir, output_file, iter_depth=0):\n    for filename in os.listdir(dire):\n        print('add readme ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            create_readme_if_not_exist(file_or_path)\n\n    for filename in os.listdir(dire):\n        print('deal with ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            # Check if README.md exists in the directory\n            readme_path = os.path.join(file_or_path, 'README.md')\n            if os.path.exists(readme_path):\n                # If README.md exists, create a markdown link to it\n                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')\n                output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(filename, relative_path))\n            # Recursively call output_markdown for nested directories\n            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)\n        else:\n            if is_markdown_file(filename):\n                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:\n                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)\n                    output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(is_markdown_file(filename), relative_path))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist"
      ],
      "reference_who": []
    },
    "markdown_file_in_dir": {
      "type": "FunctionDef",
      "name": "markdown_file_in_dir",
      "md_content": [
        "**markdown_file_in_dir**: markdown_file_in_dir函数的功能是在给定的目录中查找是否存在markdown文件。\n**parameters**: 这个函数的参数是dire，表示要搜索的目录路径。\n**Code Description**: 这个函数使用os.walk()函数遍历dire目录及其子目录中的所有文件和文件夹。然后使用正则表达式搜索文件名，如果文件名以\".md\"或\".markdown\"结尾，则返回True。如果遍历完所有文件后仍未找到匹配的文件，则返回False。\n**Note**: 使用这个函数时，需要确保dire参数是一个有效的目录路径。\n**Output Example**: 假设在给定的目录中存在一个名为\"example.md\"的markdown文件，则函数的返回值为True。"
      ],
      "code_start_line": 69,
      "code_end_line": 74,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": true,
      "code_content": "def markdown_file_in_dir(dire):\n    for root, dirs, files in os.walk(dire):\n        for filename in files:\n            if re.search('.md$|.markdown$', filename):\n                return True\n    return False\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "is_markdown_file": {
      "type": "FunctionDef",
      "name": "is_markdown_file",
      "md_content": [
        "**is_markdown_file**: is_markdown_file函数的功能是判断给定的文件名是否是Markdown文件。\n**参数**: 这个函数的参数是文件名（filename）。\n**代码描述**: 这个函数首先使用正则表达式搜索文件名中是否包含\".md\"或\".markdown\"的后缀，如果没有找到匹配的结果，则返回False。如果找到匹配的结果，函数会根据匹配结果的长度来判断文件名的后缀是\".md\"还是\".markdown\"。如果后缀是\".md\"，则返回去掉后缀的文件名；如果后缀是\".markdown\"，则返回去掉后缀的文件名。\n**注意**: 使用这段代码时需要注意以下几点：\n- 函数只能判断文件名是否是Markdown文件，不能判断文件内容是否符合Markdown格式。\n- 文件名的后缀必须是\".md\"或\".markdown\"，且后缀必须是文件名的最后几个字符。\n**输出示例**: 假设文件名为\"example.md\"，则函数返回\"example\"；假设文件名为\"example.markdown\"，则函数返回\"example\"。"
      ],
      "code_start_line": 77,
      "code_end_line": 84,
      "parent": null,
      "params": [
        "filename"
      ],
      "have_return": true,
      "code_content": "def is_markdown_file(filename):\n    match = re.search('.md$|.markdown$', filename)\n    if not match:\n        return False\n    elif len(match.group()) is len('.md'):\n        return filename[:-3]\n    elif len(match.group()) is len('.markdown'):\n        return filename[:-9]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist"
      ],
      "reference_who": []
    },
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: main函数的功能是生成书籍的摘要文件。\n**参数**: 无参数。\n**代码描述**: main函数首先获取命令行参数中的书籍名称，然后根据书籍名称创建一个文件夹。接着检查文件夹是否存在，如果不存在则创建该文件夹。然后，main函数创建一个名为SUMMARY.md的文件，并写入摘要的标题。最后，调用output_markdown函数生成摘要内容并写入文件中。\n**注意**: 在使用该代码时需要确保命令行参数中传入了正确的书籍名称。\n**输出示例**: \n```\n# Summary\n\n[摘要内容]\n```\n\nmain函数是一个用于生成书籍摘要文件的函数。它首先通过sys.argv[1]获取命令行参数中的书籍名称。然后，它使用os.path.join函数将书籍名称与'./books'、'src'拼接成一个路径字符串，表示书籍文件夹的路径。接着，它使用os.path.exists函数检查该路径是否存在，如果不存在则使用os.makedirs函数创建该路径。然后，main函数使用os.path.join函数将书籍文件夹路径与'SUMMARY.md'拼接成一个路径字符串，表示摘要文件的路径。接下来，它使用open函数创建一个名为output的文件对象，并以写入模式打开摘要文件。然后，它使用output.write函数将摘要的标题写入文件中。最后，它调用output_markdown函数生成摘要的内容，并将内容写入文件中。\n\n在使用main函数时，需要确保命令行参数中传入了正确的书籍名称。否则，程序将无法正确创建摘要文件。摘要文件的内容将以Markdown格式写入文件中，以便于阅读和展示。"
      ],
      "code_start_line": 87,
      "code_end_line": 109,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    book_name = sys.argv[1]\n\n    # mkdir the book folder\n    dir_input = os.path.join('./books', book_name, 'src')\n\n    # check the dst_dir\n    if not os.path.exists(dir_input):\n        print(dir_input)\n        os.makedirs(dir_input)\n    # Ensure the directory exists or create it\n    if not os.path.exists(dir_input):\n        os.makedirs(dir_input)\n\n    # Then proceed to create the file\n    output_path = os.path.join(dir_input, 'SUMMARY.md')\n    output = open(output_path, 'w')\n    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')\n    output.write('# Summary\\n\\n')\n    output_markdown(dir_input, dir_input, output)\n\n    print('GitBook auto summary finished:) ')\n    return 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist"
      ],
      "reference_who": []
    }
  },
  "display/book_tools/generate_repoagent_books.py": {
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: main函数的功能是将指定的Markdown文档文件夹复制到指定的书籍文件夹中，并创建书籍的README.md文件。\n**parameters**: 无参数。\n**Code Description**: \nmain函数首先通过sys.argv获取命令行参数，分别为markdown_docs_folder、book_name和repo_path。然后，main函数根据book_name创建了目标书籍文件夹dst_dir，并将repo_path和markdown_docs_folder拼接为docs_dir。\n\n接下来，main函数检查目标书籍文件夹dst_dir是否存在，如果不存在则创建该文件夹，并打印出创建文件夹的信息。\n\n然后，main函数遍历markdown_docs_folder文件夹中的所有文件和子文件夹。对于每个文件或子文件夹，main函数分别构建源路径src_path和目标路径dst_path。\n\n如果src_path是一个文件夹，则使用shutil.copytree函数将该文件夹及其内容复制到dst_path，并打印出复制文件夹的信息。\n\n如果src_path是一个文件，则使用shutil.copy2函数将该文件复制到dst_path，并打印出复制文件的信息。\n\n接下来，main函数定义了一个名为create_book_readme_if_not_exist的内部函数，用于创建书籍的README.md文件。该函数首先构建了README.md文件的路径readme_path。然后，如果readme_path不存在，则使用open函数创建该文件，并写入书籍名称。\n\n最后，main函数调用create_book_readme_if_not_exist函数，传入目标书籍文件夹dst_dir作为参数，以创建书籍的README.md文件。\n\n**Note**: \n- main函数通过sys.argv获取命令行参数，确保在运行程序时传入正确的参数。\n- main函数使用os.makedirs函数创建目标书籍文件夹，确保目标文件夹不存在时能够正确创建。\n- main函数使用shutil.copytree函数和shutil.copy2函数分别复制文件夹和文件，确保能够正确复制文件和文件夹。\n- main函数使用open函数创建README.md文件，并写入书籍名称，确保能够正确创建书籍的README.md文件。"
      ],
      "code_start_line": 7,
      "code_end_line": 44,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    markdown_docs_folder = sys.argv[1]\n    book_name = sys.argv[2]\n    repo_path = sys.argv[3]\n\n    # mkdir the book folder\n    dst_dir = os.path.join('./books', book_name, 'src')\n    docs_dir = os.path.join(repo_path, markdown_docs_folder)\n\n    # check the dst_dir\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(\"mkdir %s\" % dst_dir)\n\n    # cp the Markdown_Docs_folder to dst_dir\n    for item in os.listdir(docs_dir):\n        src_path = os.path.join(docs_dir, item)\n        dst_path = os.path.join(dst_dir, item)\n\n        # check the src_path\n        if os.path.isdir(src_path):\n            # if the src_path is a folder, use shutil.copytree to copy\n            shutil.copytree(src_path, dst_path)\n            print(\"copytree %s to %s\" % (src_path, dst_path))\n        else:\n            # if the src_path is a file, use shutil.copy2 to copy\n            shutil.copy2(src_path, dst_path)\n            print(\"copy2 %s to %s\" % (src_path, dst_path))\n\n    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n\n    # create book README.md if not exist\n    create_book_readme_if_not_exist(dst_dir)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "create_book_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_book_readme_if_not_exist",
      "md_content": [
        "**create_book_readme_if_not_exist**: create_book_readme_if_not_exist函数的功能是检查指定目录下是否存在README.md文件，如果不存在则创建一个。\n\n**参数**: \n- dire: 指定目录的路径\n\n**代码说明**:\n该函数首先使用os模块的join方法将指定目录路径和文件名README.md拼接起来，得到README.md文件的完整路径readme_path。\n\n然后，通过调用os模块的exists方法判断readme_path路径是否存在。如果不存在，说明该目录下没有README.md文件，需要创建一个。\n\n接下来，使用open函数以写入模式打开readme_path路径对应的文件，并使用write方法写入一行文本，文本内容为\"# \"加上book_name变量的值。这样就创建了一个新的README.md文件，并写入了标题。\n\n**注意**: \n- 在调用该函数之前，需要确保os模块已经导入。\n- 在调用该函数时，需要传入一个有效的目录路径作为参数dire。"
      ],
      "code_start_line": 36,
      "code_end_line": 41,
      "parent": "main",
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  }
}