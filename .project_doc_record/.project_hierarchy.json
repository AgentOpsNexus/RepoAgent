{
  "setup.py": {},
  "tests/test_change_detector.py": {
    "TestChangeDetector": {
      "type": "ClassDef",
      "name": "TestChangeDetector",
      "md_content": [
        "**TestChangeDetector**: TestChangeDetector的功能是测试ChangeDetector类的各个方法。\n\n**属性**: 该类没有定义任何属性。\n\n**代码描述**: TestChangeDetector是一个继承自unittest.TestCase的测试类。它用于测试ChangeDetector类的各个方法。\n\n在setUpClass()方法中，首先定义了测试仓库的路径，并创建了该路径下的文件夹。然后，初始化了一个Git仓库，并配置了Git用户信息。接下来，创建了两个测试文件test_file.py和test_file.md，并使用Git操作将它们添加到暂存区，并提交了一个初始提交。\n\ntest_get_staged_pys()方法用于测试ChangeDetector类的get_staged_pys()方法。首先，创建一个新的Python文件并将其暂存。然后，实例化一个ChangeDetector对象，并调用get_staged_pys()方法获取暂存的Python文件列表。最后，断言新创建的文件在暂存文件列表中。\n\ntest_get_unstaged_mds()方法用于测试ChangeDetector类的get_to_be_staged_files()方法。首先，修改一个Markdown文件但不暂存。然后，实例化一个ChangeDetector对象，并调用get_to_be_staged_files()方法获取未暂存的Markdown文件列表。最后，断言修改的文件在未暂存文件列表中。\n\ntest_add_unstaged_mds()方法用于测试ChangeDetector类的add_unstaged_files()方法。首先，调用test_get_unstaged_mds()方法确保有一个未暂存的Markdown文件。然后，实例化一个ChangeDetector对象，并调用add_unstaged_files()方法将未暂存的文件添加到暂存区。最后，检查文件是否被暂存，并断言暂存操作后没有未暂存的Markdown文件。\n\n在tearDownClass()方法中，清理测试仓库，关闭Git仓库，并删除测试仓库文件夹。\n\n**注意**: 在使用TestChangeDetector类之前，需要确保已安装了unittest和GitPython库。在使用该类的方法之前，需要先调用setUpClass()方法进行初始化设置，并在使用完毕后调用tearDownClass()方法进行清理操作。"
      ],
      "code_start_line": 6,
      "code_end_line": 89,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class TestChangeDetector(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n\n    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n\n\n    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n\n\n    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n\n\n    @classmethod\n    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "setUpClass": {
      "type": "FunctionDef",
      "name": "setUpClass",
      "md_content": [
        "**setUpClass**: setUpClass函数的作用是在测试类的所有测试方法执行之前进行一次性的设置。\n\n**参数**: cls - 类方法的第一个参数，表示当前类。\n\n**代码描述**: \nsetUpClass函数用于设置测试类的环境和资源，以便在执行测试方法之前进行一次性的准备工作。具体的代码逻辑如下：\n\n1. 定义测试仓库的路径：通过os.path.join函数将当前文件所在目录与'test_repo'拼接起来，得到测试仓库的路径，并将其赋值给cls.test_repo_path变量。\n\n2. 创建测试仓库文件夹：如果测试仓库文件夹不存在，则使用os.makedirs函数创建它。\n\n3. 初始化Git仓库：使用Repo.init函数初始化测试仓库，将其赋值给cls.repo变量。\n\n4. 配置Git用户信息：使用cls.repo.git.config函数配置Git用户的email和name，分别设置为'ci@example.com'和'CI User'。\n\n5. 创建测试文件：使用open函数创建两个测试文件，分别是'test_file.py'和'test_file.md'，并在文件中写入相应的内容。\n\n6. 模拟Git操作：使用cls.repo.git.add函数将所有文件添加到Git暂存区，然后使用cls.repo.git.commit函数提交文件，提交信息为'Initial commit'。\n\n**注意**: \n- setUpClass函数是一个类方法，需要使用@classmethod装饰器进行修饰。\n- setUpClass函数在整个测试类中只会执行一次，用于进行一次性的设置，例如创建测试数据、初始化资源等。\n- setUpClass函数中的代码逻辑应该是幂等的，即多次执行不会产生副作用。\n- setUpClass函数的参数cls表示当前类，可以通过cls访问类的属性和方法。"
      ],
      "code_start_line": 8,
      "code_end_line": 32,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "test_get_staged_pys": {
      "type": "FunctionDef",
      "name": "test_get_staged_pys",
      "md_content": [
        "**test_get_staged_pys**: test_get_staged_pys函数的功能是获取已暂存的Python文件。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: test_get_staged_pys函数的代码主要实现了以下几个步骤：\n1. 创建一个新的Python文件并将其暂存。\n2. 使用ChangeDetector类检查暂存文件。\n3. 断言新文件在暂存文件列表中。\n4. 打印暂存的Python文件列表。\n\n首先，函数会创建一个新的Python文件，并将其路径存储在new_py_file变量中。然后，使用open函数以写入模式打开new_py_file，并写入一行代码\"print(\"New Python File\")\"。接下来，调用self.repo.git.add(new_py_file)将new_py_file文件添加到暂存区。\n\n然后，函数实例化ChangeDetector类的对象change_detector，并将self.test_repo_path作为参数传递给该对象。接着，调用change_detector.get_staged_pys()方法获取已暂存的Python文件列表，并将结果存储在staged_files变量中。\n\n接下来，函数使用断言语句self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])来断言新创建的文件new_test_file.py是否在暂存文件列表中。\n\n最后，函数使用print语句打印出暂存的Python文件列表。\n\n**注意**: 在使用test_get_staged_pys函数之前，需要确保已经初始化了Git仓库对象self.repo。"
      ],
      "code_start_line": 34,
      "code_end_line": 48,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/change_detector.py/ChangeDetector",
        "repo_agent/change_detector.py/ChangeDetector/get_staged_pys"
      ]
    },
    "test_get_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_get_unstaged_mds",
      "md_content": [
        "**test_get_unstaged_mds**: test_get_unstaged_mds函数的功能是获取未暂存的Markdown文件。\n\n**参数**: 无参数。\n\n**代码描述**: 该函数首先在测试仓库中创建一个未暂存的Markdown文件。然后，它使用ChangeDetector类创建一个ChangeDetector对象change_detector，并调用其get_to_be_staged_files方法获取未暂存的文件列表unstaged_files。接下来，函数使用断言语句self.assertIn判断修改的文件是否在未暂存文件列表中。最后，函数打印输出未暂存的Markdown文件列表。\n\n**注意**: \n- 函数中使用了os.path模块的join方法来拼接文件路径。\n- 函数中使用了open方法来打开文件，并使用write方法向文件中写入内容。\n- 函数中使用了ChangeDetector类的get_to_be_staged_files方法来获取未暂存的文件列表。\n- 函数中使用了self.assertIn方法来断言修改的文件是否在未暂存文件列表中。\n\n**输出示例**: 一个可能的输出示例：\n```\ntest_get_unstaged_mds: Unstaged Markdown files: ['test_file.md']\n```"
      ],
      "code_start_line": 51,
      "code_end_line": 64,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_change_detector.py/TestChangeDetector/test_add_unstaged_mds"
      ],
      "reference_who": [
        "repo_agent/change_detector.py/ChangeDetector",
        "repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files"
      ]
    },
    "test_add_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_add_unstaged_mds",
      "md_content": [
        "**test_add_unstaged_mds**: test_add_unstaged_mds函数的功能是执行一系列操作来测试add_unstaged_mds方法。\n\n**参数**: 无参数。\n\n**代码描述**: 该函数首先调用self.test_get_unstaged_mds()方法，该方法的功能是获取未暂存的Markdown文件。然后，函数使用ChangeDetector类创建一个ChangeDetector对象change_detector，并调用其add_unstaged_files方法将未暂存的文件添加到暂存区。接下来，函数调用change_detector的get_to_be_staged_files方法获取待暂存的文件列表unstaged_files_after_add。然后，函数使用断言语句self.assertEqual判断unstaged_files_after_add的长度是否为0，以确保暂存操作后没有未暂存的Markdown文件。最后，函数使用print语句打印输出未暂存的Markdown文件列表。\n\n**注意**: \n- 函数中使用了ChangeDetector类的get_to_be_staged_files方法来获取待暂存的文件列表。\n- 函数中使用了self.assertEqual方法来断言unstaged_files_after_add的长度是否为0。\n\n**输出示例**: 一个可能的输出示例：\n```\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: 0\n```"
      ],
      "code_start_line": 67,
      "code_end_line": 82,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "tests/test_change_detector.py/TestChangeDetector/test_get_unstaged_mds",
        "repo_agent/change_detector.py/ChangeDetector",
        "repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files",
        "repo_agent/change_detector.py/ChangeDetector/add_unstaged_files"
      ]
    },
    "tearDownClass": {
      "type": "FunctionDef",
      "name": "tearDownClass",
      "md_content": [
        "**tearDownClass**: tearDownClass函数的作用是清理测试类中的资源和环境。\n**参数**: cls - 类方法的第一个参数，表示当前类。\n**代码描述**: tearDownClass函数用于清理测试类中的资源和环境。在函数内部，首先通过`cls.repo.close()`关闭测试仓库，然后通过`os.system('rm -rf ' + cls.test_repo_path)`删除测试仓库的文件夹。\n**注意**: 在使用tearDownClass函数时，需要确保已经创建了测试仓库，并且在测试类中的其他测试方法执行完毕后调用该函数，以清理测试环境和资源。"
      ],
      "code_start_line": 86,
      "code_end_line": 89,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "tests/test_structure_tree.py": {
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [
        "**build_path_tree**: build_path_tree函数的功能是构建路径树。\n**parameters**: build_path_tree函数的参数有三个：\n- who_reference_me: 一个包含引用当前对象的路径列表的列表。\n- reference_who: 一个包含当前对象引用的路径列表的列表。\n- doc_item_path: 当前对象的路径。\n\n**Code Description**: build_path_tree函数首先定义了一个内部函数tree，用于创建一个默认字典的树结构。然后，创建了一个名为path_tree的树结构对象。\n\n接下来，通过遍历who_reference_me和reference_who两个列表，将路径分割成部分，并将每个部分作为键，将其添加到path_tree中的相应位置。这样，就构建了一个包含所有引用关系的路径树。\n\n然后，将doc_item_path也分割成部分，并在最后一个部分前面加上星号。然后，将该路径添加到path_tree中的相应位置。\n\n最后，定义了一个名为tree_to_string的内部函数，用于将树结构转换为字符串。通过递归遍历树结构，将每个键和对应的值添加到字符串中，并根据层级缩进。最后，返回转换后的字符串。\n\n**Note**: 使用该代码时需要注意以下几点：\n- 输入的路径列表应该是一个包含多个路径的列表，每个路径应该是一个字符串。\n- 输入的路径应该使用操作系统的路径分隔符进行分割。\n- 函数返回的是一个字符串，表示构建的路径树的结构。\n\n**Output Example**: \n```\ntests\n    test_structure_tree.py\n        build_path_tree\n            ✳️build_path_tree\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 31,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [
        "**tree**: tree函数的功能是返回一个defaultdict(tree)对象。\n**parameters**: 该函数没有参数。\n**Code Description**: tree函数使用了defaultdict和tree两个类。defaultdict是一个字典的子类，它重写了__missing__方法，当访问一个不存在的键时，会自动调用__missing__方法返回一个默认值。tree是一个递归的数据结构，它可以无限嵌套，每一层都是一个defaultdict(tree)对象。tree函数的作用就是返回一个空的defaultdict(tree)对象。\n**Note**: 使用tree函数可以方便地创建一个多层嵌套的字典结构，每一层都是一个defaultdict(tree)对象，可以用于构建树形结构的数据。\n**Output Example**: \n```\ndefaultdict(<function tree at 0x00000123456789>, {})\n```"
      ],
      "code_start_line": 5,
      "code_end_line": 6,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [
        "**tree_to_string**: tree_to_string函数的功能是将树结构转换为字符串。\n**参数**: 这个函数的参数。\n- tree: 一个树结构，以字典的形式表示。\n- indent: 可选参数，表示缩进的级别，默认为0。\n**代码描述**: 这个函数通过递归的方式遍历树结构，并将每个节点转换为字符串形式。它首先对树的键进行排序，然后根据缩进级别添加相应数量的空格。如果节点的值是一个字典，则递归调用tree_to_string函数，将子树转换为字符串，并添加到结果字符串中。最后，函数返回结果字符串。\n**注意**: 使用这段代码时需要注意以下几点：\n- tree参数必须是一个字典形式的树结构。\n- indent参数表示缩进的级别，可以根据需要进行调整。\n**输出示例**: 模拟代码返回值的可能外观。\n```\nroot\n    child1\n        grandchild1\n        grandchild2\n    child2\n        grandchild3\n```"
      ],
      "code_start_line": 23,
      "code_end_line": 29,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "repo_agent/runner.py": {
    "need_to_generate": {
      "type": "FunctionDef",
      "name": "need_to_generate",
      "md_content": [
        "**need_to_generate**: need_to_generate函数的功能是判断是否需要生成文档。\n\n**参数**: \n- doc_item: DocItem类型的对象，表示文档项。\n- ignore_list: List类型的参数，表示忽略列表。\n\n**代码描述**：\nneed_to_generate函数用于判断是否需要生成文档。首先，它检查文档项的状态是否为\"doc_up_to_date\"，如果是，则表示文档已经是最新的状态，无需生成文档，直接返回False。然后，获取文档项的完整路径，并判断文档项的类型是否为文件、目录或仓库，如果是，则表示不需要生成文档，直接返回False。接下来，将文档项的父对象赋值给doc_item，并进入循环。在循环中，判断当前文档项的类型是否为文件，如果是，则判断当前文件是否在忽略列表中或者在忽略列表某个文件路径下，如果是，则表示需要跳过该文件，直接返回False；否则，表示需要生成文档，返回True。如果当前文档项不是文件，则将当前文档项的父对象赋值给doc_item，继续循环。如果循环结束后仍未返回，则表示不需要生成文档，返回False。\n\n**注意**：\n- need_to_generate函数用于判断是否需要生成文档。\n- 需要根据文档项的状态、类型和忽略列表来判断是否需要生成文档。\n- 如果文档项的状态为\"doc_up_to_date\"，表示文档已经是最新的状态，无需生成文档。\n- 如果文档项的类型为文件、目录或仓库，表示不需要生成文档。\n- 如果当前文件在忽略列表中或者在忽略列表某个文件路径下，表示需要跳过该文件，不生成文档。\n\n**输出示例**：\n```\nTrue\n```"
      ],
      "code_start_line": 21,
      "code_end_line": 36,
      "parent": null,
      "params": [
        "doc_item",
        "ignore_list"
      ],
      "have_return": true,
      "code_content": "def need_to_generate(doc_item: DocItem, ignore_list: List) -> bool:\n    \"\"\"只生成item的，文件及更高粒度都跳过。另外如果属于一个blacklist的文件也跳过\"\"\"\n    if doc_item.item_status == DocItemStatus.doc_up_to_date:\n        return False\n    rel_file_path = doc_item.get_full_name()\n    if doc_item.item_type in [DocItemType._file, DocItemType._dir, DocItemType._repo]:\n        return False\n    doc_item = doc_item.father\n    while doc_item:\n        if doc_item.item_type == DocItemType._file:\n            # 如果当前文件在忽略列表中，或者在忽略列表某个文件路径下，则跳过\n            if any(rel_file_path.startswith(ignore_item) for ignore_item in ignore_list):\n                return False\n            return True\n        doc_item = doc_item.father\n    return False\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name"
      ]
    },
    "load_whitelist": {
      "type": "FunctionDef",
      "name": "load_whitelist",
      "md_content": [
        "**load_whitelist**: load_whitelist函数的功能是加载白名单数据。\n**参数**: 该函数没有参数。\n**代码描述**: load_whitelist函数首先判断CONFIG[\"whitelist_path\"]是否为None，如果不为None，则断言CONFIG[\"whitelist_path\"]对应的文件存在，否则抛出异常。然后使用\"r\"模式打开CONFIG[\"whitelist_path\"]对应的文件，并使用json.load()方法加载文件中的数据。最后将加载的白名单数据返回。如果CONFIG[\"whitelist_path\"]为None，则返回None。\n**注意**: 使用该代码时需要确保CONFIG[\"whitelist_path\"]对应的文件存在且为json格式。\n**输出示例**: 假设白名单数据为[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]，则函数返回的白名单数据为[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]。"
      ],
      "code_start_line": 38,
      "code_end_line": 47,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def load_whitelist():\n    if CONFIG[\"whitelist_path\"] != None:\n        assert os.path.exists(CONFIG[\"whitelist_path\"]), f\"whitelist_path must be a json-file,and must exists: {CONFIG['whitelist_path']}\"\n        with open(CONFIG[\"whitelist_path\"], \"r\") as reader:\n            white_list_json_data = json.load(reader)\n        # for i in range(len(white_list_json_data)):\n        #     white_list_json_data[i][\"file_path\"] = white_list_json_data[i][\"file_path\"].replace(\"https://github.com/huggingface/transformers/blob/v4.36.1/\",\"\")\n        return white_list_json_data\n    else:\n        return None\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/__init__"
      ],
      "reference_who": []
    },
    "Runner": {
      "type": "ClassDef",
      "name": "Runner",
      "md_content": [
        "**Runner**: Runner的功能是生成文档和更新文档。\n\n**属性**：Runner具有以下属性：\n- project_manager：一个ProjectManager对象，用于管理项目的路径和层级结构。\n- change_detector：一个ChangeDetector对象，用于检测代码变更。\n- chat_engine：一个ChatEngine对象，用于与聊天机器人交互。\n- meta_info：一个MetaInfo对象，用于存储文档信息和状态。\n- runner_lock：一个线程锁对象，用于保证多线程操作的安全性。\n\n**代码描述**：Runner类是用于生成和更新文档的主要类。在初始化时，Runner会创建一个ProjectManager对象、一个ChangeDetector对象和一个ChatEngine对象。然后，根据配置文件中的信息，Runner会判断是否需要生成文档。如果需要生成文档，Runner会调用generate_doc_for_a_single_item方法生成文档，并将生成的文档内容添加到相应的DocItem对象中。生成文档的过程是多线程的，可以提高效率。生成文档完成后，Runner会将文档信息写入到Markdown文件中。\n\nRunner还提供了其他方法，如get_all_pys用于获取指定目录下的所有Python文件，first_generate用于生成所有文档，markdown_refresh用于将文档信息写入到Markdown文件中，git_commit用于提交代码变更，run用于运行文档更新过程，add_new_item用于添加新的项目并生成相应的文档，process_file_changes用于处理文件变更。\n\n**注意**：在使用Runner类时，需要注意以下几点：\n- 在生成文档之前，需要配置好相关的路径和参数。\n- 生成文档的过程是多线程的，可以提高效率，但需要注意线程安全性。\n- 生成文档的过程需要绑定代码为一个版本，确保代码不会被修改。\n\n**输出示例**：以下是可能的代码返回值的示例：\n- 成功生成了 X 个文档\n- 文档已生成，跳过：XXX\n- 不存在文档内容，跳过：XXX\n- 正在生成 XXX 对象文档...\n- Doc has been forwarded to the latest version\n- markdown document has been refreshed at XXX"
      ],
      "code_start_line": 49,
      "code_end_line": 452,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class Runner:\n    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        self.runner_lock = threading.Lock()\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n    \n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        rel_file_path = doc_item.get_full_name()\n\n        ignore_list = CONFIG.get('ignore_list', [])\n        if not need_to_generate(doc_item, ignore_list):\n            logger.info(f\"内容被忽略/文档已生成，跳过：{doc_item.get_full_name()}\")\n        else:\n            logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...\")\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            response_message = self.chat_engine.generate_doc(\n                doc_item = doc_item,\n                file_handler = file_handler,\n            )\n            doc_item.md_content.append(response_message.content)\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        \n\n    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(check_task_available_func) #将按照此顺序生成文档\n        # topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"成功生成了 {before_task_len - len(task_manager.task_dict)} 个文档\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {before_task_len - len(task_manager.task_dict)} docs are generated at this time\")\n\n    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        with self.runner_lock:\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n                if recursive_check(file_item) == False:\n                    # logger.info(f\"不存在文档内容，跳过：{file_item.get_full_name()}\")\n                    continue\n                rel_file_path = file_item.get_full_name()\n                # file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n                    \n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n                # 写入markdown内容到.md文件\n                file_path = os.path.join(CONFIG['Markdown_Docs_folder'], file_item.get_file_name().replace('.py', '.md'))\n                if file_path.startswith('/'):\n                    # 移除开头的 '/'\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, 'w', encoding='utf-8') as file:\n                    file.write(markdown)\n\n            logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n\n    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n\n\n    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(self.meta_info.target_repo_hierarchical_tree,task_available_func=check_task_available_func)\n        self.meta_info.print_task_list([cont.extra_info for cont in task_manager.task_dict.values()])\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        \n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n        \n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\") # 提交变更\n         \n\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n    \n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n\n\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是XXX。\n**参数**: 这个函数的参数。\n**代码描述**: 这个函数的描述。\n(详细的代码分析和描述...)\n**注意**: 使用这段代码时需要注意的事项。\n\n请注意：\n- 生成的文档内容中不应包含Markdown的标题和分隔符语法。\n- 主要使用中文编写文档。如果有必要，可以在分析和描述中使用一些英文单词，以提高文档的可读性，因为不需要将函数名或变量名翻译成目标语言。"
      ],
      "code_start_line": 50,
      "code_end_line": 62,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        self.runner_lock = threading.Lock()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/runner.py/load_whitelist",
        "repo_agent/doc_meta_info.py/MetaInfo",
        "repo_agent/doc_meta_info.py/MetaInfo/init_from_project_path",
        "repo_agent/doc_meta_info.py/MetaInfo/from_checkpoint_path",
        "repo_agent/doc_meta_info.py/MetaInfo/checkpoint",
        "repo_agent/chat_engine.py/ChatEngine",
        "repo_agent/change_detector.py/ChangeDetector",
        "repo_agent/project_manager.py/ProjectManager"
      ]
    },
    "get_all_pys": {
      "type": "FunctionDef",
      "name": "get_all_pys",
      "md_content": [
        "**get_all_pys**: get_all_pys函数的功能是获取给定目录中的所有Python文件。\n**参数**: 这个函数的参数。\n- directory (str): 要搜索的目录。\n**代码描述**: 这个函数使用os.walk函数遍历给定目录及其子目录中的所有文件，并将以\".py\"结尾的文件路径添加到python_files列表中。\n**注意**: 使用这段代码时需要注意以下几点：\n- 确保传入的directory参数是一个有效的目录路径。\n- 确保目录中存在Python文件，否则返回的列表将为空。\n**输出示例**: 模拟代码返回值的可能外观。\n```python\n['/path/to/file1.py', '/path/to/file2.py', '/path/to/file3.py']\n```"
      ],
      "code_start_line": 64,
      "code_end_line": 81,
      "parent": "Runner",
      "params": [
        "self",
        "directory"
      ],
      "have_return": true,
      "code_content": "    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "generate_doc_for_a_single_item": {
      "type": "FunctionDef",
      "name": "generate_doc_for_a_single_item",
      "md_content": [
        "**generate_doc_for_a_single_item**: generate_doc_for_a_single_item函数的功能是为一个对象生成文档。\n\n**参数**: \n- doc_item: DocItem类型的对象，表示文档项。\n\n**代码描述**：\ngenerate_doc_for_a_single_item函数用于为一个对象生成文档。首先，获取传入的doc_item对象的相关信息，包括类型、名称、代码内容、是否有返回值等。然后，根据doc_item对象的引用关系和路径信息，构建项目的层次结构。接下来，根据语言设置，确定代码的语言类型。然后，根据引用关系和路径信息，生成引用了该函数的对象和该函数引用的其他对象的提示信息。之后，根据函数的相关信息和引用关系，构建系统提示信息和用户提示信息。最后，使用OpenAI的Chat API，将系统提示信息和用户提示信息传入模型，生成文档的内容。\n\n**注意**：\n- 生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n**输出示例**：\n假设当前函数的名称为generate_doc_for_a_single_item，传入的doc_item对象的类型为Function，名称为func，代码内容为\"def func():\\n    print('Hello, world!')\"，没有返回值，被引用了两次，分别是obj1和obj2。根据这些信息，生成的文档内容可能如下所示：\n```\ngenerate_doc_for_a_single_item函数的功能是为一个对象生成文档。\n\n参数：\n- doc_item: 一个DocItem对象，表示文档项。\n\n代码描述：这个函数用于为一个对象生成文档。首先，获取传入的doc_item对象的相关信息，包括类型、名称、代码内容、是否有返回值等。然后，根据doc_item对象的引用关系和路径信息，构建项目的层次结构。接下来，根据语言设置，确定代码的语言类型。然后，根据引用关系和路径信息，生成引用了该函数的对象和该函数引用的其他对象的提示信息。之后，根据函数的相关信息和引用关系，构建系统提示信息和用户提示信息。最后，使用OpenAI的Chat API，将系统提示信息和用户提示信息传入模型，生成文档的内容。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例：假设当前函数的名称为generate_doc_for_a_single_item，传入的doc_item对象的类型为Function，名称为func，代码内容为\"def func():\\n    print('Hello, world!')\"，没有返回值，被引用了两次，分别是obj1和obj2。根据这些信息，生成的文档内容可能如下所示：\n```\ngenerate_doc_for_a_single_item函数的功能是为一个对象生成文档。\n\n参数：\n- doc_item: 一个DocItem对象，表示文档项。\n\n代码描述：这个函数用于为一个对象生成文档。首先，获取传入的doc_item对象的相关信息，包括类型、名称、代码内容、是否有返回值等。然后，根据doc_item对象的引用关系和路径信息，构建项目的层次结构。接下来，根据语言设置，确定代码的语言类型。然后，根据引用关系和路径信息，生成引用了该函数的对象和该函数引用的其他对象的提示信息。之后，根据函数的相关信息和引用关系，构建系统提示信息和用户提示信息。最后，使用OpenAI的Chat API，将系统提示信息和用户提示信息传入模型，生成文档的内容。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例："
      ],
      "code_start_line": 84,
      "code_end_line": 101,
      "parent": "Runner",
      "params": [
        "self",
        "doc_item"
      ],
      "have_return": false,
      "code_content": "    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        rel_file_path = doc_item.get_full_name()\n\n        ignore_list = CONFIG.get('ignore_list', [])\n        if not need_to_generate(doc_item, ignore_list):\n            logger.info(f\"内容被忽略/文档已生成，跳过：{doc_item.get_full_name()}\")\n        else:\n            logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...\")\n            file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n            response_message = self.chat_engine.generate_doc(\n                doc_item = doc_item,\n                file_handler = file_handler,\n            )\n            doc_item.md_content.append(response_message.content)\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/runner.py/need_to_generate",
        "repo_agent/file_handler.py/FileHandler",
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/MetaInfo/checkpoint",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc"
      ]
    },
    "first_generate": {
      "type": "FunctionDef",
      "name": "first_generate",
      "md_content": [
        "**first_generate**: first_generate函数的功能是生成所有文档。\n\n**参数**: 无\n\n**代码描述**: first_generate函数用于生成所有文档。首先，记录日志信息，表示开始生成文档。然后，获取忽略列表。接下来，根据忽略列表判断是否需要生成文档。如果不需要生成文档，则直接返回。如果需要生成文档，则获取任务管理器，并记录生成文档前的任务数量。然后，设置生成文档的状态为True。接着，使用多线程的方式处理任务，每个线程调用worker函数处理任务。处理任务的过程中，会调用generate_doc_for_a_single_item函数为每个对象生成文档。任务处理完成后，记录生成文档的版本号，并将生成文档的状态设置为False。最后，调用checkpoint函数保存MetaInfo，并记录生成了多少个文档。\n\n**注意**:\n- first_generate函数用于生成所有文档。\n- 需要根据忽略列表判断是否需要生成文档。\n- 生成文档的过程中，会使用多线程处理任务。\n- 生成文档后，会保存MetaInfo，并记录生成了多少个文档。\n\n**输出示例**:\n```\n成功生成了 10 个文档\n```"
      ],
      "code_start_line": 104,
      "code_end_line": 135,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(check_task_available_func) #将按照此顺序生成文档\n        # topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"成功生成了 {before_task_len - len(task_manager.task_dict)} 个文档\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {before_task_len - len(task_manager.task_dict)} docs are generated at this time\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/runner.py/need_to_generate",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/markdown_refresh",
        "repo_agent/multi_task_dispatch.py/worker",
        "repo_agent/doc_meta_info.py/MetaInfo/checkpoint",
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology"
      ]
    },
    "markdown_refresh": {
      "type": "FunctionDef",
      "name": "markdown_refresh",
      "md_content": [
        "**markdown_refresh**: markdown_refresh函数的功能是将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)。\n\n**参数**: 无\n\n**代码描述**: markdown_refresh函数首先获取所有的file节点，然后遍历每个file节点。在遍历过程中，定义了一个递归函数recursive_check，用于检查一个file内是否存在doc。如果file内不存在doc，则跳过该file节点。接着，定义了一个递归函数to_markdown，用于将doc信息转换成markdown格式。在to_markdown函数中，首先根据doc的层级关系生成相应的标题，然后将doc的参数和内容添加到markdown内容中。最后，遍历file节点的所有子节点，对每个子节点调用to_markdown函数，将子节点的markdown内容添加到markdown中。最后，将markdown内容写入到.md文件中。\n\n**注意**: \n- markdown_refresh函数依赖于其他对象的函数，如get_all_files、get_full_name等。\n- markdown_refresh函数需要在目标repo的层级树结构已经构建完成的情况下调用。\n\n**输出示例**: \n假设目标repo的层级树中存在两个file节点，分别为file1和file2，其中file1下有一个obj节点obj1，file2下有一个obj节点obj2。假设file1下的obj1有一个参数param1，内容为\"这是一个示例参数\"，file2下的obj2没有参数，内容为\"这是另一个示例参数\"。那么调用markdown_refresh函数后，将生成两个.md文件，内容如下：\n\nfile1.md:\n```\n## file file1\n\n### obj obj1(param1)\n\n这是一个示例参数\n```\n\nfile2.md:\n```\n## file file2\n\n### obj obj2\n\n这是另一个示例参数\n```\n"
      ],
      "code_start_line": 137,
      "code_end_line": 180,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        with self.runner_lock:\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n                if recursive_check(file_item) == False:\n                    # logger.info(f\"不存在文档内容，跳过：{file_item.get_full_name()}\")\n                    continue\n                rel_file_path = file_item.get_full_name()\n                # file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n                    \n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n                # 写入markdown内容到.md文件\n                file_path = os.path.join(CONFIG['Markdown_Docs_folder'], file_item.get_file_name().replace('.py', '.md'))\n                if file_path.startswith('/'):\n                    # 移除开头的 '/'\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, 'w', encoding='utf-8') as file:\n                    file.write(markdown)\n\n            logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem/get_file_name",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files"
      ]
    },
    "recursive_check": {
      "type": "FunctionDef",
      "name": "recursive_check",
      "md_content": [
        "**recursive_check**: recursive_check函数的功能是检查一个file内是否存在doc。\n**参数**: \n- doc_item: DocItem类型的参数，表示要检查的文档项。\n**代码描述**: \n该函数首先判断doc_item的md_content属性是否为空列表，如果不为空，则返回True。然后遍历doc_item的children属性，对每个子对象递归调用recursive_check函数。如果递归调用返回True，则说明存在doc，直接返回True。如果遍历完所有子对象后仍未找到doc，则返回False。\n**注意**: \n- 该函数依赖于DocItem类的属性和方法，需要确保传入的doc_item参数是一个有效的DocItem对象。\n**输出示例**: \n假设传入的doc_item对象存在md_content，则返回True；否则，返回False。"
      ],
      "code_start_line": 143,
      "code_end_line": 149,
      "parent": "markdown_refresh",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n",
      "name_column": 20,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "to_markdown": {
      "type": "FunctionDef",
      "name": "to_markdown",
      "md_content": [
        "**to_markdown**: to_markdown函数的功能是将DocItem对象转换为Markdown格式的字符串。\n**parameters**: to_markdown函数的参数如下：\n- item: DocItem类型，表示要转换为Markdown的文档项对象。\n- now_level: int类型，表示当前的层级。\n\n**Code Description**: to_markdown函数接受一个DocItem对象和当前层级作为参数，将该对象及其子对象转换为Markdown格式的字符串。函数首先创建一个空的markdown_content字符串，然后根据当前层级添加相应数量的\"#\"作为标题的级别，并将文档项的类型和对象名添加到markdown_content中。如果文档项的content属性中存在\"params\"键且params列表不为空，则将params列表中的参数名添加到markdown_content中。接下来，函数将文档项的最后一个版本的md_content添加到markdown_content中，如果md_content列表为空，则添加\"Doc has not been generated...\"。然后，函数遍历文档项的子对象，递归调用to_markdown函数，并将子对象的转换结果添加到markdown_content中。最后，函数返回markdown_content字符串。\n\n**Note**: 使用to_markdown函数可以将DocItem对象及其子对象转换为Markdown格式的字符串，方便在文档中展示和分享。\n\n**Output Example**: \n```\n## _class_function to_markdown(DocItem, int)\nDoc has not been generated...\n```"
      ],
      "code_start_line": 155,
      "code_end_line": 164,
      "parent": "markdown_refresh",
      "params": [
        "item",
        "now_level"
      ],
      "have_return": true,
      "code_content": "                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n",
      "name_column": 20,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "git_commit": {
      "type": "FunctionDef",
      "name": "git_commit",
      "md_content": [
        "**git_commit**: git_commit函数的功能是将更改提交到Git仓库。\n**参数**: 这个函数的参数是commit_message，表示提交的消息。\n**代码描述**: 这个函数使用subprocess模块调用系统命令来执行Git提交操作。它接受一个commit_message参数作为提交的消息，并使用`git commit`命令将更改提交到Git仓库。如果提交过程中发生错误，会捕获subprocess.CalledProcessError异常，并打印出错误信息。\n**注意**: 在使用这段代码时需要注意以下几点：\n- 确保系统中已经安装了Git，并且Git的可执行文件路径已经添加到系统的环境变量中。\n- commit_message参数应该是一个字符串类型的变量，用于描述提交的内容。\n- 如果提交过程中发生错误，会打印出错误信息，开发者可以根据错误信息进行排查和修复。"
      ],
      "code_start_line": 182,
      "code_end_line": 186,
      "parent": "Runner",
      "params": [
        "self",
        "commit_message"
      ],
      "have_return": false,
      "code_content": "    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "run": {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [
        "**run**: run函数的功能是运行文档更新过程。\n\n**参数**：\n- 无\n\n**代码描述**：\nrun函数用于运行文档更新过程。该方法检测更改的Python文件，处理每个文件，并相应地更新文档。\n\n如果self.meta_info.document_version为空字符串，则表示仍在最初生成的过程中。在这种情况下，会调用self.first_generate()方法生成文档，并检查目标目录下的文件和引用关系。然后，返回。\n\n如果self.meta_info.in_generation_process为False，则表示不在文档生成过程中。在这种情况下，会记录日志信息，表示开始检测更改。\n\n接下来，采用新的方法来处理文档更新。首先，新建一个项目层次结构。然后，将新的项目层次结构与旧的层次结构进行合并，处理创建新文件、删除文件和对象以及引用关系变化的情况。\n\n合并后的new_meta_info中：\n1. 新建的文件没有文档，因此合并后仍然没有文档。\n2. 被删除的文件和对象本来就不在新的meta信息中，相当于文档被自动删除了。\n3. 只需要观察被修改的文件以及引用关系需要通知的文件来重新生成文档。\n\n然后，将new_meta_info赋值给self.meta_info，并将self.meta_info.in_generation_process设置为True。\n\n接下来，处理任务队列。根据配置文件中的忽略列表，创建一个任务可用性函数check_task_available_func。\n\n通过self.meta_info的get_task_manager方法获取任务管理器task_manager，并打印剩余待处理的任务列表。\n\n设置task_manager的同步函数为self.markdown_refresh，并创建多个线程来处理任务。每个线程调用worker函数处理任务，其中的任务处理函数为self.generate_doc_for_a_single_item。\n\n等待所有线程完成任务后，将self.meta_info.in_generation_process设置为False，并将self.meta_info.document_version设置为当前仓库的commit hash。\n\n最后，调用self.meta_info的checkpoint方法保存MetaInfo，并记录生成了多少个文档。\n\n**注意**：\n- run函数用于运行文档更新过程。\n- 需要根据self.meta_info.document_version的值来判断是否仍在最初生成的过程中。\n- 需要根据self.meta_info.in_generation_process的值来判断是否在文档生成过程中。\n- 需要根据任务队列和引用关系来生成文档。\n- 生成文档后，会保存MetaInfo，并记录生成了多少个文档。\n\n**输出示例**：\n```\nDoc has been forwarded to the latest version\n```"
      ],
      "code_start_line": 189,
      "code_end_line": 245,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(self.meta_info.target_repo_hierarchical_tree,task_available_func=check_task_available_func)\n        self.meta_info.print_task_list([cont.extra_info for cont in task_manager.task_dict.values()])\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/runner.py/need_to_generate",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/markdown_refresh",
        "repo_agent/multi_task_dispatch.py/worker",
        "repo_agent/doc_meta_info.py/MetaInfo",
        "repo_agent/doc_meta_info.py/MetaInfo/init_from_project_path",
        "repo_agent/doc_meta_info.py/MetaInfo/checkpoint",
        "repo_agent/doc_meta_info.py/MetaInfo/print_task_list",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta"
      ]
    },
    "add_new_item": {
      "type": "FunctionDef",
      "name": "add_new_item",
      "md_content": [
        "**add_new_item**: add_new_item函数的功能是将新项目添加到JSON文件中，并生成相应的文档。\n\n**参数**：\n- file_handler (FileHandler): 用于读写文件的文件处理器对象。\n- json_data (dict): 存储项目结构信息的JSON数据。\n\n**代码说明**：\nadd_new_item函数首先创建一个空的字典file_dict，用于存储文件对象的信息。\n然后，通过调用file_handler的get_functions_and_classes方法，获取文件中所有函数和类的信息。\n接下来，遍历获取到的函数和类的信息，对每个对象生成相应的文档。\n在生成文档之前，函数会调用file_handler的get_obj_code_info方法，获取对象的代码信息。\n然后，将代码信息传递给chat_engine的generate_doc方法，生成文档的内容。\n将生成的文档内容存储在md_content字段中，并将该字段添加到代码信息中。\n接着，将代码信息添加到file_dict字典中，以对象名称为键。\n将file_dict添加到json_data中，以文件路径为键。\n最后，将更新后的json_data写入到json文件中，并将文件内容转换为markdown格式。\n将markdown内容写入到.md文件中。\n\n**注意**：\n- 使用add_new_item函数时，需要提供有效的file_handler和json_data参数。\n- 函数会将新增的对象的代码信息和文档信息添加到json_data中，并将更新后的json_data写入json文件。\n- 函数还会将新增的对象的代码信息转换为markdown格式，并将markdown内容写入.md文件。\n\n**输出示例**：\n假设file_handler.file_path为\"repo_agent/runner.py\"，json_data为{\"repo_agent/runner.py\": {}}\n经过add_new_item函数处理后，json_data的内容可能如下所示：\n```json\n{\n    \"repo_agent/runner.py\": {\n        \"add_new_item\": {\n            \"type\": \"FunctionDef\",\n            \"name\": \"add_new_item\",\n            \"md_content\": [],\n            \"code_start_line\": 10,\n            \"code_end_line\": 20,\n            \"parent\": \"Runner\",\n            \"params\": \"file_handler, json_data\",\n            \"have_return\": false,\n            \"code_content\": \"def add_new_item(self, file_handler, json_data):\\n    ...\\n\",\n            \"name_column\": 4\n        }\n    }\n}\n```\n同时，会生成一个名为\"repo_agent/runner.md\"的Markdown文件，其中包含了add_new_item函数的文档内容。"
      ],
      "code_start_line": 248,
      "code_end_line": 278,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler",
        "json_data"
      ],
      "have_return": false,
      "code_content": "    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/read_file",
        "repo_agent/file_handler.py/FileHandler/get_obj_code_info",
        "repo_agent/file_handler.py/FileHandler/write_file",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes",
        "repo_agent/file_handler.py/FileHandler/convert_to_markdown_file",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc"
      ]
    },
    "process_file_changes": {
      "type": "FunctionDef",
      "name": "process_file_changes",
      "md_content": [
        "**process_file_changes**: process_file_changes函数的功能是在检测到文件更改的循环中调用。它的目的是根据绝对文件路径处理更改的文件，包括新文件和已存在的文件。\n其中，changes_in_pyfile是一个包含更改结构信息的字典。一个示例格式是：{'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n**参数**：\n- repo_path (str): 仓库的路径。\n- file_path (str): 文件的相对路径。\n- is_new_file (bool): 表示文件是否为新文件。\n\n**代码说明**：\nprocess_file_changes函数首先创建一个FileHandler对象file_handler，用于处理文件的读写操作。\n然后，通过调用file_handler的read_file方法获取整个py文件的代码。\n接下来，使用change_detector的parse_diffs方法解析文件的差异，该方法通过调用get_file_diff方法获取文件的差异内容。\n然后，调用change_detector的identify_changes_in_structure方法识别发生更改的结构，该方法根据差异内容和文件的函数和类结构列表来判断哪些结构发生了更改。\n接着，判断project_hierarchy.json文件中是否存在与file_handler的文件路径匹配的项。\n如果存在，更新json文件中的内容，并将更新后的json_data写回到json文件中。\n然后，将变更部分的json文件内容转换成markdown内容，并将markdown内容写入.md文件。\n如果不存在，调用add_new_item方法将新的项目添加到json文件中，并生成相应的文档。\n最后，将run过程中更新的Markdown文件（未暂存）添加到暂存区。\n\n**注意**：\n- 使用process_file_changes函数时，需要提供有效的repo_path、file_path和is_new_file参数。\n- 函数会根据文件的绝对路径处理文件的更改，并更新json文件和Markdown文件。\n- 函数还会将run过程中更新的Markdown文件（未暂存）添加到暂存区。\n\n**输出示例**：\n假设repo_path为\"repo_agent\"，file_path为\"runner.py\"，is_new_file为False。\n经过process_file_changes函数处理后，可能会产生以下输出：\n```python\n检测到变更对象：\n{'added': {'add_context_stack', '__init__'}, 'removed': set()}\n```\n同时，可能会更新\"runner.py\"文件的json结构信息，并生成相应的Markdown文档。"
      ],
      "code_start_line": 281,
      "code_end_line": 329,
      "parent": "Runner",
      "params": [
        "self",
        "repo_path",
        "file_path",
        "is_new_file"
      ],
      "have_return": false,
      "code_content": "    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/update_existing_item",
        "repo_agent/file_handler.py/FileHandler",
        "repo_agent/file_handler.py/FileHandler/read_file",
        "repo_agent/file_handler.py/FileHandler/write_file",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes",
        "repo_agent/file_handler.py/FileHandler/convert_to_markdown_file",
        "repo_agent/change_detector.py/ChangeDetector/get_file_diff",
        "repo_agent/change_detector.py/ChangeDetector/parse_diffs",
        "repo_agent/change_detector.py/ChangeDetector/identify_changes_in_structure",
        "repo_agent/change_detector.py/ChangeDetector/add_unstaged_files"
      ]
    },
    "update_existing_item": {
      "type": "FunctionDef",
      "name": "update_existing_item",
      "md_content": [
        "**update_existing_item**: update_existing_item函数的功能是更新现有项目。\n\n**参数**：\n- file_dict (dict): 包含文件结构信息的字典。\n- file_handler (FileHandler): 文件处理器对象。\n- changes_in_pyfile (dict): 包含文件中发生变化的对象信息的字典。\n\n**代码描述**：这个函数根据传入的file_dict字典中的文件结构信息，以及传入的file_handler和changes_in_pyfile参数，更新现有项目。首先，调用get_new_objects函数获取新增和删除的对象。然后，遍历删除对象列表，如果对象在file_dict中存在，则从file_dict中删除该对象，并记录日志信息。接下来，创建一个空的引用者列表referencer_list。\n\n接下来，调用file_handler.generate_file_structure函数生成文件的结构信息，并将结果存储在current_objects字典中。然后，将current_objects字典转换为以对象名称为键的current_info_dict字典。\n\n接下来，遍历current_info_dict字典中的每个对象，如果对象在file_dict中存在，则更新file_dict中该对象的信息，包括类型、起始行、终止行、父级和名称列。如果对象在file_dict中不存在，则将该对象添加到file_dict中。\n\n然后，遍历changes_in_pyfile['added']中的每个对象，对于每个对象，遍历current_objects字典中的每个对象，如果对象名称匹配，则调用project_manager.find_all_referencer函数获取该对象的引用者列表，并将引用者列表添加到referencer_list中。\n\n使用ThreadPoolExecutor并发执行以下操作：对于changes_in_pyfile['added']中的每个对象，遍历referencer_list，找到与该对象名称匹配的引用者字典，然后调用update_object函数更新对象的信息。\n\n最后，返回更新后的file_dict。\n\n**注意**：\n- 该函数依赖于get_new_objects、generate_file_structure和update_object函数，因此在使用之前需要确保已经导入了相应的模块和函数。\n- 在调用该函数之前，需要先创建一个FileHandler对象，并将其作为参数传递给该函数。\n- 该函数的返回值是更新后的file_dict字典。\n\n**输出示例**：模拟代码返回值的可能外观。\n{\n    \"function_name\": {\n        \"type\": \"function\",\n        \"start_line\": 10,\n        ··· ···\n        \"end_line\": 20,\n        \"parent\": \"class_name\"\n    },\n    \"class_name\": {\n        \"type\": \"class\",\n        \"start_line\": 5,\n        ··· ···\n        \"end_line\": 25,\n        \"parent\": None\n    }\n}"
      ],
      "code_start_line": 335,
      "code_end_line": 406,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "changes_in_pyfile"
      ],
      "have_return": true,
      "code_content": "    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": [
        "repo_agent/runner.py/Runner/update_object",
        "repo_agent/runner.py/Runner/get_new_objects",
        "repo_agent/file_handler.py/FileHandler/generate_file_structure"
      ]
    },
    "update_object": {
      "type": "FunctionDef",
      "name": "update_object",
      "md_content": [
        "**update_object**: update_object函数的功能是生成文档内容并更新对象的相应字段信息。\n\n**参数**：\n- file_dict (dict): 包含旧对象信息的字典。\n- file_handler: 文件处理器。\n- obj_name (str): 对象名称。\n- obj_referencer_list (list): 对象引用者列表。\n\n**代码描述**：这个函数根据传入的file_dict字典中的旧对象信息，生成文档内容，并更新对象的相应字段信息。首先判断obj_name是否在file_dict中存在，如果存在，则获取对应的对象信息。然后调用chat_engine.generate_doc函数生成文档内容，并将内容赋值给obj字典中的\"md_content\"字段。\n\n**注意**：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n**输出示例**：假设传入的obj_name为\"func\"，file_dict中存在名为\"func\"的对象，生成的文档内容可能如下所示：\n```\nupdate_object函数的功能是生成文档内容并更新对象的相应字段信息。\n\n参数：\n- file_dict (dict): 包含旧对象信息的字典。\n- file_handler: 文件处理器。\n- obj_name (str): 对象名称。\n- obj_referencer_list (list): 对象引用者列表。\n\n代码描述：这个函数根据传入的file_dict字典中的旧对象信息，生成文档内容，并更新对象的相应字段信息。首先判断obj_name是否在file_dict中存在，如果存在，则获取对应的对象信息。然后调用chat_engine.generate_doc函数生成文档内容，并将内容赋值给obj字典中的\"md_content\"字段。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例：假设传入的obj_name为\"func\"，file_dict中存在名为\"func\"的对象，生成的文档内容可能如下所示：\n```\nupdate_object函数的功能是生成文档内容并更新对象的相应字段信息。\n\n参数：\n- file_dict (dict): 包含旧对象信息的字典。\n- file_handler: 文件处理器。\n- obj_name (str): 对象名称。\n- obj_referencer_list (list): 对象引用者列表。\n\n代码描述：这个函数根据传入的file_dict字典中的旧对象信息，生成文档内容，并更新对象的相应字段信息。首先判断obj_name是否在file_dict中存在，如果存在，则获取对应的对象信息。然后调用chat_engine.generate_doc函数生成文档内容，并将内容赋值给obj字典中的\"md_content\"字段。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例：假设传入的obj_name为\"func\"，file_dict中存在名为\"func\"的对象，生成的文档内容可能如下所示：\n```\nupdate_object函数的功能是生成文档内容并更新对象的相应字段信息。\n\n参数：\n- file_dict (dict): 包含旧对象信息的字典。\n- file_handler: 文件处理器。\n- obj_name (str): 对象名称。\n- obj_referencer_list (list): 对象引用者列表。\n\n代码描述：这个函数根据传入的file_dict字典中的旧对象信息，生成文档内容，并更新对象的相应字段信息。首先判断obj_name是否在file_dict中存在，如果存在，则获取对应的对象信息。然后调用chat_engine.generate_doc函数生成文档内容，并将内容赋值给obj字典中的\"md_content\"字段。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例：假设"
      ],
      "code_start_line": 409,
      "code_end_line": 425,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "obj_name",
        "obj_referencer_list"
      ],
      "have_return": false,
      "code_content": "    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/update_existing_item"
      ],
      "reference_who": [
        "repo_agent/chat_engine.py/ChatEngine/generate_doc"
      ]
    },
    "get_new_objects": {
      "type": "FunctionDef",
      "name": "get_new_objects",
      "md_content": [
        "**get_new_objects**: get_new_objects函数的功能是通过比较.py文件的当前版本和上一个版本，获取添加和删除的对象。\n\n**参数**:\n- file_handler (FileHandler): 文件处理器对象。\n\n**代码描述**:\n该函数首先调用file_handler.get_modified_file_versions方法获取当前版本和上一个版本的文件。然后，使用file_handler.get_functions_and_classes方法分别解析当前版本和上一个版本的文件，获取函数和类的信息。如果存在上一个版本，则解析上一个版本的文件，否则将其设置为空列表。\n\n接下来，函数通过遍历解析后的当前版本和上一个版本的文件，获取当前版本和上一个版本的对象集合。然后，通过集合的差集操作，得到新增的对象和删除的对象。将新增的对象和删除的对象分别转换为列表，并返回包含新增对象和删除对象的元组。\n\n**注意**:\n- 该函数依赖于file_handler模块中的get_modified_file_versions和get_functions_and_classes方法，因此在使用之前需要确保已经导入了file_handler模块。\n- 在调用该函数之前，需要先创建一个FileHandler对象，并将其作为参数传递给该函数。\n- 该函数的返回值是一个包含新增对象和删除对象的元组。\n\n**输出示例**:\nnew_obj: ['add_context_stack', '__init__']\ndel_obj: []"
      ],
      "code_start_line": 429,
      "code_end_line": 452,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/update_existing_item"
      ],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/get_modified_file_versions",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes"
      ]
    }
  },
  "repo_agent/file_handler.py": {
    "FileHandler": {
      "type": "ClassDef",
      "name": "FileHandler",
      "md_content": [
        "**FileHandler**: FileHandler的功能是处理文件的读写操作。\n\n**属性**：\n- file_path：文件的相对路径\n- repo_path：仓库的路径\n- project_hierarchy：项目层级结构的路径\n\n**代码描述**：\nFileHandler类提供了一些方法来处理文件的读写操作。它包含了初始化方法、读取文件内容的方法、获取代码信息的方法、写入文件的方法、获取修改文件版本的方法、获取节点的结束行号的方法、为AST中的节点添加父节点引用的方法、获取文件中的函数和类的方法、生成文件结构的方法、将文件内容转换为Markdown格式的方法以及生成整个仓库结构的方法。\n\n- `__init__(self, repo_path, file_path)`：初始化FileHandler对象，设置仓库路径和文件路径。\n\n- `read_file(self)`：读取文件内容并返回。\n\n- `get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path=None)`：获取给定对象的代码信息。\n\n- `write_file(self, file_path, content)`：将内容写入文件。\n\n- `get_modified_file_versions(self)`：获取修改文件的当前版本和上一个版本。\n\n- `get_end_lineno(self, node)`：获取给定节点的结束行号。\n\n- `add_parent_references(self, node, parent=None)`：为AST中的节点添加父节点引用。\n\n- `get_functions_and_classes(self, code_content)`：获取文件中的函数和类的信息。\n\n- `generate_file_structure(self, file_path)`：生成给定文件路径的文件结构。\n\n- `generate_overall_structure(self)`：生成整个仓库的结构。\n\n- `convert_to_markdown_file(self, file_path=None)`：将文件内容转换为Markdown格式。\n\n**注意**：\n- 在使用`write_file`方法时，确保`file_path`是相对路径。\n- 在使用`convert_to_markdown_file`方法时，如果在`project_hierarchy.json`中找不到指定文件路径的项，将会引发`ValueError`异常。\n\n**输出示例**：\n```python\n{\n    \"type\": \"function\",\n    \"name\": \"read_file\",\n    \"md_content\": [],\n    \"code_start_line\": 10,\n    \"code_end_line\": 20,\n    \"parent\": None,\n    \"params\": []\n}\n```"
      ],
      "code_start_line": 14,
      "code_end_line": 302,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class FileHandler:\n    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n\n    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n\n    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n\n\n    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n        \n    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                # if node.name == \"recursive_check\":\n                #     import pdb; pdb.set_trace()\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                def get_recursive_parent_name(node):\n                    now = node\n                    while \"parent\" in dir(now):\n                        if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                            assert 'name' in dir(now.parent)\n                            return now.parent.name\n                        now = now.parent\n                    return None\n                parent_name = get_recursive_parent_name(node)\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                )\n        return functions_and_classes\n        \n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n    \n\n    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n    \n\n    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef'] and obj['params']:\n                params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py",
        "repo_agent/doc_meta_info.py",
        "repo_agent/doc_meta_info.py/MetaInfo/init_from_project_path",
        "repo_agent/runner.py",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化FileHandler对象。\n**参数**: 这个函数的参数。\n- self: 表示类的实例对象。\n- repo_path: 表示仓库的路径。\n- file_path: 表示相对于仓库根目录的文件路径。\n**代码描述**: 这个函数的描述。\n这个函数用于初始化FileHandler对象。在初始化过程中，将传入的repo_path和file_path分别赋值给self.repo_path和self.file_path属性。其中，file_path是相对于仓库根目录的路径。此外，还将repo_path、CONFIG['project_hierarchy']和\".project_hierarchy.json\"拼接起来，赋值给self.project_hierarchy属性。\n**注意**: 关于代码使用的注意事项。\n- file_path是相对于仓库根目录的路径，需要确保传入的路径是正确的。\n- CONFIG['project_hierarchy']是一个配置文件中的参数，需要确保该参数的正确性。"
      ],
      "code_start_line": 15,
      "code_end_line": 18,
      "parent": "FileHandler",
      "params": [
        "self",
        "repo_path",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "read_file": {
      "type": "FunctionDef",
      "name": "read_file",
      "md_content": [
        "**read_file**: read_file函数的功能是读取文件内容。\n**参数**: 无\n**代码说明**: 该函数首先通过os.path.join()方法获取文件的绝对路径，然后使用open()函数以只读方式打开文件，并指定编码为utf-8。接着使用file.read()方法读取文件内容，并将内容赋值给变量content。最后将content作为函数的返回值。\n**注意**: 该函数要求文件路径必须是有效的，并且文件必须存在。否则会抛出FileNotFoundError异常。\n**输出示例**: 假设文件内容为\"Hello, World!\"，则函数的返回值为\"Hello, World!\"。"
      ],
      "code_start_line": 20,
      "code_end_line": 31,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py",
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    },
    "get_obj_code_info": {
      "type": "FunctionDef",
      "name": "get_obj_code_info",
      "md_content": [
        "**get_obj_code_info**: get_obj_code_info函数的作用是获取给定对象的代码信息。\n**参数**: 这个函数的参数。\n- code_type (str): 代码的类型。\n- code_name (str): 代码的名称。\n- start_line (int): 代码的起始行号。\n- end_line (int): 代码的结束行号。\n- parent (str): 代码的父级。\n- file_path (str, 可选): 文件路径。默认为None。\n**代码描述**: 这个函数的功能是根据给定的对象获取代码信息。\n- 首先，创建一个空的字典code_info用于存储代码信息。\n- 将代码的类型、名称、起始行号、结束行号、父级和参数存储到code_info中。\n- 使用open函数打开文件，并读取文件的所有行。\n- 根据起始行号和结束行号提取代码内容。\n- 在代码的第一行中查找对象名称的位置。\n- 判断代码中是否包含'return'关键字。\n- 将是否有返回值的信息存储到code_info中。\n- 将代码内容、对象名称的位置和其他信息存储到code_info中。\n- 最后，返回code_info字典作为代码信息的结果。\n**注意**: 使用这个函数时需要注意以下几点：\n- file_path参数是可选的，如果不提供则使用默认的self.file_path。\n**输出示例**: 模拟代码返回值的可能外观。\n{\n    \"type\": \"function\",\n    \"name\": \"get_obj_code_info\",\n    \"md_content\": [],\n    \"code_start_line\": 10,\n    \"code_end_line\": 20,\n    \"parent\": \"class_name\",\n    \"params\": \"param1, param2\",\n    \"have_return\": True,\n    \"code_content\": \"def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\\n    ...\\n\",\n    \"name_column\": 4\n}"
      ],
      "code_start_line": 33,
      "code_end_line": 75,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_type",
        "code_name",
        "start_line",
        "end_line",
        "parent",
        "params",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/generate_file_structure",
        "repo_agent/runner.py/Runner/add_new_item"
      ],
      "reference_who": []
    },
    "write_file": {
      "type": "FunctionDef",
      "name": "write_file",
      "md_content": [
        "**write_file**: write_file函数的功能是将内容写入文件。\n**参数**：此函数的参数。\n- file_path (str): 文件的相对路径。\n- content (str): 要写入文件的内容。\n**代码说明**：此函数首先确保file_path是相对路径，如果以'/'开头，则移除开头的'/'。然后，使用os.path.join函数将相对路径和仓库路径拼接成绝对路径abs_file_path。接下来，使用os.makedirs函数创建abs_file_path的父目录（如果不存在）。最后，使用open函数以写入模式打开abs_file_path，并使用utf-8编码写入content。\n**注意**：在使用此函数之前，请确保已经设置了正确的repo_path和file_path，并且content是字符串类型。"
      ],
      "code_start_line": 77,
      "code_end_line": 93,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path",
        "content"
      ],
      "have_return": false,
      "code_content": "    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    },
    "get_modified_file_versions": {
      "type": "FunctionDef",
      "name": "get_modified_file_versions",
      "md_content": [
        "**get_modified_file_versions**: get_modified_file_versions函数的功能是获取修改文件的当前版本和上一个版本。\n**参数**: 无参数。\n**代码描述**: 该函数首先使用git.Repo方法获取仓库对象repo，然后通过os.path.join方法获取当前版本文件的路径current_version_path。接着使用open方法打开文件，并使用read方法读取文件内容，得到当前版本current_version。然后使用repo.iter_commits方法获取最近一次提交的commit对象列表commits，再通过commit.tree获取文件版本的路径，使用data_stream方法读取文件内容，得到上一个版本previous_version。如果没有找到上一个版本，则将previous_version设置为None。最后返回当前版本和上一个版本的元组。\n**注意**: 该函数依赖于git和os模块，需要先导入相应的模块。在使用该函数之前，需要确保已经初始化了git仓库，并且传入了正确的仓库路径和文件路径。\n**输出示例**: \ncurrent_version: 'def get_modified_file_versions(self):\\n    \"\"\"\\n    Get the current and previous versions of the modified file.\\n\\n    Returns:\\n        tuple: A tuple containing the current version and the previous version of the file.\\n    \"\"\"\\n    repo = git.Repo(self.repo_path)\\n\\n    # Read the file in the current working directory (current version)\\n    current_version_path = os.path.join(self.repo_path, self.file_path)\\n    with open(current_version_path, \\'r\\', encoding=\\'utf-8\\') as file:\\n        current_version = file.read()\\n\\n    # Get the file version from the last commit (previous version)\\n    commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\\n    previous_version = None\\n    if commits:\\n        commit = commits[0]\\n        try:\\n            previous_version = (commit.tree / self.file_path).data_stream.read().decode(\\'utf-8\\')\\n        except KeyError:\\n            previous_version = None  # The file may be newly added and not present in previous commits\\n\\n    return current_version, previous_version\\n'\nprevious_version: None"
      ],
      "code_start_line": 96,
      "code_end_line": 120,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/get_new_objects"
      ],
      "reference_who": []
    },
    "get_end_lineno": {
      "type": "FunctionDef",
      "name": "get_end_lineno",
      "md_content": [
        "**get_end_lineno**: get_end_lineno函数的功能是获取给定节点的结束行号。\n**参数**: 此函数的参数。\n**代码描述**: 此函数的描述。\nget_end_lineno函数接受一个节点作为参数，然后通过遍历节点的子节点来找到最后一行的行号。如果节点没有行号，则返回-1。如果节点有行号，则将其作为初始的结束行号。然后，对于节点的每个子节点，递归调用get_end_lineno函数来获取子节点的结束行号。如果子节点的结束行号大于-1，则将其与当前的结束行号进行比较，并更新结束行号为较大的值。最后，返回结束行号作为结果。\n\n**注意**: 使用此代码的注意事项。\n- 此函数依赖于ast模块，因此在使用之前需要确保已经导入了ast模块。\n- 此函数只能用于解析Python代码，不能用于其他编程语言的代码解析。\n\n**输出示例**: 模拟代码返回值的可能外观。\n例如，对于给定的节点，其结束行号为10，则函数的返回值将为10。"
      ],
      "code_start_line": 122,
      "code_end_line": 140,
      "parent": "FileHandler",
      "params": [
        "self",
        "node"
      ],
      "have_return": true,
      "code_content": "    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes"
      ],
      "reference_who": []
    },
    "add_parent_references": {
      "type": "FunctionDef",
      "name": "add_parent_references",
      "md_content": [
        "**add_parent_references**: add_parent_references函数的功能是为AST中的每个节点添加一个父节点引用。\n**parameters**: 这个函数的参数。\n- node: AST中的当前节点。\n- parent: 父节点，默认为None。\n**Code Description**: 这个函数通过递归遍历AST的每个节点，为每个节点添加一个父节点引用。具体步骤如下：\n1. 使用ast.iter_child_nodes(node)遍历当前节点的所有子节点。\n2. 将子节点的parent属性设置为当前节点，即将当前节点作为子节点的父节点。\n3. 递归调用add_parent_references函数，将子节点作为当前节点，将当前节点作为父节点传入。\n**Note**: 使用这个函数可以方便地在AST中获取每个节点的父节点信息，便于后续的分析和处理。"
      ],
      "code_start_line": 142,
      "code_end_line": 154,
      "parent": "FileHandler",
      "params": [
        "self",
        "node",
        "parent"
      ],
      "have_return": false,
      "code_content": "    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes"
      ],
      "reference_who": []
    },
    "get_functions_and_classes": {
      "type": "FunctionDef",
      "name": "get_functions_and_classes",
      "md_content": [
        "**get_functions_and_classes**: get_functions_and_classes函数的功能是检索所有函数、类、它们的参数（如果有的话）以及它们的层级关系。\n输出示例：[('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n在上面的示例中，PipelineEngine是get_all_pys的父结构。\n\n**参数**：\n- code_content: 要解析的整个文件的代码内容。\n\n**代码描述**：\nget_functions_and_classes函数接受一个code_content参数，然后使用ast模块解析该参数中的代码内容。接下来，函数通过遍历AST树的每个节点来获取所有的函数和类。对于每个节点，函数获取节点的类型、名称、起始行号、结束行号、父节点名称以及参数列表（如果有的话），并将这些信息以元组的形式添加到一个列表中。最后，函数返回这个列表作为结果。\n\n**注意**：\n- 此函数依赖于ast模块，因此在使用之前需要确保已经导入了ast模块。\n- 此函数只能用于解析Python代码，不能用于其他编程语言的代码解析。\n\n**输出示例**：\n- 对于给定的节点，其类型为FunctionDef，名称为AI_give_params，起始行号为86，结束行号为95，父节点为None，参数列表为['param1', 'param2']。"
      ],
      "code_start_line": 156,
      "code_end_line": 191,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_content"
      ],
      "have_return": true,
      "code_content": "    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                # if node.name == \"recursive_check\":\n                #     import pdb; pdb.set_trace()\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                def get_recursive_parent_name(node):\n                    now = node\n                    while \"parent\" in dir(now):\n                        if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                            assert 'name' in dir(now.parent)\n                            return now.parent.name\n                        now = now.parent\n                    return None\n                parent_name = get_recursive_parent_name(node)\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                )\n        return functions_and_classes\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py",
        "repo_agent/file_handler.py/FileHandler/generate_file_structure",
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/process_file_changes",
        "repo_agent/runner.py/Runner/get_new_objects"
      ],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/get_end_lineno",
        "repo_agent/file_handler.py/FileHandler/add_parent_references"
      ]
    },
    "get_recursive_parent_name": {
      "type": "FunctionDef",
      "name": "get_recursive_parent_name",
      "md_content": [
        "**get_recursive_parent_name**: get_recursive_parent_name函数的功能是获取给定节点的递归父节点名称。\n**参数**: 这个函数的参数是一个节点对象(node)。\n**代码描述**: 这个函数首先将给定的节点对象赋值给变量now，然后通过循环判断now对象是否有\"parent\"属性。如果有\"parent\"属性，则判断now.parent是否是FunctionDef、ClassDef或AsyncFunctionDef类型的对象。如果是，则断言now.parent对象有\"name\"属性，并返回now.parent.name作为结果。如果不是，则将now.parent赋值给now，继续循环判断。如果循环结束后仍未找到符合条件的父节点，则返回None作为结果。\n**注意**: 使用这段代码时需要注意以下几点：\n- 输入的节点对象必须是AST模块中的节点类型。\n- 父节点的名称只能是FunctionDef、ClassDef或AsyncFunctionDef类型的对象的名称。\n**输出示例**: 假设给定的节点对象的递归父节点是一个函数定义节点，则返回该函数的名称作为结果。如果没有找到符合条件的父节点，则返回None作为结果。"
      ],
      "code_start_line": 178,
      "code_end_line": 185,
      "parent": "get_functions_and_classes",
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "                def get_recursive_parent_name(node):\n                    now = node\n                    while \"parent\" in dir(now):\n                        if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                            assert 'name' in dir(now.parent)\n                            return now.parent.name\n                        now = now.parent\n                    return None\n",
      "name_column": 20,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "generate_file_structure": {
      "type": "FunctionDef",
      "name": "generate_file_structure",
      "md_content": [
        "**generate_file_structure**: generate_file_structure函数的功能是为给定的文件路径生成文件结构。\n\n**参数**：\n- file_path (str): 文件的相对路径。\n\n**代码描述**：\ngenerate_file_structure函数接受一个file_path参数，然后根据给定的文件路径生成文件结构。函数的具体步骤如下：\n- 首先，使用open函数打开文件，并以utf-8编码读取文件的内容。\n- 调用get_functions_and_classes函数，获取文件中的所有函数和类的结构信息。\n- 创建一个空的字典file_objects，用于存储文件结构信息。\n- 遍历结构信息列表，对于每个结构，获取结构的类型、名称、起始行号、结束行号、父级和参数，并调用get_obj_code_info函数获取代码信息。\n- 将代码信息存储到file_objects字典中，以结构名称为键。\n- 最后，返回file_objects字典作为文件结构的结果。\n\n**注意**：使用这个函数时需要注意以下几点：\n- file_path参数是必需的，需要提供文件的相对路径。\n\n**输出示例**：模拟代码返回值的可能外观。\n{\n    \"function_name\": {\n        \"type\": \"function\",\n        \"start_line\": 10,\n        ··· ···\n        \"end_line\": 20,\n        \"parent\": \"class_name\"\n    },\n    \"class_name\": {\n        \"type\": \"class\",\n        \"start_line\": 5,\n        ··· ···\n        \"end_line\": 25,\n        \"parent\": None\n    }\n}"
      ],
      "code_start_line": 193,
      "code_end_line": 230,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/generate_overall_structure",
        "repo_agent/runner.py/Runner/update_existing_item"
      ],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/get_obj_code_info",
        "repo_agent/file_handler.py/FileHandler/get_functions_and_classes"
      ]
    },
    "generate_overall_structure": {
      "type": "FunctionDef",
      "name": "generate_overall_structure",
      "md_content": [
        "**generate_overall_structure**: generate_overall_structure函数的功能是生成仓库的整体结构。\n\n**参数**：无\n\n**代码描述**：generate_overall_structure函数用于生成仓库的整体结构。函数的具体步骤如下：\n- 首先，创建一个空的字典repo_structure，用于存储仓库的结构信息。\n- 创建一个GitignoreChecker对象gitignore_checker，用于检查文件和文件夹是否被.gitignore文件忽略。\n- 使用tqdm库创建一个进度条bar，用于显示生成仓库结构的进度。\n- 对于每个未被忽略的文件，执行以下操作：\n  - 尝试调用generate_file_structure函数生成文件结构，并将结果存储到repo_structure字典中，以文件路径为键。\n  - 如果生成文件结构时出现错误，打印错误信息并继续下一个文件的处理。\n  - 更新进度条的描述信息。\n- 返回repo_structure字典作为仓库的整体结构。\n\n**注意**：使用这个函数时需要注意以下几点：\n- 该函数依赖于generate_file_structure函数和GitignoreChecker类，因此在调用该函数之前，需要确保这两个对象已经定义并可用。\n\n**输出示例**：模拟代码返回值的可能外观。\n{\n    \"file1.py\": {\n        \"function_name\": {\n            \"type\": \"function\",\n            \"start_line\": 10,\n            ··· ···\n            \"end_line\": 20,\n            \"parent\": \"class_name\"\n        },\n        \"class_name\": {\n            \"type\": \"class\",\n            \"start_line\": 5,\n            ··· ···\n            \"end_line\": 25,\n            \"parent\": None\n        }\n    },\n    \"file2.py\": {\n        \"function_name\": {\n            \"type\": \"function\",\n            \"start_line\": 15,\n            ··· ···\n            \"end_line\": 25,\n            \"parent\": None\n        }\n    },\n    ···\n}"
      ],
      "code_start_line": 233,
      "code_end_line": 251,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/init_from_project_path"
      ],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler/generate_file_structure",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/check_files_and_folders"
      ]
    },
    "convert_to_markdown_file": {
      "type": "FunctionDef",
      "name": "convert_to_markdown_file",
      "md_content": [
        "**convert_to_markdown_file**: convert_to_markdown_file函数的功能是将文件的内容转换为markdown格式。\n**参数**: 这个函数的参数。\n- file_path (str, optional): 要转换的文件的相对路径。如果未提供，默认使用None作为文件路径。\n**代码说明**: 这个函数的描述。\n这个函数首先使用utf-8编码打开self.project_hierarchy文件，并将其读取为json_data。\n如果file_path为None，则将file_path设置为self.file_path。\n然后在json_data中查找与file_path匹配的文件对象。\n如果找不到file_dict，则抛出ValueError异常，指示在project_hierarchy.json中找不到指定文件路径的文件对象。\n接下来，函数会遍历file_dict中的所有对象，并根据对象的层级和类型生成markdown格式的内容。\n最后，函数返回markdown内容。\n**注意**: 使用代码时需要注意的事项。\n- 这个函数依赖于self.project_hierarchy文件和self.file_path属性。\n**输出示例**: 模拟代码返回值的可能外观。\n```markdown\n# 1 FunctionDef convert_to_markdown_file():\n\n这个函数的功能是将文件的内容转换为markdown格式。\n\n***\n\n\n# 2 FunctionDef add_new_item(file_handler, json_data):\n\nAdd new projects to the JSON file and generate corresponding documentation.\n\n***\n\n\n# 2 FunctionDef process_file_changes(repo_path, file_path, is_new_file):\n\nThis function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\nAmong them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n***\n\n\n```"
      ],
      "code_start_line": 254,
      "code_end_line": 302,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef'] and obj['params']:\n                params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    }
  },
  "repo_agent/config.py": {},
  "repo_agent/multi_task_dispatch.py": {
    "Task": {
      "type": "ClassDef",
      "name": "Task",
      "md_content": [
        "**Task**: Task的功能是表示一个任务，并管理任务的状态和依赖关系。\n**属性**: \n- task_id: 任务的唯一标识符，类型为int。\n- extra_info: 任务的额外信息，类型为任意类型，默认为None。\n- dependencies: 任务的依赖关系，类型为Task对象的列表。\n- status: 任务的状态，0表示未开始，1表示正在进行，2表示已经完成，3表示出错了。\n**代码描述**: Task类是一个表示任务的类，它有一个构造函数__init__，用于初始化任务的属性。构造函数接受三个参数：task_id表示任务的唯一标识符，dependencies表示任务的依赖关系，extra_info表示任务的额外信息，默认为None。构造函数将这些参数赋值给对应的属性。status属性默认为0，表示任务未开始。\n**注意**: \n- Task类用于表示一个任务，并管理任务的状态和依赖关系。\n- 任务的状态可以通过status属性进行访问和修改。\n- 任务的依赖关系可以通过dependencies属性进行访问和修改。\n- 任务的额外信息可以通过extra_info属性进行访问和修改。"
      ],
      "code_start_line": 9,
      "code_end_line": 14,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class Task:\n    def __init__(self, task_id: int, dependencies: List[Task],extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0 #任务状态：0未开始，1正在进行，2已经完成，3出错了\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/multi_task_dispatch.py/TaskManager/__init__",
        "repo_agent/multi_task_dispatch.py/TaskManager/add_task"
      ],
      "reference_who": []
    },
    "TaskManager": {
      "type": "ClassDef",
      "name": "TaskManager",
      "md_content": [
        "**TaskManager**: TaskManager的功能是管理任务的类。\n\n**属性**: \n- task_dict: 一个字典，用于存储任务的字典，键为任务ID，值为Task对象。\n- task_lock: 一个线程锁，用于保证多线程操作时的数据安全。\n- now_id: 一个整数，表示当前任务的ID。\n- query_id: 一个整数，表示查询任务的ID。\n- sync_func: 一个函数，用于同步任务。\n\n**代码描述**: \nTaskManager类是一个任务管理器，用于管理任务的添加、获取、标记完成等操作。它具有以下功能：\n\n- all_success属性：返回一个布尔值，表示是否所有任务都已完成。\n- add_task方法：添加一个任务到任务字典中，并返回任务的ID。可以指定依赖的任务ID列表和额外信息。\n- get_next_task方法：获取下一个可执行的任务，并返回任务对象和任务ID。可以指定进程ID。\n- mark_completed方法：标记任务为已完成，并更新其他任务的依赖关系。\n\n**注意**: \n- TaskManager类是线程安全的，使用了线程锁来保证多线程操作时的数据安全。\n- 任务ID是自增的，每次添加任务时会自动分配一个新的ID。\n- 任务的依赖关系通过任务ID来表示，可以根据依赖关系来确定任务的执行顺序。\n\n**输出示例**:\n```python\ntask_manager = TaskManager()\ntask_id = task_manager.add_task([1, 2], extra=\"additional info\")\ntask, task_id = task_manager.get_next_task(1)\ntask_manager.mark_completed(task_id)\n```"
      ],
      "code_start_line": 17,
      "code_end_line": 55,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class TaskManager:\n    def __init__(self):\n        self.task_dict: Dict[int, Task]  = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n\n    @property\n    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n\n    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        with self.task_lock:\n            denp_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(task_id=self.now_id, dependencies=denp_tasks, extra_info=extra)\n            self.now_id += 1\n            return self.now_id - 1\n\n    def get_next_task(self, process_id: int):\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (len(self.task_dict[task_id].dependencies) == 0) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    logger.info(f\"[{process_id}] get task_id {task_id}, remain task: {len(self.task_dict)}\")\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n        \n    def mark_completed(self, task_id: int):\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager",
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化TaskManager对象的属性。\n**参数**: 该函数没有参数。\n**代码描述**: __init__函数通过给TaskManager对象的属性赋初值来初始化对象。它包括以下属性的初始化：\n- task_dict: 一个字典，用于存储任务的唯一标识符和对应的Task对象。\n- task_lock: 一个线程锁，用于保证对task_dict的操作的线程安全。\n- now_id: 一个整数，表示当前任务的唯一标识符。\n- query_id: 一个整数，表示查询任务的唯一标识符。\n- sync_func: 一个函数，用于同步任务的执行。\n\n**注意**: \n- TaskManager对象用于管理任务的执行和查询。\n- task_dict属性用于存储任务的唯一标识符和对应的Task对象。\n- task_lock属性用于保证对task_dict的操作的线程安全。\n- now_id属性用于生成任务的唯一标识符。\n- query_id属性用于生成查询任务的唯一标识符。\n- sync_func属性用于同步任务的执行。"
      ],
      "code_start_line": 18,
      "code_end_line": 23,
      "parent": "TaskManager",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.task_dict: Dict[int, Task]  = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/multi_task_dispatch.py/Task"
      ]
    },
    "all_success": {
      "type": "FunctionDef",
      "name": "all_success",
      "md_content": [
        "**all_success**: all_success函数的作用是判断任务字典是否为空。\n**参数**: 该函数没有参数。\n**代码描述**: all_success函数通过比较任务字典的长度是否为0来判断任务字典是否为空。如果任务字典为空，则返回True；否则返回False。\n**注意**: 使用该代码时需要注意任务字典的数据类型，确保任务字典是一个有效的字典对象。\n**输出示例**: 假设任务字典为空，函数将返回True。"
      ],
      "code_start_line": 26,
      "code_end_line": 27,
      "parent": "TaskManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "add_task": {
      "type": "FunctionDef",
      "name": "add_task",
      "md_content": [
        "**add_task**: add_task函数的功能是将一个任务添加到任务管理器中。\n**参数**: \n- dependency_task_id: 依赖的任务ID列表，类型为List[int]。\n- extra: 任务的额外信息，类型为任意类型，默认为None。\n**代码描述**: add_task函数首先通过依赖的任务ID列表获取对应的任务对象，然后将这些任务对象以及额外信息作为参数创建一个新的任务对象，并将其添加到任务管理器的任务字典中。接着，将当前任务ID加1，并返回当前任务ID减1作为新添加任务的ID。\n**注意**: \n- add_task函数用于将一个任务添加到任务管理器中。\n- 任务的依赖关系通过dependency_task_id参数指定。\n- 任务的额外信息可以通过extra参数指定。\n**输出示例**: 返回新添加任务的ID。"
      ],
      "code_start_line": 29,
      "code_end_line": 34,
      "parent": "TaskManager",
      "params": [
        "self",
        "dependency_task_id",
        "extra"
      ],
      "have_return": true,
      "code_content": "    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        with self.task_lock:\n            denp_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(task_id=self.now_id, dependencies=denp_tasks, extra_info=extra)\n            self.now_id += 1\n            return self.now_id - 1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager"
      ],
      "reference_who": [
        "repo_agent/multi_task_dispatch.py/Task"
      ]
    },
    "get_next_task": {
      "type": "FunctionDef",
      "name": "get_next_task",
      "md_content": [
        "**get_next_task**: get_next_task函数的作用是获取下一个任务。\n**参数**: 这个函数的参数是process_id，表示进程的ID。\n**代码描述**: 这个函数首先使用self.task_lock进行线程锁定，然后递增self.query_id。接着，它遍历self.task_dict中的所有任务ID。对于每个任务ID，它判断任务的依赖是否为空且任务的状态是否为0，如果满足条件，则将任务的状态设置为1，并输出日志信息。如果self.query_id能被10整除，则调用self.sync_func()函数。最后，返回任务和任务ID。如果没有满足条件的任务，则返回None和-1。\n**注意**: 使用这段代码时需要注意以下几点：\n- 需要确保在调用get_next_task函数之前已经获取了self.task_lock的锁定。\n- 需要确保在调用get_next_task函数之后释放了self.task_lock的锁定。\n**输出示例**: 假设self.task_dict中有满足条件的任务，返回的结果可能是(task, task_id)。如果没有满足条件的任务，则返回的结果是(None, -1)。"
      ],
      "code_start_line": 36,
      "code_end_line": 47,
      "parent": "TaskManager",
      "params": [
        "self",
        "process_id"
      ],
      "have_return": true,
      "code_content": "    def get_next_task(self, process_id: int):\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (len(self.task_dict[task_id].dependencies) == 0) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    logger.info(f\"[{process_id}] get task_id {task_id}, remain task: {len(self.task_dict)}\")\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "mark_completed": {
      "type": "FunctionDef",
      "name": "mark_completed",
      "md_content": [
        "**mark_completed**: mark_completed函数的功能是将指定任务标记为已完成。\n**parameters**: mark_completed函数的参数为task_id，表示要标记为已完成的任务的ID。\n**Code Description**: mark_completed函数的代码逻辑如下：\n1. 使用self.task_lock对任务字典进行加锁，确保在操作任务字典时不会发生并发问题。\n2. 根据task_id从任务字典中获取目标任务target_task。\n3. 遍历任务字典中的所有任务，对于每个任务task：\n   - 如果target_task在task的依赖列表中，就将target_task从依赖列表中移除。\n4. 使用task_id从任务字典中删除已完成的任务target_task。\n**Note**: \n- 在调用mark_completed函数之前，需要确保任务字典self.task_dict中存在指定的task_id。\n- mark_completed函数会修改任务字典self.task_dict的内容，因此在调用该函数时需要注意并发访问的问题。"
      ],
      "code_start_line": 49,
      "code_end_line": 55,
      "parent": "TaskManager",
      "params": [
        "self",
        "task_id"
      ],
      "have_return": false,
      "code_content": "    def mark_completed(self, task_id: int):\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "worker": {
      "type": "FunctionDef",
      "name": "worker",
      "md_content": [
        "**worker**: worker函数的作用是处理任务的工作函数。\n**参数**: worker函数接受三个参数：\n- task_manager: 任务管理器，用于获取下一个任务并标记任务完成。\n- process_id: 进程ID，用于区分不同的工作进程。\n- handler: 任务处理函数，用于处理任务的额外信息。\n\n**代码描述**: worker函数是一个无限循环的函数，它会不断地从任务管理器中获取下一个任务并进行处理，直到所有任务都完成。具体的处理过程如下：\n1. 首先判断任务管理器中的所有任务是否都已经完成，如果是，则直接返回。\n2. 调用任务管理器的get_next_task方法获取下一个任务和任务ID。\n3. 如果获取到的任务为空，则等待0.5秒后继续下一次循环。\n4. 调用任务处理函数handler，传入任务的额外信息进行处理。\n5. 调用任务管理器的mark_completed方法标记任务完成。\n\n**注意**: \n- worker函数是一个无限循环的函数，只有当任务管理器中的所有任务都完成时才会退出循环。\n- worker函数的执行需要依赖任务管理器和任务处理函数的正确实现。\n- 在处理任务的过程中，可以根据实际需求对任务进行额外的处理或操作。\n\n**输出示例**:\n假设任务管理器中有3个任务，任务的额外信息分别为\"info1\"、\"info2\"和\"info3\"，任务处理函数为打印任务的额外信息。则worker函数的执行过程如下：\n```\n将执行任务: 1\ninfo1\n将执行任务: 2\ninfo2\n将执行任务: 3\ninfo3\n```"
      ],
      "code_start_line": 59,
      "code_end_line": 69,
      "parent": null,
      "params": [
        "task_manager",
        "process_id",
        "handler"
      ],
      "have_return": true,
      "code_content": "def worker(task_manager, process_id: int, handler: Callable):\n    while True:\n        if task_manager.all_success:\n            return\n        task, task_id = task_manager.get_next_task(process_id)\n        if task is None: \n            time.sleep(0.5)\n            continue\n        # print(f\"will perform task: {task_id}\")\n        handler(task.extra_info)\n        task_manager.mark_completed(task.task_id)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py",
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": []
    },
    "some_function": {
      "type": "FunctionDef",
      "name": "some_function",
      "md_content": [
        "**some_function**: some_function的功能是随机睡一会\n**参数**: 这个函数没有参数\n**代码描述**: 这个函数使用了time模块的sleep方法来实现睡眠功能。sleep方法接受一个参数，表示睡眠的时间，这里使用了random模块的random方法生成一个0到1之间的随机数，并乘以3作为睡眠时间。这样就实现了随机睡眠的功能。\n**注意**: 使用这个函数时需要导入time和random模块。调用这个函数后，程序会暂停执行一段随机时间，时间的范围在0到3之间。"
      ],
      "code_start_line": 76,
      "code_end_line": 77,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "    def some_function(): #随机睡一会\n        time.sleep(random.random()*3)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "repo_agent/doc_meta_info.py": {
    "EdgeType": {
      "type": "ClassDef",
      "name": "EdgeType",
      "md_content": [
        "**EdgeType**: EdgeType的功能是定义边的类型。\n\n**属性**: 该类没有属性。\n\n**代码描述**: EdgeType是一个枚举类，用于定义边的类型。它包含了三种类型的边：reference_edge、subfile_edge和file_item_edge。每种类型的边都有一个自动生成的值。\n\n- reference_edge: 表示一个对象引用另一个对象。\n- subfile_edge: 表示一个文件或文件夹属于一个文件夹。\n- file_item_edge: 表示一个对象属于一个文件。\n\n**注意**: 在使用EdgeType时，可以通过引用枚举值来表示不同类型的边。例如，可以使用EdgeType.reference_edge来表示引用关系的边。"
      ],
      "code_start_line": 21,
      "code_end_line": 24,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class EdgeType(Enum):\n    reference_edge = auto() #一个obj引用另一个obj\n    subfile_edge = auto() # 一个 文件/文件夹 属于一个文件夹\n    file_item_edge = auto() #一个 obj 属于一个文件\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/DocItemType/get_edge_type"
      ],
      "reference_who": []
    },
    "DocItemType": {
      "type": "ClassDef",
      "name": "DocItemType",
      "md_content": [
        "**DocItemType**: DocItemType的功能是定义了一组枚举类型，用于表示文档项的类型。\n\n**属性**：\n- _repo: 根节点，需要生成readme\n- _dir: 目录\n- _file: 文件\n- _class: 类\n- _class_function: 类中的函数\n- _function: 文件内的常规函数\n- _sub_function: 函数内的定义的子函数\n- _global_var: 全局变量\n\n**代码描述**：\nDocItemType是一个枚举类，用于表示文档项的类型。它定义了不同类型的文档项，包括根节点、目录、文件、类、类中的函数、文件内的常规函数、函数内的定义的子函数和全局变量。每个文档项都有一个to_str()方法，用于将其转换为字符串表示。它还有一个print_self()方法，用于打印文档项的名称，并根据类型使用不同的颜色进行标记。此外，它还有一个get_edge_type()方法，用于获取从一个文档项到另一个文档项的边的类型。\n\n**注意**：\n- DocItemType是一个枚举类，用于表示文档项的类型。\n- 每个文档项都有一个to_str()方法，用于将其转换为字符串表示。\n- 每个文档项都有一个print_self()方法，用于打印文档项的名称，并根据类型使用不同的颜色进行标记。\n- DocItemType还定义了一个get_edge_type()方法，用于获取从一个文档项到另一个文档项的边的类型。\n\n**输出示例**：\n```\n_class\n```"
      ],
      "code_start_line": 28,
      "code_end_line": 63,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItemType(Enum):\n    _repo = auto() #根节点，需要生成readme\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_function = auto()\n    _function = auto() #文件内的常规function\n    _sub_function = auto() #function内的定义的subfunction\n    _global_var = auto()\n\n    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n\n    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n\n    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/find",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files/walk_tree",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json/walk_file",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json/parse_one_item",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate"
      ],
      "reference_who": []
    },
    "to_str": {
      "type": "FunctionDef",
      "name": "to_str",
      "md_content": [
        "**to_str**: to_str函数的作用是将DocItemType枚举类型的实例转换为对应的字符串表示。\n\n**参数**: 该函数没有参数。\n\n**代码说明**: to_str函数根据DocItemType的不同取值，返回对应的字符串表示。如果self等于DocItemType._class，则返回\"ClassDef\"；如果self等于DocItemType._function、DocItemType._class_function或DocItemType._sub_function，则返回\"FunctionDef\"；否则返回self.name。\n\n**注意**: 使用该函数时需要确保self是DocItemType枚举类型的实例。\n\n**输出示例**: \n- 示例1:\n    ```python\n    item_type = DocItemType._class\n    print(item_type.to_str())\n    ```\n    输出:\n    ```\n    ClassDef\n    ```\n\n- 示例2:\n    ```python\n    item_type = DocItemType._function\n    print(item_type.to_str())\n    ```\n    输出:\n    ```\n    FunctionDef\n    ```"
      ],
      "code_start_line": 38,
      "code_end_line": 48,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json/walk_file"
      ],
      "reference_who": []
    },
    "print_self": {
      "type": "FunctionDef",
      "name": "print_self",
      "md_content": [
        "**print_self**: print_self函数的功能是根据不同的DocItemType类型，打印出相应的颜色和名称。\n**参数**: 无参数。\n**代码描述**: 这个函数首先定义了一个变量color，初始值为Fore.WHITE。然后通过判断self的值，将color的值设置为对应的颜色。最后返回color加上self.name和Style.RESET_ALL的组合。\n**注意**: 这个函数依赖于DocItemType类的定义，需要确保DocItemType类已经被正确导入。\n**输出示例**: 假设self的值为DocItemType._class，那么返回的结果可能是Fore.BLUE + \"class\" + Style.RESET_ALL。"
      ],
      "code_start_line": 50,
      "code_end_line": 60,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/DocItem/print_recursive"
      ],
      "reference_who": []
    },
    "get_edge_type": {
      "type": "FunctionDef",
      "name": "get_edge_type",
      "md_content": [
        "**get_edge_type**: get_edge_type函数的功能是根据给定的from_item_type和to_item_type参数，获取边的类型。\n\n**参数**: \n- from_item_type: 表示边的起始对象的类型，类型为DocItemType。\n- to_item_type: 表示边的目标对象的类型，类型为DocItemType。\n\n**代码描述**: \nget_edge_type函数是一个用于获取边的类型的函数。它接受两个参数from_item_type和to_item_type，这两个参数都是DocItemType类型的对象。函数的返回值是一个EdgeType类型的对象，表示边的类型。\n\n**注意**: \n在使用get_edge_type函数时，需要传入合法的from_item_type和to_item_type参数，否则可能会导致函数无法正常工作。函数的返回值是一个EdgeType类型的对象，可以通过引用枚举值来表示不同类型的边。例如，可以使用EdgeType.reference_edge来表示引用关系的边。"
      ],
      "code_start_line": 62,
      "code_end_line": 63,
      "parent": "DocItemType",
      "params": [
        "from_item_type",
        "to_item_type"
      ],
      "have_return": false,
      "code_content": "    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/EdgeType"
      ]
    },
    "DocItemStatus": {
      "type": "ClassDef",
      "name": "DocItemStatus",
      "md_content": [
        "**DocItemStatus**: DocItemStatus的功能是定义文档项的状态。\n\n**属性**：\n- doc_up_to_date：文档已经是最新的状态，无需生成文档。\n- doc_has_not_been_generated：文档还未生成，需要生成。\n- code_changed：源码被修改了，需要改文档。\n- add_new_referencer：添加了新的引用者。\n- referencer_not_exist：曾经引用他的obj被删除了，或者不再引用他了。\n\n**代码描述**：\nDocItemStatus是一个枚举类，用于表示文档项的状态。它定义了五种状态，分别表示文档的不同情况。这些状态包括文档已经是最新的状态（doc_up_to_date）、文档还未生成（doc_has_not_been_generated）、源码被修改了（code_changed）、添加了新的引用者（add_new_referencer）和曾经引用他的obj被删除了或者不再引用他了（referencer_not_exist）。\n\n**注意**：\n- DocItemStatus是一个枚举类，用于表示文档项的状态。\n- 可以根据具体情况使用不同的状态来表示文档的状态。\n- 可以通过访问枚举类的属性来获取文档项的状态。"
      ],
      "code_start_line": 66,
      "code_end_line": 71,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class DocItemStatus(Enum):\n    doc_up_to_date = auto() #无需生成文档\n    doc_has_not_been_generated = auto() #文档还未生成，需要生成\n    code_changed = auto() #源码被修改了，需要改文档\n    add_new_referencer = auto() #添加了新的引用者\n    referencer_not_exist = auto() #曾经引用他的obj被删除了，或者不再引用他了\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel2",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json/parse_one_item",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item"
      ],
      "reference_who": []
    },
    "DocItem": {
      "type": "ClassDef",
      "name": "DocItem",
      "md_content": [
        "**DocItem**: DocItem的功能是定义了一个类，用于表示文档项。\n\n**属性**：\n- item_type: 文档项的类型\n- item_status: 文档项的状态\n- obj_name: 对象的名字\n- md_content: 存储不同版本的文档内容\n- content: 原本存储的信息\n- children: 子对象\n- father: 父对象\n- depth: 对象的深度\n- tree_path: 对象的路径\n- max_reference_ansce: 最大的引用祖先节点\n- reference_who: 引用了哪些对象\n- who_reference_me: 被哪些对象引用\n- reference_who_name_list: 引用了哪些对象的名字\n- who_reference_me_name_list: 被哪些对象引用的名字\n- multithread_task_id: 多线程中的任务ID\n\n**方法**：\n- \\_\\_eq\\_\\_(self, other): 检查两个对象是否相等\n- has_ans_relation(now_a, now_b): 判断两个节点之间是否存在祖先关系\n- get_travel_list(self): 获取对象及其子对象的列表\n- check_depth(self): 计算对象的深度\n- find_min_ances(node_a, node_b): 查找两个节点的最小公共祖先节点\n- parse_tree_path(self, now_path): 解析对象的路径\n- get_file_name(self): 获取对象所在的文件名\n- get_full_name(self): 获取对象的完整名称\n- find(self, recursive_file_path): 根据路径查找对象\n- print_recursive(self, indent=0, print_content=False): 递归打印对象及其子对象的信息\n\n请注意：\n- DocItem是一个类，用于表示文档项。\n- 每个文档项都有不同的属性和方法，用于描述和操作文档项的信息。\n- 可以根据需要使用这些属性和方法来处理文档项的相关操作。\n- 请根据具体情况使用适当的方法来操作文档项。"
      ],
      "code_start_line": 75,
      "code_end_line": 194,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItem():\n    item_type: DocItemType = DocItemType._class_function\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n\n    obj_name: str = \"\" #对象的名字\n    md_content: List[str] = field(default_factory=list) #存储不同版本的doc\n    content: Dict[Any,Any] = field(default_factory=dict) #原本存储的信息\n\n    children: Dict[str, DocItem] = field(default_factory=dict) #子对象\n    father: Any[DocItem] = None\n\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list) #一整条链路，从root开始\n    max_reference_ansce: Any[DocItem] = None\n\n    reference_who: List[DocItem] = field(default_factory=list) #他引用了谁\n    who_reference_me: List[DocItem] = field(default_factory=list) #谁引用了他\n\n    reference_who_name_list: List[str] = field(default_factory=list) #他引用了谁，这个可能是老版本的\n    who_reference_me_name_list: List[str] = field(default_factory=list) #谁引用了他，这个可能是老版本的\n\n    multithread_task_id: int = -1 #在多线程中的task_id\n\n    def __eq__(self, other) -> bool:\n        # 检查other是否是MyCustomClass的实例\n        if not isinstance(other, DocItem):\n            return False\n        if self.item_type != other.item_type:\n            return False\n        if self.obj_name != other.obj_name:\n            return False\n        return self.get_full_name() == other.get_full_name()\n\n\n    @staticmethod\n    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n    \n    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n    \n    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n\n\n    \n    @staticmethod\n    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n\n    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n\n    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n    \n    \n    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referenced_prompt",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referencer_prompt",
        "repo_agent/doc_meta_info.py/MetaInfo",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files",
        "repo_agent/doc_meta_info.py/MetaInfo/find_obj_with_lineno",
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager/in_white_list",
        "repo_agent/doc_meta_info.py/MetaInfo/_map",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/find_item",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel2",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json/walk_file",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json/parse_one_item",
        "repo_agent/runner.py",
        "repo_agent/runner.py/need_to_generate",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/markdown_refresh/recursive_check",
        "repo_agent/runner.py/Runner/markdown_refresh/to_markdown"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemStatus"
      ]
    },
    "__eq__": {
      "type": "FunctionDef",
      "name": "__eq__",
      "md_content": [
        "**__eq__**: __eq__函数的作用是比较两个对象是否相等。\n**parameters**: 该函数有一个参数other，表示要比较的另一个对象。\n**Code Description**: 该函数用于比较两个对象是否相等。首先，函数会检查other是否是DocItem类的实例，如果不是，则直接返回False。然后，函数会逐个比较两个对象的item_type和obj_name属性是否相等，如果有不相等的情况，则返回False。最后，函数会比较两个对象的get_full_name()方法的返回值是否相等，如果相等，则返回True，否则返回False。\n\n**Note**: 该函数适用于判断两个对象是否相等，可以用于对象的比较操作。\n\n**Output Example**: 假设有两个对象obj1和obj2，它们的item_type和obj_name属性分别为type1和name1，type2和name2，并且它们的get_full_name()方法的返回值分别为\"obj1/full_name1\"和\"obj2/full_name2\"。那么，调用obj1.__eq__(obj2)的返回值为False。"
      ],
      "code_start_line": 98,
      "code_end_line": 106,
      "parent": "DocItem",
      "params": [
        "self",
        "other"
      ],
      "have_return": true,
      "code_content": "    def __eq__(self, other) -> bool:\n        # 检查other是否是MyCustomClass的实例\n        if not isinstance(other, DocItem):\n            return False\n        if self.item_type != other.item_type:\n            return False\n        if self.obj_name != other.obj_name:\n            return False\n        return self.get_full_name() == other.get_full_name()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem/get_full_name"
      ]
    },
    "has_ans_relation": {
      "type": "FunctionDef",
      "name": "has_ans_relation",
      "md_content": [
        "**has_ans_relation**: has_ans_relation函数的功能是判断两个节点是否存在祖先关系，并返回更早的节点。\n**parameters**: 这个函数有两个参数：\n- now_a: DocItem类型，表示当前节点A。\n- now_b: DocItem类型，表示当前节点B。\n**Code Description**: 这个函数首先判断节点B是否在节点A的树路径上，如果是，则返回节点B。接着判断节点A是否在节点B的树路径上，如果是，则返回节点A。如果两个节点之间不存在祖先关系，则返回None。\n**Note**: 使用这个函数时需要注意以下几点：\n- 参数now_a和now_b必须是DocItem类型的对象。\n- 函数返回的结果可能是节点A或节点B，或者是None。\n**Output Example**: 假设节点A的树路径为[节点1, 节点2, 节点3]，节点B的树路径为[节点4, 节点5, 节点2, 节点3]，则函数的返回值为节点2。"
      ],
      "code_start_line": 110,
      "code_end_line": 116,
      "parent": "DocItem",
      "params": [
        "now_a",
        "now_b"
      ],
      "have_return": true,
      "code_content": "    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference"
      ],
      "reference_who": []
    },
    "get_travel_list": {
      "type": "FunctionDef",
      "name": "get_travel_list",
      "md_content": [
        "**get_travel_list**: get_travel_list函数的功能是获取当前节点及其所有子节点的列表。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数通过递归调用获取当前节点及其所有子节点的列表，并将其存储在now_list中，最后返回now_list。\n**代码分析**: \n- 首先，创建一个列表now_list，将当前节点self添加到列表中。\n- 然后，遍历当前节点的所有子节点，对每个子节点调用get_travel_list函数，并将返回的列表与now_list相加，更新now_list。\n- 最后，返回now_list作为结果。\n\n**注意**: \n- 该函数是一个递归函数，通过递归调用获取当前节点及其所有子节点的列表。\n- 该函数只能在DocItem对象中调用。\n\n**输出示例**: \n假设当前节点self有两个子节点child1和child2，且child1有一个子节点grandchild1，child2有一个子节点grandchild2。则调用get_travel_list函数的结果为[now_list, child1, grandchild1, child2, grandchild2]。"
      ],
      "code_start_line": 118,
      "code_end_line": 122,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager"
      ],
      "reference_who": []
    },
    "check_depth": {
      "type": "FunctionDef",
      "name": "check_depth",
      "md_content": [
        "**check_depth**: check_depth函数的功能是计算当前节点的深度。\n\n**参数**: 该函数没有参数。\n\n**代码说明**: check_depth函数首先判断当前节点是否有子节点，如果没有子节点，则将当前节点的深度设置为0，并返回深度值。如果有子节点，则遍历所有子节点，并递归调用每个子节点的check_depth函数，获取子节点的深度值。然后将子节点的最大深度值加1，作为当前节点的深度值。最后返回当前节点的深度值。\n\n**注意**: \n- 该函数是一个递归函数，会遍历整个节点树。\n- 该函数需要在节点树构建完成后调用，否则可能无法正确计算深度。\n\n**输出示例**: \n假设当前节点为根节点，且有两个子节点，其中一个子节点有两个子节点，另一个子节点没有子节点。调用check_depth函数后，返回的深度值为2。"
      ],
      "code_start_line": 124,
      "code_end_line": 133,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json"
      ],
      "reference_who": []
    },
    "find_min_ances": {
      "type": "FunctionDef",
      "name": "find_min_ances",
      "md_content": [
        "**find_min_ances**: find_min_ances函数的功能是找到两个节点的最小公共祖先。\n**parameters**: find_min_ances函数的参数有两个，分别是node_a和node_b，它们的类型都是DocItem。\n**Code Description**: find_min_ances函数的代码逻辑如下：\n1. 首先，初始化变量pos为0。\n2. 接下来，使用断言语句判断node_a和node_b的tree_path的第一个元素是否相等，如果不相等则会触发断言错误。\n3. 然后，进入一个无限循环。\n4. 在循环中，将pos的值加1。\n5. 判断node_a和node_b的tree_path在pos位置上的元素是否相等，如果不相等，则返回node_a的tree_path在pos-1位置上的元素作为最小公共祖先。\n**Note**: 使用该函数时需要保证传入的参数node_a和node_b都是DocItem类型的对象，并且它们的tree_path属性是有效的。\n**Output Example**: 假设node_a的tree_path为[1, 2, 3, 4]，node_b的tree_path为[1, 2, 5, 6]，则函数的返回值为2。"
      ],
      "code_start_line": 138,
      "code_end_line": 144,
      "parent": "DocItem",
      "params": [
        "node_a",
        "node_b"
      ],
      "have_return": true,
      "code_content": "    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference"
      ],
      "reference_who": []
    },
    "parse_tree_path": {
      "type": "FunctionDef",
      "name": "parse_tree_path",
      "md_content": [
        "**parse_tree_path**: parse_tree_path函数的作用是将当前路径添加到now_path列表中，并遍历子节点，递归调用parse_tree_path函数。\n\n**参数**: \n- self: 类的实例对象\n- now_path: 当前路径列表\n\n**代码描述**:\nparse_tree_path函数用于构建树的路径。它接受一个当前路径列表now_path作为参数，并将当前节点self添加到路径列表中。然后，它遍历子节点字典，对每个子节点递归调用parse_tree_path函数，将当前路径作为参数传递给子节点。\n\n**代码分析**:\n1. 将当前节点self添加到路径列表now_path中，形成新的路径self.tree_path。\n2. 遍历子节点字典self.children.items()，其中key为子节点的键，child为子节点的值。\n3. 对每个子节点child，调用child.parse_tree_path(self.tree_path)进行递归调用，将当前路径self.tree_path作为参数传递给子节点。\n\n**注意**:\n- parse_tree_path函数用于构建树的路径，通过递归调用实现了树的遍历。\n- 在调用parse_tree_path函数之前，需要确保当前路径now_path已经包含了父节点的路径信息。"
      ],
      "code_start_line": 146,
      "code_end_line": 149,
      "parent": "DocItem",
      "params": [
        "self",
        "now_path"
      ],
      "have_return": false,
      "code_content": "    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json"
      ],
      "reference_who": []
    },
    "get_file_name": {
      "type": "FunctionDef",
      "name": "get_file_name",
      "md_content": [
        "**get_file_name**: get_file_name函数的功能是获取文件名。\n\n**参数**: 无\n\n**代码描述**: 该函数用于获取文件名。首先调用get_full_name函数获取完整路径，然后通过split函数将路径按照\".py\"进行分割，取分割后的第一个元素（即文件名），再将文件名加上\".py\"后缀作为返回值。\n\n**注意**: 该函数适用于从完整路径中提取文件名。\n\n**输出示例**: 假设完整路径为\"repo_agent/doc_meta_info.py\"，则调用get_file_name函数的返回值为\"doc_meta_info.py\"。"
      ],
      "code_start_line": 151,
      "code_end_line": 153,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager/in_white_list",
        "repo_agent/runner.py/Runner/markdown_refresh"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem/get_full_name"
      ]
    },
    "get_full_name": {
      "type": "FunctionDef",
      "name": "get_full_name",
      "md_content": [
        "**get_full_name**: 获取从下到上所有的obj名字\n**parameters**: 无\n**Code Description**: 该函数用于获取从下到上所有的obj名字。如果当前对象没有父对象，则直接返回当前对象的名字。否则，通过遍历父对象链，将每个对象的名字添加到一个列表中，然后使用\"/\"将列表中的名字连接起来作为返回值。\n\n该函数的实现逻辑如下：\n1. 首先判断当前对象是否有父对象，如果没有，则直接返回当前对象的名字。\n2. 创建一个空列表name_list用于存储所有的obj名字。\n3. 初始化一个变量now为当前对象。\n4. 进入循环，循环条件为now不为None。\n5. 在循环中，将当前对象的名字添加到name_list列表的开头。\n6. 将当前对象的父对象赋值给now，更新循环条件。\n7. 循环结束后，去除name_list列表的第一个元素（即当前对象的名字），然后使用\"/\"将列表中的名字连接起来作为返回值。\n\n**Note**: 该函数适用于获取对象的完整路径，可以用于构建项目的层级结构或者查找对象的位置信息。\n\n**Output Example**: 假设当前对象的名字为obj1，父对象的名字为obj2，父对象的父对象的名字为obj3，则调用get_full_name函数的返回值为\"obj3/obj2/obj1\"。"
      ],
      "code_start_line": 154,
      "code_end_line": 165,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/ChatEngine/generate_doc",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referenced_prompt",
        "repo_agent/chat_engine.py/ChatEngine/generate_doc/get_referencer_prompt",
        "repo_agent/doc_meta_info.py/DocItem/__eq__",
        "repo_agent/doc_meta_info.py/DocItem/get_file_name",
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel2",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json/walk_file",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json",
        "repo_agent/runner.py/need_to_generate",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/markdown_refresh"
      ],
      "reference_who": []
    },
    "find": {
      "type": "FunctionDef",
      "name": "find",
      "md_content": [
        "**find**: find函数的功能是根据给定的路径列表从repo根节点中找到对应的文件，并返回该文件的DocItem对象，如果找不到则返回None。\n**参数**: \n- recursive_file_path: 一个包含路径列表的参数，用于指定要查找的文件的路径。\n\n**代码描述**：\nfind函数首先使用assert语句检查当前对象的item_type属性是否为DocItemType._repo，如果不是，则会抛出异常。然后，函数使用一个while循环来遍历recursive_file_path列表中的每个路径。在每次循环中，函数会检查当前路径是否存在于当前节点的子节点中，如果不存在，则返回None。如果存在，则将当前节点更新为子节点，并继续下一个路径的遍历。当遍历完所有路径后，函数返回最后一个节点。\n\n**注意**：\n- find函数需要在DocItem对象上调用。\n- find函数要求当前对象的item_type属性必须为DocItemType._repo。\n- find函数返回一个Optional[DocItem]类型的对象，表示找到的文件的DocItem对象，如果找不到则返回None。\n\n**输出示例**：\n```\n<DocItem object>\n```"
      ],
      "code_start_line": 168,
      "code_end_line": 179,
      "parent": "DocItem",
      "params": [
        "self",
        "recursive_file_path"
      ],
      "have_return": true,
      "code_content": "    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType"
      ]
    },
    "print_recursive": {
      "type": "FunctionDef",
      "name": "print_recursive",
      "md_content": [
        "**print_recursive**: print_recursive函数的功能是递归打印repo对象。\n**参数**: \n- indent: 打印时的缩进量，默认为0。\n- print_content: 是否打印内容，默认为False。\n**代码描述**: 这个函数首先定义了一个内部函数print_indent，用于生成打印时的缩进字符串。然后通过调用print_indent函数打印当前对象的类型和名称。如果当前对象有子对象，则打印子对象的数量。接着，对每个子对象，调用子对象的print_recursive函数进行递归打印，同时将缩进量加1。\n**注意**: 这个函数依赖于DocItem类的定义，需要确保DocItem类已经被正确导入。\n**输出示例**: \n假设当前对象的类型为DocItem._dir，名称为\"dir\"，并且有2个子对象，那么打印的结果可能是：\n```\n|-dir: dir, 2 children\n  |-file: file1\n  |-file: file2\n```"
      ],
      "code_start_line": 181,
      "code_end_line": 194,
      "parent": "DocItem",
      "params": [
        "self",
        "indent",
        "print_content"
      ],
      "have_return": true,
      "code_content": "    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType/print_self"
      ]
    },
    "print_indent": {
      "type": "FunctionDef",
      "name": "print_indent",
      "md_content": [
        "**print_indent**: print_indent函数的功能是根据给定的缩进级别打印相应的缩进字符串。\n**参数**: 这个函数的参数是indent，表示缩进级别，默认值为0。\n**代码描述**: 这个函数首先判断缩进级别是否为0，如果是0则返回空字符串。如果不是0，则根据缩进级别生成相应的缩进字符串，每个缩进级别对应两个空格，并在最后加上一个\"|-\"\n**注意**: 使用这段代码时需要注意传入的缩进级别应为非负整数。\n**输出示例**: 假设传入的缩进级别为3，那么函数的返回值为\"      |-\""
      ],
      "code_start_line": 184,
      "code_end_line": 187,
      "parent": "print_recursive",
      "params": [
        "indent"
      ],
      "have_return": true,
      "code_content": "        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "find_all_referencer": {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [
        "**find_all_referencer**: find_all_referencer函数的作用是在给定的代码文件中查找特定变量的引用位置。\n**参数**: find_all_referencer函数接受以下参数：\n- repo_path：代码仓库的路径\n- variable_name：要查找引用的变量名\n- file_path：代码文件的路径\n- line_number：变量名所在行号\n- column_number：变量名所在列号\n- in_file_only（可选）：是否只在当前文件内查找引用，默认为False\n\n**代码描述**: find_all_referencer函数首先使用jedi库创建一个Script对象，该对象表示代码文件。然后，根据参数in_file_only的值，使用get_references方法获取变量的引用位置。如果in_file_only为True，则只在当前文件内查找引用；否则，在整个代码仓库中查找引用。接下来，函数过滤出变量名为variable_name的引用，并返回它们的位置。最后，函数将引用位置的相对路径、行号和列号组成的列表返回。\n\n**注意**: 在函数执行过程中，如果发生异常，函数会打印错误信息和相关参数，并返回一个空列表作为结果。\n\n**输出示例**: \n假设在代码文件中存在以下引用关系：\n- 引用位置1：文件路径为\"repo_agent/doc_meta_info.py\"，行号为10，列号为20\n- 引用位置2：文件路径为\"repo_agent/doc_meta_info.py\"，行号为15，列号为30\n\n调用find_all_referencer函数，传入参数repo_path=\"repo_agent\"，variable_name=\"var\"，file_path=\"doc_meta_info.py\"，line_number=5，column_number=10，in_file_only=False，将返回以下结果：\n[(\"doc_meta_info.py\", 10, 20), (\"doc_meta_info.py\", 15, 30)]"
      ],
      "code_start_line": 198,
      "code_end_line": 214,
      "parent": null,
      "params": [
        "repo_path",
        "variable_name",
        "file_path",
        "line_number",
        "column_number",
        "in_file_only"
      ],
      "have_return": true,
      "code_content": "def find_all_referencer(repo_path, variable_name, file_path, line_number, column_number, in_file_only=False):\n    \"\"\"复制过来的之前的实现\"\"\"\n    script = jedi.Script(path=os.path.join(repo_path, file_path))\n\n    try:\n        if in_file_only:\n            references = script.get_references(line=line_number, column=column_number, scope=\"file\")\n        else:\n            references = script.get_references(line=line_number, column=column_number)\n        # 过滤出变量名为 variable_name 的引用，并返回它们的位置\n        variable_references = [ref for ref in references if ref.name == variable_name]\n        return [(os.path.relpath(ref.module_path, repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n    except Exception as e:\n        # 打印错误信息和相关参数\n        print(f\"Error occurred: {e}\")\n        print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n        return []\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference"
      ],
      "reference_who": []
    },
    "MetaInfo": {
      "type": "ClassDef",
      "name": "MetaInfo",
      "md_content": [
        "**MetaInfo**: MetaInfo的功能是管理项目的元信息，包括仓库路径、文档版本、目标仓库的文件结构、白名单等信息。\n\n**属性**：\n- repo_path: 仓库路径\n- document_version: 文档版本，代表目标仓库的commit hash\n- target_repo_hierarchical_tree: 整个仓库的文件结构\n- white_list: 白名单，只处理白名单上的对象\n- in_generation_process: 是否处于文档生成过程中\n- checkpoint_lock: 线程锁，用于保证checkpoint方法的原子性操作\n\n**方法**：\n- init_from_project_path(project_abs_path: str) -> MetaInfo: 从仓库路径中初始化MetaInfo对象\n- from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo: 从已有的checkpoint目录中读取MetaInfo对象\n- checkpoint(target_dir_path: str, flash_reference_relation=False): 将MetaInfo对象保存到指定目录\n- print_task_list(item_list): 打印剩余待处理的任务列表\n- get_all_files() -> List[DocItem]: 获取所有的文件节点\n- find_obj_with_lineno(file_node, start_line_num) -> DocItem: 根据行号查找对应的对象\n- parse_reference(): 解析双向引用关系\n- get_task_manager(now_node: DocItem, task_available_func: Callable = None) -> TaskManager: 获取任务管理器\n- get_topology(task_available_func = None) -> TaskManager: 计算对象的拓扑顺序\n- _map(deal_func: Callable): 对所有节点进行同一个操作\n- load_doc_from_older_meta(older_meta: MetaInfo): 从旧版本的MetaInfo中加载文档\n- from_project_hierarchy_path(repo_path: str) -> MetaInfo: 从项目层次结构路径中加载MetaInfo对象\n- to_hierarchy_json(flash_reference_relation = False): 将MetaInfo对象转换为层次结构JSON\n- from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo: 从层次结构JSON中加载MetaInfo对象\n\n**MetaInfo**类是用于管理项目的元信息的。它包含了仓库路径、文档版本、目标仓库的文件结构、白名单等属性。通过该类，可以从项目路径中初始化MetaInfo对象，从已有的checkpoint目录中读取MetaInfo对象，将MetaInfo对象保存到指定目录，打印剩余待处理的任务列表，获取所有的文件节点，根据行号查找对应的对象，解析双向引用关系，获取任务管理器，计算对象的拓扑顺序，对所有节点进行同一个操作，从旧版本的MetaInfo中加载文档，从项目层次结构路径中加载MetaInfo对象，将MetaInfo对象转换为层次结构JSON，从层次结构JSON中加载MetaInfo对象等操作。\n\n需要注意的是，MetaInfo类是用于管理项目的元信息，可以根据具体需求使用相应的方法来处理元信息的相关操作。\n\n示例输出:\n```python\nmeta_info = MetaInfo()\nmeta_info.repo_path = \"/path/to/repo\"\nmeta_info.document_version = \"abc123\"\nmeta_info.target_repo_hierarchical_tree = DocItem(item_type=DocItemType._repo, obj_name=\"full_repo\")\nmeta_info.white_list = [\"file1.py\", \"file2.py\"]\n\nmeta_info.init_from_project_path(\"/path/to/repo\")\nmeta_info.from_checkpoint_path(\"/path/to/checkpoint\")\nmeta_info.checkpoint(\"/path/to/checkpoint\")\nmeta_info.print_task_list(item_list)\nfiles = meta_info.get_all_files()\nobj = meta_info.find_obj_with_lineno(file_node, start_line_num)\nmeta_info.parse_reference()\ntask_manager = meta_info.get_task_manager(meta_info.target_repo_hierarchical_tree)\ntopology = meta_info.get_topology()\nmeta_info._map(deal_func)\nmeta_info.load_doc_from_older_meta(older_meta)\nmeta_info.from_project_hierarchy_path(\"/path/to/repo\")\nhierarchy_json = meta_info.to_hierarchy_json()\nmeta_info.from_project_hierarchy_json(hierarchy_json)\n```\n\n注意：\n- MetaInfo类用于管理项目的元信息。\n- 可以根据具体需求使用相应的方法来处理元信息的相关操作。\n- 请根据具体情况使用适当的方法来操作元信息。\n",
        "**MetaInfo**: MetaInfo的功能是管理仓库的元信息。\n\n**属性**：\n- repo_path: 仓库的路径\n- document_version: 文档的版本号，用于记录文档的更新状态\n- target_repo_hierarchical_tree: 仓库的文件结构\n- white_list: 白名单，用于指定需要处理的对象列表\n- in_generation_process: 是否在文档生成过程中\n- checkpoint_lock: 用于保证多线程安全的锁\n\n**方法**：\n- init_from_project_path(project_abs_path: str) -> MetaInfo: 从仓库路径初始化MetaInfo对象\n- from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo: 从已有的元信息目录中读取MetaInfo对象\n- checkpoint(self, target_dir_path: str, flash_reference_relation=False): 将MetaInfo保存到指定目录\n- print_task_list(self, item_list): 打印待处理任务列表\n- get_all_files(self) -> List[DocItem]: 获取所有的文件节点\n- find_obj_with_lineno(self, file_node, start_line_num) -> DocItem: 根据行号查找对应的对象\n- parse_reference(self): 解析引用关系\n- get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager: 获取任务管理器\n- get_topology(self, task_available_func = None) -> TaskManager: 计算拓扑顺序\n- _map(self, deal_func: Callable): 对所有节点进行操作\n- load_doc_from_older_meta(self, older_meta: MetaInfo): 从旧版本的元信息中加载文档\n- from_project_hierarchy_path(repo_path: str) -> MetaInfo: 从项目层次结构路径中加载MetaInfo对象\n- to_hierarchy_json(self, flash_reference_relation = False): 将MetaInfo对象转换为层次结构JSON\n- from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo: 从项目层次结构JSON中加载MetaInfo对象\n\n请注意：\n- MetaInfo用于管理仓库的元信息，包括仓库路径、文档版本、文件结构等。\n- 可以通过初始化、从已有的元信息目录中读取、保存到指定目录等方法来操作MetaInfo对象。\n- 可以解析引用关系、计算拓扑顺序、加载文档等操作。\n- 可以根据需要使用这些方法来处理仓库的元信息和文档生成过程。\n- 请根据具体情况使用适当的方法来操作MetaInfo对象。\n\n**DocItem**: DocItem的功能是定义了一个类，用于表示文档项。\n\n**属性**：\n- item_type: 文档项的类型\n- item_status: 文档项的状态\n- obj_name: 对象的名字\n- md_content: 存储不同版本的文档内容\n- content: 原本存储的信息\n- children: 子对象\n- father: 父对象\n- depth: 对象的深度\n- tree_path: 对象的路径\n- max_reference_ansce: 最大的引用祖先节点\n- reference_who: 引用了哪些对象\n- who_reference_me: 被哪些对象引用\n- reference_who_name_list: 引用了哪些对象的名字\n- who_reference_me_name_list: 被哪些对象引用的名字\n- multithread_task_id: 多线程中的任务ID\n\n**方法**：\n- \\_\\_eq\\_\\_(self, other): 检查两个对象是否相等\n- has_ans_relation(now_a, now_b): 判断两个节点之间是否存在祖先关系\n- get_travel_list(self): 获取对象及其子对象的列表\n- check_depth(self): 计算对象的深度\n- find_min_ances(node_a, node_b): 查找两个节点的最小公共祖先节点\n- parse_tree_path(self, now_path): 解析对象的路径\n- get_file_name(self): 获取对象所在的文件名\n- get_full_name(self): 获取对象的完整名称\n- find(self, recursive_file_path): 根据路径查找对象\n- print_recursive(self, indent=0, print_content=False): 递归打印"
      ],
      "code_start_line": 218,
      "code_end_line": 638,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class MetaInfo():\n    repo_path: str = \"\"\n    document_version: str = \"\" #随时间变化，\"\"代表没完成，否则对应一个目标仓库的commit hash\n    target_repo_hierarchical_tree: DocItem = field(default_factory=\"Docitem\") #整个repo的文件结构\n    white_list: Any[List] = None\n\n    in_generation_process: bool = False\n\n    checkpoint_lock: threading.Lock = threading.Lock()\n\n    @staticmethod\n    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n    \n    @staticmethod\n    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n\n    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        with self.checkpoint_lock:\n            logger.info(f\"will save MetaInfo at {target_dir_path}\")\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n            with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            \n            with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n                meta = {\n                    \"repo_path\": self.repo_path,\n                    \"doc_version\": self.document_version,\n                    \"in_generation_process\": self.in_generation_process,\n                }\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n    \n    \n    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n\n    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n\n    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\n        一个行属于这个obj的范围，并且没法属于他的儿子的范围了\"\"\"\n        now_node = file_node\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num and child.content[\"code_end_line\"] >= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n\n            \n\n    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = [], [] #如果指定白名单，只处理白名单上的双向引用关系\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            if white_list_file_names != [] and (file_node.get_file_name() not in white_list_file_names): #如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (now_obj.obj_name not in white_list_obj_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    # if now_obj.obj_name == \"_AgentSkill\":\n                    #     import pdb; pdb.set_trace()\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n            # logger.info(f\"find {ref_count} refer-relation in {file_node.get_full_name()}\")\n    \n\n    def get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n            doc_items = list(filter(in_white_list, doc_items))\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total = len(items_by_depth),desc=\"parsing topology task-list\")\n        while items_by_depth:\n            min_break_level = 1e7\n            target_item = None\n            for item in items_by_depth:\n                now_break_level = 0\n                for referenced in item.reference_who:\n                    \"\"\"一个任务依赖于所有引用者和他的子节点。\n                    我们不能保证引用不成环(也许有些仓库的废代码会出现成环)。这时就只能选择一个相对来说遵守程度最好的了\"\"\"\n                    if not (referenced in deal_items):\n                        now_break_level += 1\n                if now_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = now_break_level\n                if now_break_level == 0:\n                    break\n            \n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(child.multithread_task_id)\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(referenced_item.multithread_task_id)\n            item_denp_task_ids = list(set(item_denp_task_ids)) #去重\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(dependency_task_id=item_denp_task_ids,extra=target_item)\n                target_item.multithread_task_id = task_id\n            deal_items.append(target_item)\n            items_by_depth.remove(target_item)\n            bar.update(1)\n            if min_break_level > 0:\n                print(f\"Reference becoming a circle: have a choose break-level={min_break_level}\")\n\n\n        # Further optimization for minimizing tree distance could be added here\n        return task_manager\n\n    def get_topology(self, task_available_func = None) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n        return task_manager\n    \n    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n\n    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference() \n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n    \n    def to_hierarchy_json(self, flash_reference_relation = False):\n        \"\"\"\n        如果flash_reference_relation=True,则会将最新的双向引用关系写回到meta文件中\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n\n    @staticmethod\n    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py",
        "repo_agent/runner.py/Runner/__init__",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "init_from_project_path": {
      "type": "FunctionDef",
      "name": "init_from_project_path",
      "md_content": [
        "**init_from_project_path**: init_from_project_path函数的功能是从一个仓库路径中初始化MetaInfo对象。\n**参数**: \n- project_abs_path: 仓库的绝对路径\n\n**代码描述**:\ninit_from_project_path函数首先将传入的project_abs_path赋值给变量project_abs_path。然后，函数使用logger记录日志，表示正在从project_abs_path初始化一个新的meta-info。接下来，函数创建一个FileHandler对象file_handler，用于处理文件的读写操作。然后，函数调用file_handler的generate_overall_structure方法生成整个仓库的结构，并将结果赋值给变量repo_structure。接下来，函数调用MetaInfo类的from_project_hierarchy_json方法，根据repo_structure生成一个新的MetaInfo对象metainfo。然后，函数将project_abs_path赋值给metainfo的repo_path属性。最后，函数返回metainfo对象。\n\n**注意**:\n- 在使用init_from_project_path函数时，需要传入仓库的绝对路径作为参数。\n\n**输出示例**:\n```python\n<MetaInfo object>\n```"
      ],
      "code_start_line": 229,
      "code_end_line": 237,
      "parent": "MetaInfo",
      "params": [
        "project_abs_path"
      ],
      "have_return": true,
      "code_content": "    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/__init__",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/file_handler.py/FileHandler",
        "repo_agent/file_handler.py/FileHandler/generate_overall_structure",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json"
      ]
    },
    "from_checkpoint_path": {
      "type": "FunctionDef",
      "name": "from_checkpoint_path",
      "md_content": [
        "**from_checkpoint_path**: from_checkpoint_path函数的功能是从已有的metainfo dir里面读取metainfo。\n\n**参数**： \n- checkpoint_dir_path: metainfo文件夹的路径\n\n**代码描述**:\nfrom_checkpoint_path函数首先根据checkpoint_dir_path和\".project_hierarchy.json\"拼接出项目层级结构的JSON文件路径project_hierarchy_json_path。然后，函数使用open函数打开project_hierarchy_json_path文件，并使用json.load函数将文件内容解析为project_hierarchy_json。\n\n接下来，函数调用MetaInfo.from_project_hierarchy_json函数，将project_hierarchy_json作为参数传入，生成metainfo对象metainfo。\n\n然后，函数根据checkpoint_dir_path和\"meta-info.json\"拼接出meta-info.json文件路径，并使用open函数打开该文件。函数使用json.load函数将文件内容解析为meta_data。\n\n接下来，函数将meta_data中的\"repo_path\"、\"doc_version\"和\"in_generation_process\"分别赋值给metainfo的repo_path、document_version和in_generation_process属性。\n\n最后，函数使用logger.info函数输出日志信息，表示从checkpoint_dir_path加载meta-info，并返回metainfo对象。\n\n**注意**:\n- from_checkpoint_path函数的功能是从已有的metainfo dir里面读取metainfo。\n- 函数会根据checkpoint_dir_path拼接出项目层级结构的JSON文件路径，并读取该文件内容。\n- 函数会调用MetaInfo.from_project_hierarchy_json函数生成metainfo对象。\n- 函数会根据meta-info.json文件的内容更新metainfo对象的属性。\n- 函数会输出日志信息表示加载meta-info，并返回metainfo对象。\n\n**输出示例**:\n```\n<MetaInfo对象>\n```"
      ],
      "code_start_line": 240,
      "code_end_line": 256,
      "parent": "MetaInfo",
      "params": [
        "checkpoint_dir_path"
      ],
      "have_return": true,
      "code_content": "    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/__init__"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json"
      ]
    },
    "checkpoint": {
      "type": "FunctionDef",
      "name": "checkpoint",
      "md_content": [
        "**checkpoint**: checkpoint函数的功能是将MetaInfo保存到指定的目录中。\n\n**参数**:\n- target_dir_path: 一个字符串，表示保存MetaInfo的目标目录的路径。\n- flash_reference_relation（可选）：一个布尔值，表示是否将最新的双向引用关系写回到meta文件中。默认为False。\n\n**代码描述**:\n该函数用于将MetaInfo保存到指定的目录中。具体的代码逻辑如下：\n1. 使用checkpoint_lock进行线程同步。\n2. 使用logger记录将要保存MetaInfo的目标目录路径。\n3. 如果目标目录不存在，则创建目标目录。\n4. 调用to_hierarchy_json函数将层级结构转换为JSON格式，并将结果保存到\".project_hierarchy.json\"文件中。\n5. 将MetaInfo的相关信息保存到\"meta-info.json\"文件中。\n\n**注意**:\n- 如果需要获取最新的双向引用关系并写回到meta文件中，可以将flash_reference_relation参数设置为True。\n- 该函数依赖于to_hierarchy_json函数的实现，需要确保该函数的正确性和可用性。\n\n**输出示例**:\n假设目标repo的层级结构如下：\n- file1\n  - obj1\n  - obj2\n- file2\n  - obj3\n  - obj4\n\n调用checkpoint函数后，将在指定的目录中保存MetaInfo。保存的文件内容示例如下：\n.project_hierarchy.json:\n```python\n{\n    \"file1\": {\n        \"obj1\": {\n            \"name\": \"obj1\",\n            \"type\": \"type1\",\n            \"md_content\": \"content1\",\n            \"item_status\": \"status1\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file1\"\n        },\n        \"obj2\": {\n            \"name\": \"obj2\",\n            \"type\": \"type2\",\n            \"md_content\": \"content2\",\n            \"item_status\": \"status2\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file1\"\n        }\n    },\n    \"file2\": {\n        \"obj3\": {\n            \"name\": \"obj3\",\n            \"type\": \"type3\",\n            \"md_content\": \"content3\",\n            \"item_status\": \"status3\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file2\"\n        },\n        \"obj4\": {\n            \"name\": \"obj4\",\n            \"type\": \"type4\",\n            \"md_content\": \"content4\",\n            \"item_status\": \"status4\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file2\"\n        }\n    }\n}\n```\n\nmeta-info.json:\n```python\n{\n    \"repo_path\": \"repo_path\",\n    \"doc_version\": \"document_version\",\n    \"in_generation_process\": \"in_generation_process\"\n}\n```"
      ],
      "code_start_line": 258,
      "code_end_line": 273,
      "parent": "MetaInfo",
      "params": [
        "self",
        "target_dir_path",
        "flash_reference_relation"
      ],
      "have_return": false,
      "code_content": "    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        with self.checkpoint_lock:\n            logger.info(f\"will save MetaInfo at {target_dir_path}\")\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n            with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            \n            with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n                meta = {\n                    \"repo_path\": self.repo_path,\n                    \"doc_version\": self.document_version,\n                    \"in_generation_process\": self.in_generation_process,\n                }\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/__init__",
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/first_generate",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json"
      ]
    },
    "print_task_list": {
      "type": "FunctionDef",
      "name": "print_task_list",
      "md_content": [
        "**print_task_list**: print_task_list函数的功能是打印任务列表。\n**参数**: 这个函数的参数是item_list，表示任务列表。\n**代码描述**: 这个函数首先导入了prettytable模块，然后创建了一个名为task_table的表格，表格的列名分别是\"task_id\"、\"Doc Generation Reason\"和\"Path\"。接着，函数使用一个循环遍历item_list中的每个元素，并将元素的属性值添加到task_table中。最后，函数打印出\"Remain tasks to be done\"和task_table的内容。\n**注意**: 这个函数依赖于prettytable模块，使用之前需要确保该模块已经安装。"
      ],
      "code_start_line": 276,
      "code_end_line": 284,
      "parent": "MetaInfo",
      "params": [
        "self",
        "item_list"
      ],
      "have_return": false,
      "code_content": "    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": []
    },
    "get_all_files": {
      "type": "FunctionDef",
      "name": "get_all_files",
      "md_content": [
        "**get_all_files**: get_all_files函数的功能是获取所有的file节点。\n\n**参数**：该函数没有参数。\n\n**代码描述**：该函数通过遍历目标repo的层级树，找到所有类型为file的节点，并将其存储在一个列表中返回。\n\n具体的代码逻辑如下：\n1. 创建一个空列表files，用于存储所有的file节点。\n2. 定义一个内部函数walk_tree，用于递归遍历树的节点。\n3. 在walk_tree函数中，首先判断当前节点的类型是否为file，如果是，则将该节点添加到files列表中。\n4. 然后遍历当前节点的所有子节点，对每个子节点调用walk_tree函数进行递归遍历。\n5. 在get_all_files函数中，调用walk_tree函数，并将目标repo的根节点作为参数传入。\n6. 最后，返回存储了所有file节点的列表files。\n\n**注意**：该函数依赖于目标repo的层级树结构，需要确保目标repo的层级树已经构建完成。\n\n**输出示例**：假设目标repo的层级树中存在3个file节点，分别为file1、file2和file3，那么调用get_all_files函数后，将返回一个包含这3个file节点的列表。\n\n```python\n[DocItem(file1), DocItem(file2), DocItem(file3)]\n```"
      ],
      "code_start_line": 286,
      "code_end_line": 295,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/to_hierarchy_json",
        "repo_agent/runner.py/Runner/markdown_refresh"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "walk_tree": {
      "type": "FunctionDef",
      "name": "walk_tree",
      "md_content": [
        "**walk_tree**: walk_tree函数的功能是遍历树形结构的节点，并将文件节点添加到文件列表中。\n**参数**: now_node - 当前节点\n**代码描述**: walk_tree函数接受一个当前节点作为参数。如果当前节点的类型为文件节点，则将该节点添加到文件列表中。然后，对当前节点的所有子节点递归调用walk_tree函数。通过递归调用，walk_tree函数可以遍历整个树形结构，并将所有文件节点添加到文件列表中。\n**注意**: 该函数的目的是遍历树形结构的节点，并将文件节点添加到文件列表中。在调用该函数之前，需要确保树形结构的根节点已经被设置为now_node。"
      ],
      "code_start_line": 289,
      "code_end_line": 293,
      "parent": "get_all_files",
      "params": [
        "now_node"
      ],
      "have_return": false,
      "code_content": "        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType"
      ]
    },
    "find_obj_with_lineno": {
      "type": "FunctionDef",
      "name": "find_obj_with_lineno",
      "md_content": [
        "**find_obj_with_lineno**: find_obj_with_lineno函数的功能是在给定的文件节点和起始行号中查找对象。\n**参数**：该函数接受以下参数：\n- self: 对象本身\n- file_node: 文件节点，表示要查找的文件\n- start_line_num: 起始行号，表示要查找的行号\n\n**代码描述**：该函数通过遍历文件节点及其子节点，查找与给定起始行号匹配的对象。具体步骤如下：\n1. 初始化now_node为file_node。\n2. 当now_node的子节点数量大于0时，执行以下循环：\n   - 设置find_qualify_child为False。\n   - 遍历now_node的子节点，对于每个子节点执行以下操作：\n     - 断言子节点的content不为None。\n     - 如果子节点的code_start_line小于等于start_line_num且code_end_line大于等于start_line_num，则将now_node更新为该子节点，并将find_qualify_child设置为True。\n     - 如果找到合格的子节点，则跳出循环。\n   - 如果没有找到合格的子节点，则返回now_node。\n3. 返回now_node。\n\n**注意**：在查找过程中，函数会根据给定的起始行号找到与之匹配的对象。如果找到合格的子节点，则将now_node更新为该子节点，并继续查找其子节点。如果没有找到合格的子节点，则返回当前节点。\n\n**输出示例**：假设file_node是一个文件节点，start_line_num是一个起始行号，函数将返回与起始行号匹配的对象。"
      ],
      "code_start_line": 298,
      "code_end_line": 312,
      "parent": "MetaInfo",
      "params": [
        "self",
        "file_node",
        "start_line_num"
      ],
      "have_return": true,
      "code_content": "    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\n        一个行属于这个obj的范围，并且没法属于他的儿子的范围了\"\"\"\n        now_node = file_node\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num and child.content[\"code_end_line\"] >= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "parse_reference": {
      "type": "FunctionDef",
      "name": "parse_reference",
      "md_content": [
        "**parse_reference**: parse_reference函数的功能是双向提取所有引用关系。\n**parameters**: 该函数没有参数。\n**Code Description**: parse_reference函数的代码逻辑如下：\n1. 首先，调用get_all_files函数获取所有的文件节点。\n2. 然后，根据白名单的设置，获取白名单上的文件名和对象名。\n3. 对于每个文件节点，遍历其子节点，并调用walk_file函数。\n4. 在walk_file函数中，首先判断当前对象是否在白名单上，如果不在，则跳过当前对象。\n5. 然后，调用find_all_referencer函数查找当前对象的引用位置。\n6. 对于每个引用位置，获取引用者的文件路径和行号，并根据路径找到引用者的节点。\n7. 如果当前对象和引用者之间不存在祖先关系，则将引用者添加到当前对象的引用列表中，并将当前对象添加到引用者的被引用列表中。\n8. 如果引用者的最大引用祖先节点为空，则将当前对象作为最大引用祖先节点。\n9. 否则，判断当前对象和引用者的最小公共祖先节点是否在引用者的最大引用祖先节点的路径上，如果是，则将最小公共祖先节点作为最大引用祖先节点。\n10. 统计引用的数量。\n11. 对于当前对象的每个子对象，递归调用walk_file函数。\n12. 对于每个文件节点的每个子节点，递归调用walk_file函数。\n**Note**: 使用parse_reference函数时需要注意以下几点：\n- 该函数依赖于get_all_files函数和find_all_referencer函数。\n- 在函数执行过程中，会根据白名单的设置和引用关系的判断来提取引用关系。\n- 函数返回的结果是引用关系的统计数量。\n**Output Example**: 假设白名单上的文件名为[\"file1.py\", \"file2.py\"]，对象名为[\"obj1\", \"obj2\"]，文件节点的子节点包含对象A和对象B，对象A引用了对象B，则函数的返回值为1。"
      ],
      "code_start_line": 316,
      "code_end_line": 374,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = [], [] #如果指定白名单，只处理白名单上的双向引用关系\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            if white_list_file_names != [] and (file_node.get_file_name() not in white_list_file_names): #如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (now_obj.obj_name not in white_list_obj_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    # if now_obj.obj_name == \"_AgentSkill\":\n                    #     import pdb; pdb.set_trace()\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/has_ans_relation",
        "repo_agent/doc_meta_info.py/DocItem/find_min_ances",
        "repo_agent/doc_meta_info.py/DocItem/get_file_name",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/DocItem/find",
        "repo_agent/doc_meta_info.py/find_all_referencer",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files",
        "repo_agent/doc_meta_info.py/MetaInfo/find_obj_with_lineno"
      ]
    },
    "get_task_manager": {
      "type": "FunctionDef",
      "name": "get_task_manager",
      "md_content": [
        "**get_task_manager**: get_task_manager函数的功能是根据拓扑引用关系获取任务管理器。\n\n**参数**: \n- now_node: 当前节点的DocItem对象。\n- task_available_func: 可调用对象，用于判断任务是否可用，默认为None。\n\n**代码描述**: \nget_task_manager函数根据拓扑引用关系获取任务管理器。首先，通过调用now_node的get_travel_list方法获取当前节点及其所有子节点的列表。然后，根据白名单过滤列表中的节点。接着，根据节点的深度对列表进行排序，并初始化一些变量。接下来，通过循环遍历列表中的节点，找到一个最小的打断级别的节点。对于每个节点，获取其子节点和引用者的多线程任务ID，并将其添加到任务依赖列表中。然后，根据任务可用性函数和节点是否满足条件，将任务添加到任务管理器中，并更新相关变量。最后，返回任务管理器。\n\n**注意**: \n- get_task_manager函数根据拓扑引用关系获取任务管理器。\n- 可以通过task_available_func参数指定任务可用性函数。\n- 任务管理器是一个TaskManager对象，用于管理任务的添加、获取和标记完成等操作。\n\n**输出示例**:\n```python\ntask_manager = get_task_manager(now_node, task_available_func)\n```\n",
        "**get_task_manager**: get_task_manager函数的功能是根据拓扑引用关系获取任务管理器。\n\n**参数**: \n- now_node: 当前节点的DocItem对象。\n- task_available_func: 可调用对象，用于判断任务是否可用，默认为None。\n\n**代码描述**: \nget_task_manager函数首先获取当前节点及其子节点的列表doc_items。然后，根据白名单过滤doc_items，得到过滤后的列表。接着，根据深度对doc_items进行排序，得到按深度排序的列表items_by_depth。\n\n接下来，创建一个空列表deal_items和一个任务管理器task_manager。然后，创建一个进度条bar，用于显示拓扑任务列表的解析进度。\n\n在循环中，遍历items_by_depth列表，找到当前深度最小的节点target_item。然后，计算target_item的依赖任务ID列表item_denp_task_ids，包括子节点和引用者的任务ID。接着，根据任务可用性函数task_available_func判断target_item是否可用，如果可用，则将target_item添加到任务管理器task_manager中，并返回任务ID。然后，将target_item添加到deal_items列表中，并从items_by_depth列表中移除。最后，更新进度条。\n\n循环结束后，返回任务管理器task_manager。\n\n**注意**: \n- get_task_manager函数根据拓扑引用关系获取任务管理器。\n- 可以通过now_node参数指定当前节点的DocItem对象。\n- 可以通过task_available_func参数指定任务可用性函数，用于判断任务是否可用。\n- 任务管理器是一个TaskManager对象，用于管理任务的添加、获取和标记完成等操作。\n\n**输出示例**:\n```python\ntask_manager = get_task_manager(now_node, task_available_func)\n```\n"
      ],
      "code_start_line": 378,
      "code_end_line": 428,
      "parent": "MetaInfo",
      "params": [
        "self",
        "now_node",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n            doc_items = list(filter(in_white_list, doc_items))\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total = len(items_by_depth),desc=\"parsing topology task-list\")\n        while items_by_depth:\n            min_break_level = 1e7\n            target_item = None\n            for item in items_by_depth:\n                now_break_level = 0\n                for referenced in item.reference_who:\n                    \"\"\"一个任务依赖于所有引用者和他的子节点。\n                    我们不能保证引用不成环(也许有些仓库的废代码会出现成环)。这时就只能选择一个相对来说遵守程度最好的了\"\"\"\n                    if not (referenced in deal_items):\n                        now_break_level += 1\n                if now_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = now_break_level\n                if now_break_level == 0:\n                    break\n            \n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(child.multithread_task_id)\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(referenced_item.multithread_task_id)\n            item_denp_task_ids = list(set(item_denp_task_ids)) #去重\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(dependency_task_id=item_denp_task_ids,extra=target_item)\n                target_item.multithread_task_id = task_id\n            deal_items.append(target_item)\n            items_by_depth.remove(target_item)\n            bar.update(1)\n            if min_break_level > 0:\n                print(f\"Reference becoming a circle: have a choose break-level={min_break_level}\")\n\n\n        # Further optimization for minimizing tree distance could be added here\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/get_topology",
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/multi_task_dispatch.py/TaskManager",
        "repo_agent/multi_task_dispatch.py/TaskManager/add_task",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_travel_list"
      ]
    },
    "in_white_list": {
      "type": "FunctionDef",
      "name": "in_white_list",
      "md_content": [
        "**in_white_list**: in_white_list函数的功能是判断给定的文档项是否在白名单中。\n\n**参数**: \n- item: DocItem类型的参数，表示待判断的文档项。\n\n**代码描述**: 该函数通过遍历白名单中的每个元素，判断给定的文档项的文件名和id_text是否与白名单中的元素匹配。如果匹配成功，则返回True；否则返回False。\n\n**注意**: 该函数适用于判断文档项是否在白名单中。\n\n**输出示例**: 假设白名单中有一个元素，其file_path为\"repo_agent/doc_meta_info.py\"，id_text为\"MetaInfo\"，现有一个文档项的文件名为\"repo_agent/doc_meta_info.py\"，obj_name为\"MetaInfo\"，则调用in_white_list函数的返回值为True。"
      ],
      "code_start_line": 383,
      "code_end_line": 387,
      "parent": "get_task_manager",
      "params": [
        "item"
      ],
      "have_return": true,
      "code_content": "            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_file_name"
      ]
    },
    "get_topology": {
      "type": "FunctionDef",
      "name": "get_topology",
      "md_content": [
        "**get_topology**: get_topology函数的功能是计算repo中所有对象的拓扑顺序。\n\n**参数**: \n- task_available_func: 可调用对象，用于判断任务是否可用，默认为None。\n\n**代码描述**: \nget_topology函数首先调用self.parse_reference()函数解析引用关系。然后根据拓扑引用关系获取任务管理器task_manager。接下来，返回任务管理器task_manager。\n\n**注意**: \n- get_topology函数会调用self.parse_reference()函数和self.get_task_manager()函数。\n- 可以通过task_available_func参数指定任务可用性函数。\n- 任务管理器是一个TaskManager对象，用于管理任务的添加、获取和标记完成等操作。\n\n**输出示例**:\n```python\ntask_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n```\n\n请注意:\n- 该函数会计算repo中所有对象的拓扑顺序，并返回一个任务管理器。\n- 任务管理器可以根据任务的依赖关系来确定任务的执行顺序。\n- 可以通过task_available_func参数指定任务的可用性函数，用于判断任务是否可用。\n- 函数的返回值是一个TaskManager对象，可以用于管理任务的添加、获取和标记完成等操作。"
      ],
      "code_start_line": 430,
      "code_end_line": 435,
      "parent": "MetaInfo",
      "params": [
        "self",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_topology(self, task_available_func = None) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/first_generate"
      ],
      "reference_who": [
        "repo_agent/multi_task_dispatch.py/TaskManager",
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference",
        "repo_agent/doc_meta_info.py/MetaInfo/get_task_manager"
      ]
    },
    "_map": {
      "type": "FunctionDef",
      "name": "_map",
      "md_content": [
        "**_map**: _map函数的功能是将所有节点进行同一个操作。\n**参数**: \n- deal_func: Callable类型，表示要对每个节点执行的操作函数。\n\n**代码说明**:\n_map函数是一个递归函数，用于对目标对象的所有节点进行同一个操作。它接受一个deal_func参数，该参数是一个可调用对象，表示要对每个节点执行的操作函数。\n\n_map函数内部定义了一个名为travel的内部函数，用于遍历目标对象的节点。travel函数接受一个名为now_item的参数，表示当前遍历到的节点。在travel函数中，首先调用deal_func函数对当前节点进行操作，然后遍历当前节点的所有子节点，并递归调用travel函数对每个子节点进行操作。\n\n最后，_map函数调用travel函数，传入目标对象的根节点self.target_repo_hierarchical_tree作为参数，从根节点开始遍历整个对象树。\n\n**注意**: \n- _map函数会对目标对象的所有节点进行操作，可以根据具体需求来定义deal_func函数来处理每个节点的操作。\n- _map函数是一个递归函数，会遍历目标对象的所有子节点，因此需要确保目标对象的结构是正确的，否则可能导致无限递归或其他错误。\n- 在使用_map函数时，需要传入一个合适的deal_func函数来执行具体的操作，确保操作的正确性和有效性。"
      ],
      "code_start_line": 437,
      "code_end_line": 443,
      "parent": "MetaInfo",
      "params": [
        "self",
        "deal_func"
      ],
      "have_return": false,
      "code_content": "    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "load_doc_from_older_meta": {
      "type": "FunctionDef",
      "name": "load_doc_from_older_meta",
      "md_content": [
        "**load_doc_from_older_meta**: load_doc_from_older_meta函数的功能是从旧版本的MetaInfo中加载文档。\n\n**parameters**: \n- older_meta: MetaInfo类型的参数，表示旧版本的、已经生成文档的meta info。\n\n**Code Description**: \nload_doc_from_older_meta函数的代码逻辑如下：\n1. 首先，记录日志信息，表示正在合并来自旧版本MetaInfo的文档。\n2. 获取目标仓库的层次树的根节点。\n3. 定义一个名为find_item的函数，用于在新版本的meta中查找原来的某个对象。\n4. 定义一个名为travel的函数，用于遍历旧版本的meta中的每个对象，并将文档信息合并到新版本的meta中。\n5. 在travel函数中，调用find_item函数查找当前旧版本对象在新版本中的对应对象。\n6. 如果找不到对应对象，则回退到上一级。\n7. 如果找到对应对象，则将旧版本对象的文档内容和状态更新到新版本对象中。\n8. 如果旧版本对象的源码内容发生了修改，则将新版本对象的状态设置为\"code_changed\"。\n9. 对于旧版本对象的每个子对象，递归调用travel函数。\n10. 调用parse_reference函数，解析新版本的双向引用关系。\n11. 定义一个名为travel2的函数，用于遍历旧版本的meta中的每个对象，并观察引用者是否发生了变化。\n12. 在travel2函数中，调用find_item函数查找当前旧版本对象在新版本中的对应对象。\n13. 如果找不到对应对象，则回退到上一级。\n14. 如果找到对应对象，则比较新版本对象的引用者列表和旧版本对象的引用者列表是否相同。\n15. 如果引用者列表发生了变化，并且新版本对象的状态为\"doc_up_to_date\"，则根据变化情况更新新版本对象的状态。\n16. 对于旧版本对象的每个子对象，递归调用travel2函数。\n\n**Note**: 使用load_doc_from_older_meta函数时需要注意以下几点：\n- 该函数依赖于MetaInfo类和DocItem类。\n- 函数的主要功能是将旧版本的文档信息合并到新版本的meta中。\n- 函数会根据旧版本对象的源码是否被修改以及引用者是否发生变化来更新新版本对象的状态。\n- 函数没有返回值。\n\n**Output Example**: 无返回值。"
      ],
      "code_start_line": 445,
      "code_end_line": 499,
      "parent": "MetaInfo",
      "params": [
        "self",
        "older_meta"
      ],
      "have_return": true,
      "code_content": "    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference() \n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/run"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/MetaInfo/parse_reference"
      ]
    },
    "travel": {
      "type": "FunctionDef",
      "name": "travel",
      "md_content": [
        "**travel**: travel函数的功能是在新版的meta中查找是否能找到原来的某个文档项。\n\n**参数**：now_older_item: DocItem类型，表示当前要查找的文档项。\n\n**代码描述**：travel函数首先调用find_item函数，传入当前要查找的文档项，以在新版的meta中查找是否能找到原来的文档项。如果找到了文档项，则将新版的文档内容和状态更新到原来的文档项中。接着，travel函数会检查源码是否被修改了。如果源码被修改了，则将文档项的状态设置为\"code_changed\"。然后，travel函数会递归调用自身，传入当前文档项的子对象，以便在子对象中继续查找是否能找到原来的文档项。\n\n**注意**：在调用travel函数之前，需要确保find_item函数已经被定义，并且root_item变量已经赋值为根节点。\n\n**输出示例**：假设当前文档项的名字为\"item1\"，父节点的子对象中存在名字为\"item1\"的子对象，则返回该子对象。否则返回None。"
      ],
      "code_start_line": 463,
      "code_end_line": 478,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/find_item"
      ]
    },
    "find_item": {
      "type": "FunctionDef",
      "name": "find_item",
      "md_content": [
        "**find_item**: find_item函数的功能是在新版的meta中查找是否能找到原来的某个文档项。\n\n**参数**：now_item: DocItem类型，表示当前要查找的文档项。\n\n**代码描述**：find_item函数首先判断当前文档项是否为根节点，如果是根节点则直接返回根节点。然后递归调用find_item函数，传入当前文档项的父节点，直到找到根节点或者找不到父节点为止。如果找到了父节点，则判断当前文档项的名字是否在父节点的子对象中，如果在则返回该子对象，否则返回None。\n\n**注意**：在调用find_item函数之前，需要确保root_item变量已经赋值为根节点。\n\n**输出示例**：假设当前文档项的名字为\"item1\"，父节点的子对象中存在名字为\"item1\"的子对象，则返回该子对象。否则返回None。"
      ],
      "code_start_line": 450,
      "code_end_line": 460,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_item"
      ],
      "have_return": true,
      "code_content": "        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/travel2"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    },
    "travel2": {
      "type": "FunctionDef",
      "name": "travel2",
      "md_content": [
        "**travel2**: travel2函数的功能是在新版的meta中查找是否能找到原来的某个文档项。\n\n**参数**：now_older_item: DocItem类型，表示当前要查找的文档项。\n\n**代码描述**：travel2函数首先调用find_item函数，传入当前要查找的文档项，以在新版的meta中查找是否能找到原来的文档项。如果找到了文档项，则判断该文档项引用的人是否发生了变化。通过比较新版文档项引用的人和旧版文档项引用的人，判断是否有新的引用者或者曾经引用该文档项的对象不再引用它。如果有新的引用者，则将文档项的状态设置为\"add_new_referencer\"；如果曾经引用该文档项的对象不再引用它，则将文档项的状态设置为\"referencer_not_exist\"。然后，遍历当前文档项的子对象，对每个子对象递归调用travel2函数，以处理子对象及其子对象的情况。\n\n**注意**：在调用travel2函数之前，需要确保find_item函数已经定义并且root_item变量已经赋值为根节点。\n\n**输出示例**：假设当前文档项的名字为\"item1\"，在新版的meta中找到了原来的文档项，并且该文档项引用的人发生了变化，则返回文档项的状态为\"add_new_referencer\"。\n"
      ],
      "code_start_line": 484,
      "code_end_line": 498,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/MetaInfo/load_doc_from_older_meta/find_item"
      ]
    },
    "from_project_hierarchy_path": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_path",
      "md_content": [
        "**from_project_hierarchy_path**: from_project_hierarchy_path函数的功能是根据项目层级结构的路径生成MetaInfo对象。\n\n**参数**: \n- repo_path: 项目的路径\n\n**代码描述**:\nfrom_project_hierarchy_path函数首先根据repo_path和\".project_hierarchy.json\"拼接出项目层级结构的JSON文件路径project_hierarchy_json_path。然后，函数使用logger记录日志，提示正在解析该文件路径。接下来，函数使用os.path.exists检查project_hierarchy_json_path是否存在，如果不存在则抛出NotImplementedError异常。\n\n然后，函数使用open函数打开project_hierarchy_json_path文件，并使用json.load函数加载文件内容到project_hierarchy_json变量中。接下来，函数调用MetaInfo对象的from_project_hierarchy_json方法，传入project_hierarchy_json作为参数，生成并返回一个新的MetaInfo对象。\n\n**注意**:\n- from_project_hierarchy_path函数根据项目层级结构的路径生成MetaInfo对象。\n- 函数会检查项目层级结构的JSON文件是否存在，如果不存在则抛出异常。\n- 函数会加载项目层级结构的JSON文件内容，并调用MetaInfo对象的from_project_hierarchy_json方法生成MetaInfo对象。\n\n**输出示例**:\n```\n<MetaInfo对象>\n```"
      ],
      "code_start_line": 503,
      "code_end_line": 513,
      "parent": "MetaInfo",
      "params": [
        "repo_path"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_json"
      ]
    },
    "to_hierarchy_json": {
      "type": "FunctionDef",
      "name": "to_hierarchy_json",
      "md_content": [
        "**to_hierarchy_json**: to_hierarchy_json函数的功能是将层级结构转换为JSON格式。\n\n**参数**: \n- flash_reference_relation（可选）：一个布尔值，表示是否将最新的双向引用关系写回到meta文件中。默认为False。\n\n**代码描述**: \n该函数用于将层级结构转换为JSON格式。它首先获取所有的file节点，然后遍历每个file节点及其子节点，将节点的相关信息存储在一个字典中。最后，将所有的字典组成一个层级结构的JSON对象并返回。\n\n具体的代码逻辑如下：\n1. 创建一个空字典hierachy_json，用于存储层级结构的JSON对象。\n2. 调用get_all_files函数获取所有的file节点，并将结果存储在file_item_list列表中。\n3. 遍历file_item_list列表，对每个file节点及其子节点进行处理。\n4. 创建一个空字典file_hierarchy_content，用于存储每个file节点及其子节点的相关信息。\n5. 定义一个内部函数walk_file，用于递归遍历每个file节点及其子节点。\n6. 在walk_file函数中，首先将当前节点的相关信息存储在file_hierarchy_content字典中。\n7. 如果flash_reference_relation为True，则将最新的双向引用关系写回到meta文件中。\n8. 将当前节点的父节点的名字存储在file_hierarchy_content字典中的\"parent\"键中。\n9. 遍历当前节点的所有子节点，对每个子节点调用walk_file函数进行递归遍历。\n10. 在file_item_list列表中，将每个file节点及其子节点的相关信息存储在hierachy_json字典中。\n11. 返回层级结构的JSON对象hierachy_json。\n\n**注意**: \n- 如果需要获取最新的双向引用关系并写回到meta文件中，可以将flash_reference_relation参数设置为True。\n- 该函数依赖于get_all_files函数和其他相关函数的实现，需要确保这些函数的正确性和可用性。\n\n**输出示例**: \n假设目标repo的层级结构如下：\n- file1\n  - obj1\n  - obj2\n- file2\n  - obj3\n  - obj4\n\n调用to_hierarchy_json函数后，将返回一个层级结构的JSON对象，示例如下：\n```python\n{\n    \"file1\": {\n        \"obj1\": {\n            \"name\": \"obj1\",\n            \"type\": \"type1\",\n            \"md_content\": \"content1\",\n            \"item_status\": \"status1\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file1\"\n        },\n        \"obj2\": {\n            \"name\": \"obj2\",\n            \"type\": \"type2\",\n            \"md_content\": \"content2\",\n            \"item_status\": \"status2\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file1\"\n        }\n    },\n    \"file2\": {\n        \"obj3\": {\n            \"name\": \"obj3\",\n            \"type\": \"type3\",\n            \"md_content\": \"content3\",\n            \"item_status\": \"status3\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file2\"\n        },\n        \"obj4\": {\n            \"name\": \"obj4\",\n            \"type\": \"type4\",\n            \"md_content\": \"content4\",\n            \"item_status\": \"status4\",\n            \"who_reference_me\": [],\n            \"reference_who\": [],\n            \"parent\": \"file2\"\n        }\n    }\n}\n```"
      ],
      "code_start_line": 515,
      "code_end_line": 546,
      "parent": "MetaInfo",
      "params": [
        "self",
        "flash_reference_relation"
      ],
      "have_return": true,
      "code_content": "    def to_hierarchy_json(self, flash_reference_relation = False):\n        \"\"\"\n        如果flash_reference_relation=True,则会将最新的双向引用关系写回到meta文件中\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/checkpoint"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/doc_meta_info.py/MetaInfo/get_all_files"
      ]
    },
    "walk_file": {
      "type": "FunctionDef",
      "name": "walk_file",
      "md_content": [
        "**walk_file**: walk_file函数的功能是遍历文件对象及其子对象，并将它们的信息存储到file_hierarchy_content字典中。\n\n**参数**: \n- now_obj: 当前的文档项对象。\n\n**代码说明**: walk_file函数首先将当前文档项对象的信息存储到file_hierarchy_content字典中，包括对象的名称、类型、md_content、item_status等。然后，如果flash_reference_relation为True，将当前对象引用的其他对象和引用当前对象的对象的完整名称存储到file_hierarchy_content字典中。接下来，判断当前对象的父对象是否为文件类型，如果不是，则将父对象的名称存储到file_hierarchy_content字典中的parent字段中。最后，遍历当前对象的子对象，并递归调用walk_file函数。\n\n**注意**: \n- walk_file函数用于遍历文件对象及其子对象，并将它们的信息存储到file_hierarchy_content字典中。\n- file_hierarchy_content字典用于存储文件对象及其子对象的信息。\n- 如果flash_reference_relation为True，则会将当前对象引用的其他对象和引用当前对象的对象的完整名称存储到file_hierarchy_content字典中。\n- walk_file函数是一个递归函数，会遍历当前对象的子对象，并对每个子对象调用walk_file函数。\n\n**输出示例**: 无\n\n请注意:\n- 该函数是一个递归函数，用于遍历文件对象及其子对象。\n- 可以根据需要使用该函数来处理文件对象及其子对象的信息。\n- 请根据具体情况使用适当的参数来调用该函数。"
      ],
      "code_start_line": 524,
      "code_end_line": 541,
      "parent": "to_hierarchy_json",
      "params": [
        "now_obj"
      ],
      "have_return": false,
      "code_content": "            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemType/to_str",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name"
      ]
    },
    "from_project_hierarchy_json": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_json",
      "md_content": [
        "**from_project_hierarchy_json**: from_project_hierarchy_json函数的功能是根据项目层级结构的JSON数据生成MetaInfo对象。\n**参数**: \n- project_hierarchy_json: 项目层级结构的JSON数据\n\n**代码描述**:\nfrom_project_hierarchy_json函数首先创建一个空的MetaInfo对象target_meta_info。然后，它遍历project_hierarchy_json中的每个文件名和文件内容。对于每个文件，函数首先检查文件是否存在，如果不存在则跳过该文件。然后，函数检查文件是否为空，如果为空则跳过该文件。\n\n接下来，函数将文件名拆分为递归文件路径，并初始化now_structure为target_meta_info的根节点target_repo_hierarchical_tree。然后，函数使用while循环遍历递归文件路径中的每个路径。在每次循环中，函数检查当前路径是否存在于now_structure的子节点中。如果不存在，则创建一个新的DocItem对象，并将其添加到now_structure的子节点中。如果存在，则将now_structure更新为当前子节点，并继续下一个路径的遍历。循环结束后，函数创建一个新的DocItem对象，并将其添加到now_structure的子节点中。\n\n接下来，函数使用assert语句检查file_content的类型是否为字典。然后，函数定义了一个名为parse_one_item的内部函数，用于递归解析文件内容。parse_one_item函数首先检查key是否存在于item_reflection中，如果存在则跳过该项。然后，函数检查value中的父节点是否为None，如果不为None，则递归调用parse_one_item函数解析父节点。接下来，函数根据key、value和item_reflection创建一个新的DocItem对象，并将其添加到item_reflection中。然后，函数根据value中的其他属性更新新创建的DocItem对象。最后，函数根据value的类型更新新创建的DocItem对象的item_type属性。\n\n接下来，函数使用item_reflection中的信息更新target_meta_info的树结构。如果value的父节点不为None，则将新创建的DocItem对象添加到父节点的子节点中，并更新父节点和新创建的DocItem对象之间的关系。否则，将新创建的DocItem对象添加到文件节点的子节点中，并更新文件节点和新创建的DocItem对象之间的关系。\n\n最后，函数调用target_meta_info的parse_tree_path方法解析树结构的路径，并调用check_depth方法计算树结构的深度。最后，函数返回target_meta_info对象。\n\n**注意**:\n- from_project_hierarchy_json函数根据项目层级结构的JSON数据生成MetaInfo对象。\n- 函数会遍历项目层级结构的JSON数据，解析文件名和文件内容，并构建相应的树结构。\n- 函数会根据文件内容的类型和属性更新相应的DocItem对象。\n- 函数会根据文件内容的父节点关系更新树结构的层级关系。\n- 函数会解析树结构的路径，并计算树结构的深度。\n\n**输出示例**:\n```\n<MetaInfo object>\n```"
      ],
      "code_start_line": 549,
      "code_end_line": 638,
      "parent": "MetaInfo",
      "params": [
        "project_hierarchy_json"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/doc_meta_info.py/MetaInfo/init_from_project_path",
        "repo_agent/doc_meta_info.py/MetaInfo/from_checkpoint_path",
        "repo_agent/doc_meta_info.py/MetaInfo/from_project_hierarchy_path"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/check_depth",
        "repo_agent/doc_meta_info.py/DocItem/parse_tree_path",
        "repo_agent/doc_meta_info.py/DocItem/find"
      ]
    },
    "parse_one_item": {
      "type": "FunctionDef",
      "name": "parse_one_item",
      "md_content": [
        "**parse_one_item**: parse_one_item函数的功能是解析一个项目。\n\n**参数**：\n- key: 项目的键\n- value: 项目的值\n- item_reflection: 项目的反射信息\n\n**代码描述**：\nparse_one_item函数用于递归解析一个项目。它首先检查项目是否已经在item_reflection中存在，如果存在则直接返回。然后，它判断项目是否有父节点，如果有父节点，则先解析父节点。接下来，它根据项目的键、值和md_content创建一个DocItem对象，并将其添加到item_reflection中。如果项目的类型是ClassDef，则将其item_type设置为_class；如果项目的类型是FunctionDef，则根据父节点的类型设置item_type为_class_function或_sub_function。然后，它将项目添加到父节点的children中，并设置项目的父节点。最后，它根据项目的一些属性值，如item_status、reference_who和who_reference_me，更新DocItem对象的属性。\n\n**注意**：\n- parse_one_item函数用于递归解析一个项目。\n- 项目的键和值用于创建DocItem对象，并将其添加到item_reflection中。\n- 根据项目的类型和父节点的类型，设置DocItem对象的item_type。\n- 项目的属性值用于更新DocItem对象的属性。\n\n**输出示例**：\n无"
      ],
      "code_start_line": 593,
      "code_end_line": 629,
      "parent": "from_project_hierarchy_json",
      "params": [
        "key",
        "value",
        "item_reflection"
      ],
      "have_return": true,
      "code_content": "            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n",
      "name_column": 16,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItemType",
        "repo_agent/doc_meta_info.py/DocItemStatus",
        "repo_agent/doc_meta_info.py/DocItem"
      ]
    }
  },
  "repo_agent/chat_engine.py": {
    "get_import_statements": {
      "type": "FunctionDef",
      "name": "get_import_statements",
      "md_content": [
        "**get_import_statements**: get_import_statements函数的功能是获取当前模块中的所有导入语句。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数通过使用inspect模块的getsourcelines方法获取当前模块的源代码行，并通过遍历源代码行来筛选出所有的导入语句。最后将筛选出的导入语句返回。\n**注意**: 使用该函数时需要确保当前模块已经被导入，并且在调用该函数之前没有对当前模块进行修改。\n**输出示例**: 假设当前模块中存在以下导入语句：\n```\nimport os\nfrom datetime import datetime\n```\n则该函数的返回值为：\n```\n['import os\\n', 'from datetime import datetime\\n']\n```"
      ],
      "code_start_line": 16,
      "code_end_line": 19,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [
        "**build_path_tree**: build_path_tree函数的功能是构建路径树。\n**parameters**: 这个函数的参数有三个：\n- who_reference_me: 一个列表，包含引用了该函数的对象的路径列表。\n- reference_who: 一个列表，包含该函数引用的其他对象的路径列表。\n- doc_item_path: 一个字符串，表示当前函数的路径。\n\n**Code Description**: 这个函数的作用是根据给定的引用关系和路径信息构建路径树。路径树是一个嵌套字典的数据结构，用于表示项目的层次结构。函数首先定义了一个内部函数tree，用于创建一个空的路径树。然后，根据引用关系和路径信息，遍历引用了该函数的对象和该函数引用的其他对象的路径列表。对于每个路径，将其按照路径分隔符分割成部分，并逐级在路径树中创建相应的节点。最后，根据当前函数的路径，在路径树中找到对应的节点，并在节点名称前加上一个星号。\n\n**Note**: 在构建路径树时，需要注意路径的分隔符，通常是操作系统的路径分隔符。此外，函数返回的路径树是一个嵌套字典的数据结构，可以通过tree_to_string函数将其转换为字符串形式进行展示。\n\n**Output Example**: 下面是一个可能的路径树的示例：\n```\nrepo_agent\n    chat_engine.py\n        build_path_tree\n            ✳️build_path_tree\n```\n这个路径树表示了项目的层次结构，其中当前函数build_path_tree被标记为*。\n\n以上是对build_path_tree函数的详细解释和分析。"
      ],
      "code_start_line": 21,
      "code_end_line": 48,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/ChatEngine/generate_doc"
      ],
      "reference_who": []
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [
        "**tree**: tree函数的功能是返回一个默认字典的树结构。\n**参数**: 该函数没有参数。\n**代码描述**: tree函数使用了defaultdict函数来创建一个默认字典的树结构。默认字典是一种特殊的字典，它在访问不存在的键时会返回一个默认值，而不会抛出KeyError异常。在这个函数中，我们使用defaultdict(tree)来创建一个默认字典的树结构，其中tree是一个递归调用的函数名。这意味着当我们访问不存在的键时，会返回一个新的默认字典的树结构，从而形成了一个无限深度的树。\n**注意**: 使用tree函数时需要注意以下几点：\n- tree函数返回的是一个默认字典的树结构，可以通过键来访问树的节点。\n- 当访问不存在的键时，会返回一个新的默认字典的树结构，而不会抛出异常。\n- 可以通过递归调用tree函数来创建无限深度的树结构。\n**输出示例**: 假设我们调用tree函数，并访问了一些键，可能的输出结果如下所示：\n{\n    'key1': defaultdict(<function tree at 0x00000123456789>, {}),\n    'key2': defaultdict(<function tree at 0x00000123456789>, {}),\n    'key3': defaultdict(<function tree at 0x00000123456789>, {})\n}"
      ],
      "code_start_line": 22,
      "code_end_line": 23,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [
        "**tree_to_string**: tree_to_string函数的功能是将树形结构转换为字符串。\n**parameters**: 该函数的参数有两个：\n- tree: 表示树形结构的字典。\n- indent: 表示缩进的级别，默认为0。\n**Code Description**: 该函数通过递归的方式遍历树形结构，将每个节点的键值对转换为字符串，并根据缩进级别添加相应的缩进。如果节点的值是一个字典，则继续递归调用tree_to_string函数处理该字典。最后将转换后的字符串返回。\n**Note**: 使用该函数时需要注意以下几点：\n- tree参数必须是一个字典类型。\n- indent参数必须是一个整数类型。\n**Output Example**: 假设tree参数为{'A': {'B': {'C': {}, 'D': {}}, 'E': {}}, 'F': {}}, 则函数的返回值为：\n    A\n        B\n            C\n            D\n        E\n    F"
      ],
      "code_start_line": 40,
      "code_end_line": 46,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "ChatEngine": {
      "type": "ClassDef",
      "name": "ChatEngine",
      "md_content": [
        "**ChatEngine**: ChatEngine的功能是生成函数或类的文档。\n\n**attributes**: 这个类的属性。\n\n**Code Description**: 这个类的描述。\n\nChatEngine类有一个构造函数`__init__`，它接受一个CONFIG参数，并将其赋值给self.config属性。\n\nChatEngine类还有一个方法`num_tokens_from_string`，它接受一个字符串和一个编码名称作为参数，并返回文本字符串中的标记数。\n\nChatEngine类还有一个方法`generate_doc`，它接受一个DocItem对象和一个文件处理器作为参数。它根据传入的参数提取代码信息，并根据代码信息生成文档。\n\n**Note**: 使用ChatEngine类时需要注意的一些事项。\n\n**Output Example**: 模拟代码返回值的可能外观。\n\n请注意：\n- 生成的文档内容中不应包含Markdown的标题和分隔符语法。\n- 主要使用中文编写文档。如果有必要，可以在分析和描述中使用一些英文单词，以提高文档的可读性，因为不需要将函数名或变量名翻译成目标语言。"
      ],
      "code_start_line": 50,
      "code_end_line": 278,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChatEngine:\n    \"\"\"\n    ChatEngine is used to generate the doc of functions or classes.\n    \"\"\"\n    def __init__(self, CONFIG):\n        self.config = CONFIG\n\n    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                # print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=project_structure_prefix,\n                        project_structure=project_structure, \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=combine_ref_situation, \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            reference_letter=reference_letter, \n                            referencer_content=referencer_content,\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py",
        "repo_agent/runner.py/Runner/__init__"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化ChatEngine对象。\n**参数**: 这个函数的参数是CONFIG，表示配置信息。\n**代码描述**: 这个函数将传入的CONFIG赋值给self.config，用于初始化ChatEngine对象的配置信息。\n**注意**: 在使用这段代码时需要注意以下几点：\n- CONFIG参数必须是一个有效的配置信息。\n- 初始化ChatEngine对象后，可以通过self.config来访问配置信息。"
      ],
      "code_start_line": 54,
      "code_end_line": 55,
      "parent": "ChatEngine",
      "params": [
        "self",
        "CONFIG"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, CONFIG):\n        self.config = CONFIG\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "num_tokens_from_string": {
      "type": "FunctionDef",
      "name": "num_tokens_from_string",
      "md_content": [
        "**num_tokens_from_string**: num_tokens_from_string函数的功能是返回文本字符串中的标记数。\n**parameters**: 这个函数的参数是一个字符串(string)和一个编码(encoding_name)，默认值为\"cl100k_base\"。\n**Code Description**: 这个函数首先根据给定的编码名称获取编码(encoding)，然后使用编码将字符串进行编码，并计算编码后的标记数(num_tokens)，最后返回标记数。\n**Note**: 使用默认的编码名称\"cl100k_base\"可以获得基于100k词汇表的编码。如果需要使用其他编码，请提供相应的编码名称。\n**Output Example**: 假设输入的字符串为\"Hello, world!\"，编码后的标记数为3。"
      ],
      "code_start_line": 57,
      "code_end_line": 61,
      "parent": "ChatEngine",
      "params": [
        "self",
        "string",
        "encoding_name"
      ],
      "have_return": true,
      "code_content": "    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py/ChatEngine/generate_doc"
      ],
      "reference_who": []
    },
    "generate_doc": {
      "type": "FunctionDef",
      "name": "generate_doc",
      "md_content": [
        "**generate_doc**: generate_doc函数的功能是生成文档。\n\n**parameters**: 这个函数的参数有两个：\n- doc_item: 一个DocItem对象，表示文档项。\n- file_handler: 一个FileHandler对象，用于处理文件。\n\n**Code Description**: 这个函数首先获取传入的doc_item对象的相关信息，包括类型、名称、代码内容、是否有返回值等。然后根据doc_item对象的引用关系和路径信息，构建项目的层次结构。接下来，根据语言设置，确定代码的语言类型。然后，根据引用关系和路径信息，生成引用了该函数的对象和该函数引用的其他对象的提示信息。之后，根据函数的相关信息和引用关系，构建系统提示信息和用户提示信息。最后，使用OpenAI的Chat API，将系统提示信息和用户提示信息传入模型，生成文档的内容。\n\n**Note**: 生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n**Output Example**: 假设当前函数的名称为generate_doc，传入的doc_item对象的类型为Function，名称为func，代码内容为\"def func():\\n    print('Hello, world!')\"，没有返回值，被引用了两次，分别是obj1和obj2。根据这些信息，生成的文档内容可能如下所示：\n```\ngenerate_doc函数的功能是生成文档。\n\n参数：\n- doc_item: 一个DocItem对象，表示文档项。\n- file_handler: 一个FileHandler对象，用于处理文件。\n\n代码描述：这个函数根据传入的doc_item对象的相关信息，包括类型、名称、代码内容、是否有返回值等，生成文档的内容。首先根据doc_item对象的引用关系和路径信息，构建项目的层次结构。然后根据语言设置，确定代码的语言类型。接下来，根据引用关系和路径信息，生成引用了该函数的对象和该函数引用的其他对象的提示信息。之后，根据函数的相关信息和引用关系，构建系统提示信息和用户提示信息。最后，使用OpenAI的Chat API，将系统提示信息和用户提示信息传入模型，生成文档的内容。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例：假设当前函数的名称为generate_doc，传入的doc_item对象的类型为Function，名称为func，代码内容为\"def func():\\n    print('Hello, world!')\"，没有返回值，被引用了两次，分别是obj1和obj2。根据这些信息，生成的文档内容可能如下所示：\n```\ngenerate_doc函数的功能是生成文档。\n\n参数：\n- doc_item: 一个DocItem对象，表示文档项。\n- file_handler: 一个FileHandler对象，用于处理文件。\n\n代码描述：这个函数根据传入的doc_item对象的相关信息，包括类型、名称、代码内容、是否有返回值等，生成文档的内容。首先根据doc_item对象的引用关系和路径信息，构建项目的层次结构。然后根据语言设置，确定代码的语言类型。接下来，根据引用关系和路径信息，生成引用了该函数的对象和该函数引用的其他对象的提示信息。之后，根据函数的相关信息和引用关系，构建系统提示信息和用户提示信息。最后，使用OpenAI的Chat API，将系统提示信息和用户提示信息传入模型，生成文档的内容。\n\n注意：生成的文档内容中包含了引用了该函数的对象和该函数引用的其他对象的代码和文档信息。可以根据需要使用这些信息来理解和使用该函数。\n\n输出示例：假设当前函数的名称为generate_doc"
      ],
      "code_start_line": 63,
      "code_end_line": 278,
      "parent": "ChatEngine",
      "params": [
        "self",
        "doc_item",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                # print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=project_structure_prefix,\n                        project_structure=project_structure, \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=combine_ref_situation, \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            reference_letter=reference_letter, \n                            referencer_content=referencer_content,\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/generate_doc_for_a_single_item",
        "repo_agent/runner.py/Runner/add_new_item",
        "repo_agent/runner.py/Runner/update_object"
      ],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name",
        "repo_agent/chat_engine.py/build_path_tree",
        "repo_agent/chat_engine.py/ChatEngine/num_tokens_from_string"
      ]
    },
    "get_referenced_prompt": {
      "type": "FunctionDef",
      "name": "get_referenced_prompt",
      "md_content": [
        "**get_referenced_prompt**: get_referenced_prompt函数的功能是获取引用了哪些对象的提示信息。\n**parameters**: get_referenced_prompt函数的参数为doc_item，类型为DocItem，表示文档项。\n**Code Description**: get_referenced_prompt函数根据传入的文档项，获取引用了哪些对象的提示信息。首先判断文档项的reference_who列表是否为空，如果为空，则返回空字符串。然后遍历reference_who列表，对于每个引用对象，生成一个提示信息instance_prompt，包括对象的完整名称、文档内容和原始代码。将所有的提示信息连接起来，返回一个字符串。\n\n该函数的实现逻辑如下：\n1. 首先判断文档项的reference_who列表是否为空，如果为空，则返回空字符串。\n2. 创建一个空列表prompt用于存储提示信息。\n3. 遍历reference_who列表，对于每个引用对象，执行以下步骤：\n   - 创建一个字符串instance_prompt，包括引用对象的完整名称、文档内容和原始代码。\n   - 将instance_prompt添加到prompt列表中。\n4. 将prompt列表中的所有元素使用换行符连接起来，作为函数的返回值。\n\n**Note**: 该函数适用于获取引用了哪些对象的提示信息，可以用于展示代码中的引用关系和相关文档内容。\n\n**Output Example**: 假设有两个引用对象，完整名称分别为obj1和obj2，文档内容分别为\"文档1\"和\"文档2\"，原始代码分别为\"代码1\"和\"代码2\"，则调用get_referenced_prompt函数的返回值为：\n```\nobj: obj1\nDocument: 文档1\nRaw code:\n代码1\n==========\nobj: obj2\nDocument: 文档2\nRaw code:\n代码2\n==========\n```"
      ],
      "code_start_line": 110,
      "code_end_line": 117,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: {reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name"
      ]
    },
    "get_referencer_prompt": {
      "type": "FunctionDef",
      "name": "get_referencer_prompt",
      "md_content": [
        "**get_referencer_prompt**: get_referencer_prompt函数的功能是获取引用了当前对象的其他对象的提示信息。\n**parameters**: get_referencer_prompt函数的参数为doc_item，表示当前对象的文档项。\n**Code Description**: get_referencer_prompt函数根据当前对象的who_reference_me属性，获取引用了当前对象的其他对象的提示信息。如果当前对象没有被其他对象引用，则返回空字符串。否则，遍历who_reference_me列表，对每个引用者对象生成一个提示信息，并将这些提示信息添加到一个列表中。最后，将列表中的所有提示信息使用换行符连接起来作为返回值。\n\nget_referencer_prompt函数的实现逻辑如下：\n1. 首先判断当前对象的who_reference_me列表是否为空，如果为空，则返回空字符串。\n2. 创建一个列表prompt，用于存储引用者对象的提示信息。\n3. 遍历who_reference_me列表，对每个引用者对象生成一个提示信息。\n4. 每个引用者对象的提示信息包括以下内容：\n   - 引用者对象的完整名称（使用get_full_name方法获取）\n   - 引用者对象的文档（如果有多个版本，则取最新版本的文档内容；如果没有文档，则显示\"None\"）\n   - 引用者对象的原始代码（如果有代码内容，则显示代码内容；否则显示\"None\"）\n   - 分隔符\"==========\"\n5. 将每个引用者对象的提示信息添加到prompt列表中。\n6. 使用换行符将prompt列表中的所有提示信息连接起来作为返回值。\n\n**Note**: get_referencer_prompt函数适用于获取引用了当前对象的其他对象的提示信息，可以用于查找当前对象的引用者或者生成引用关系的可视化图表。\n\n**Output Example**: 假设当前对象被两个其他对象引用，引用者对象的完整名称分别为obj1和obj2，引用者对象的文档分别为\"文档1\"和\"文档2\"，引用者对象的原始代码分别为\"代码1\"和\"代码2\"，则调用get_referencer_prompt函数的返回值为：\n```\nAlso, the code has been referenced by the following objects, their code and docs are as following:\nobj: obj1\nDocument: 文档1\nRaw code:\n代码1\n==========\nobj: obj2\nDocument: 文档2\nRaw code:\n代码2\n==========\n```"
      ],
      "code_start_line": 120,
      "code_end_line": 127,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been referenced by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: {referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/doc_meta_info.py/DocItem",
        "repo_agent/doc_meta_info.py/DocItem/get_full_name"
      ]
    }
  },
  "repo_agent/prompt.py": {},
  "repo_agent/change_detector.py": {
    "ChangeDetector": {
      "type": "ClassDef",
      "name": "ChangeDetector",
      "md_content": [
        "**ChangeDetector**: ChangeDetector的功能是检测代码仓库中的变动。\n\n**属性**：该类具有一个属性`repo_path`，表示代码仓库的路径。\n\n**代码描述**：ChangeDetector类是一个用于检测代码仓库变动的工具类。它提供了一些方法来获取代码仓库中的变动信息。\n\n- `__init__(self, repo_path)`：初始化ChangeDetector对象。接受一个参数`repo_path`，表示代码仓库的路径。该方法会将`repo_path`赋值给`self.repo_path`属性，并使用`git.Repo(repo_path)`创建一个Git仓库对象赋值给`self.repo`属性。\n\n- `get_staged_pys(self)`：获取已暂存的Python文件。该方法会返回一个字典，其中键为文件路径，值为布尔值，表示文件是否为新创建的文件。\n\n- `get_file_diff(self, file_path, is_new_file)`：获取指定文件的变动内容。该方法接受两个参数，`file_path`表示文件的相对路径，`is_new_file`表示文件是否为新创建的文件。根据`is_new_file`的值，该方法使用不同的方式获取文件的变动内容，并返回一个包含变动内容的列表。\n\n- `parse_diffs(self, diffs)`：解析变动内容，提取添加和删除的对象信息。该方法接受一个包含变动内容的列表`diffs`，并将变动内容解析为添加和删除的行信息，返回一个包含添加和删除行信息的字典。\n\n- `identify_changes_in_structure(self, changed_lines, structures)`：识别发生变动的函数或类的结构。该方法接受两个参数，`changed_lines`为发生变动的行号信息，`structures`为函数或类的结构信息。该方法会遍历所有发生变动的行，对于每一行，它会检查该行是否在某个结构（函数或类）的起始行和结束行之间，如果是，则将该结构及其父结构的名称添加到结果字典`changes_in_structures`中。\n\n- `get_to_be_staged_files(self)`：获取待暂存的文件。该方法会返回一个列表，包含满足条件的未暂存文件的路径。\n\n- `add_unstaged_files(self)`：将待暂存的文件添加到暂存区。该方法会将满足条件的未暂存文件添加到暂存区。\n\n**注意**：在使用`get_staged_pys`方法时，需要先确保已经初始化了Git仓库对象`self.repo`。\n\n**输出示例**：一个可能的返回值的示例：\n```\n{\n    'file1.py': True,\n    'file2.py': False\n}\n```"
      ],
      "code_start_line": 12,
      "code_end_line": 229,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChangeDetector:\n    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n\n\n    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n\n    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n    \n    \n    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.\n    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.\n    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.\n    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n    \n    # TODO:可能有错，需要单元测试覆盖； 可能有更好的实现方式\n    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n\n    \n    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py",
        "repo_agent/runner.py/Runner/__init__",
        "tests/test_change_detector.py",
        "tests/test_change_detector.py/TestChangeDetector/test_get_staged_pys",
        "tests/test_change_detector.py/TestChangeDetector/test_get_unstaged_mds",
        "tests/test_change_detector.py/TestChangeDetector/test_add_unstaged_mds"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化一个ChangeDetector对象。\n**参数**: 这个函数的参数。\n**代码描述**: 这个函数的描述。\n__init__函数接受一个参数repo_path，它是一个字符串类型，表示仓库的路径。这个函数没有返回值。在函数体内部，将传入的repo_path赋值给self.repo_path，然后使用git.Repo函数将repo_path作为参数创建一个git仓库对象，并将其赋值给self.repo。\n\n**注意**: 使用这段代码时需要注意的事项。"
      ],
      "code_start_line": 13,
      "code_end_line": 24,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "repo_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_staged_pys": {
      "type": "FunctionDef",
      "name": "get_staged_pys",
      "md_content": [
        "**get_staged_pys**: get_staged_pys函数的功能是获取已暂存的代码库中新增的Python文件。\n\n**参数**: 该函数没有参数。\n\n**代码描述**: 该函数通过GitPython库检测已暂存的代码库中新增的Python文件。它首先获取代码库对象repo，然后创建一个空字典staged_files用于存储新增的Python文件。接下来，通过调用repo.index.diff(\"HEAD\", R=True)获取已暂存的变更列表diffs。然后，遍历diffs列表，对于变更类型为\"A\"或\"M\"且文件后缀为\".py\"的变更，将文件路径作为键，是否为新增文件作为值，添加到staged_files字典中。最后，返回staged_files字典。\n\n**注意**: 该函数只追踪已暂存的Python文件的变更，即使用`git add`添加的文件。在GitPython库中，repo.index.diff('HEAD')将暂存区（index）作为新状态与原始HEAD提交（旧状态）进行比较。这意味着如果当前暂存区有一个新文件，它将显示为在HEAD中不存在，即\"deleted\"。R=True参数反转了这个逻辑，将最后一次提交（HEAD）作为旧状态，并将其与当前暂存区（新状态）（Index）进行比较。在这种情况下，暂存区中的新文件将正确显示为已添加，因为它在HEAD中不存在。\n\n**输出示例**: \n```python\n{\n    'path/to/file1.py': True,\n    'path/to/file2.py': False\n}\n```"
      ],
      "code_start_line": 26,
      "code_end_line": 50,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "tests/test_change_detector.py/TestChangeDetector/test_get_staged_pys"
      ],
      "reference_who": []
    },
    "get_file_diff": {
      "type": "FunctionDef",
      "name": "get_file_diff",
      "md_content": [
        "**get_file_diff**: get_file_diff函数的功能是检索特定文件的更改。对于新文件，它使用git diff --staged命令获取差异。\n**参数**：\n- file_path (str): 文件的相对路径\n- is_new_file (bool): 表示文件是否为新文件\n**代码说明**：\n该函数首先获取self.repo对象，然后根据is_new_file的值来判断是新文件还是已存在的文件。如果是新文件，则先将其添加到暂存区，然后从暂存区获取差异。如果是已存在的文件，则从HEAD获取差异。最后，将差异以列表形式返回。\n**注意**：\n- 该函数依赖于self.repo对象，因此在调用之前需要确保self.repo已经初始化。\n**输出示例**：\n['diff --git a/repo_agent/change_detector.py b/repo_agent/change_detector.py', 'index 5e9e3c0..f2d9e6b 100644', '--- a/repo_agent/change_detector.py', '+++ b/repo_agent/change_detector.py', '@@ -1,5 +1,6 @@', ' import subprocess', ' import os', ' import git', '+import sys', ' ', ' from repo_agent.file_handler import FileHandler', ' from repo_agent.logger import logger']"
      ],
      "code_start_line": 53,
      "code_end_line": 75,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "file_path",
        "is_new_file"
      ],
      "have_return": true,
      "code_content": "    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    },
    "parse_diffs": {
      "type": "FunctionDef",
      "name": "parse_diffs",
      "md_content": [
        "**parse_diffs**: parse_diffs函数的功能是解析差异内容，提取出添加和删除的对象信息，这些对象可以是类或函数。\n**参数**: diffs (list): 包含差异内容的列表。通过类内的get_file_diff()函数获取。\n**代码描述**: 这个函数用于解析差异内容，提取出添加和删除的行信息。函数首先初始化了一个字典changed_lines，用于存储添加和删除的行信息。然后通过遍历diffs列表，逐行解析差异内容。对于每一行，函数首先使用正则表达式匹配行号信息，如果匹配成功，则更新当前行号和变更行号。然后根据行的开头字符判断是添加行还是删除行，并将对应的行号和内容添加到changed_lines字典中。对于没有变化的行，函数会将两个行号都增加1。最后，函数返回changed_lines字典，其中包含了添加和删除行的信息。\n**注意**: 对于修改的内容，也会被表示为添加操作。如果需要明确知道一个对象是新增的，需要使用get_added_objs()函数。\n**输出示例**: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}"
      ],
      "code_start_line": 77,
      "code_end_line": 115,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "diffs"
      ],
      "have_return": true,
      "code_content": "    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    },
    "identify_changes_in_structure": {
      "type": "FunctionDef",
      "name": "identify_changes_in_structure",
      "md_content": [
        "**identify_changes_in_structure**: identify_changes_in_structure函数的功能是识别发生更改的结构。它遍历所有更改的行，对于每一行，它检查该行是否在一个结构（函数或类）的起始行和结束行之间。如果是，则认为该结构发生了更改，并将其名称和父结构的名称添加到结果字典changes_in_structures的相应集合中（根据该行是添加还是删除而定）。\n\n**参数**：\n- changed_lines（dict）：包含发生更改的行号的字典，格式为{'added': [(行号, 更改内容)], 'removed': [(行号, 更改内容)]}\n- structures（list）：从get_functions_and_classes接收的是一个函数或类结构的列表，每个结构由结构类型、名称、起始行号、结束行号和父结构名称组成。\n\n**代码说明**：\n- 首先，创建一个空的字典changes_in_structures，用于存储发生更改的结构。\n- 然后，根据changed_lines中的更改类型（'added'或'removed'），遍历每个更改的行。\n- 对于每个更改的行，遍历structures中的每个结构，判断该行是否在结构的起始行和结束行之间。\n- 如果是，则将该结构的名称和父结构的名称添加到changes_in_structures字典的相应集合中。\n- 最后，返回changes_in_structures字典，其中包含发生更改的结构，键是更改类型，值是结构名称和父结构名称的集合。\n\n**注意**：\n- 该函数依赖于get_functions_and_classes函数的返回结果，需要确保传入正确的结构列表。\n- changed_lines参数中的行号应与源代码中的行号一致。\n\n**输出示例**：\n{'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}"
      ],
      "code_start_line": 121,
      "code_end_line": 148,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "changed_lines",
        "structures"
      ],
      "have_return": true,
      "code_content": "    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/process_file_changes"
      ],
      "reference_who": []
    },
    "get_to_be_staged_files": {
      "type": "FunctionDef",
      "name": "get_to_be_staged_files",
      "md_content": [
        "**get_to_be_staged_files**: get_to_be_staged_files函数的功能是检索仓库中所有未暂存的文件，这些文件满足以下条件之一：\n1. 将文件的扩展名更改为.md后，与已暂存的文件对应。\n2. 文件的路径与CONFIG中的'project_hierarchy'字段相同。\n\n它返回这些文件的路径列表。\n\n**参数**: 无参数。\n\n**代码描述**: 该函数首先获取已暂存的文件列表staged_files，然后获取CONFIG中的'project_hierarchy'字段。接下来，它通过self.repo.index.diff(None)获取所有未暂存的更改文件列表diffs，以及通过self.repo.untracked_files获取所有未跟踪文件列表untracked_files。然后，它遍历untracked_files和diffs列表，处理每个文件的路径，并根据条件判断是否将其加入到to_be_staged_files列表中。最后，函数返回to_be_staged_files列表。\n\n**注意**: \n- 函数中的路径处理使用了os.path模块的join和relpath方法。\n- 函数中的文件类型判断使用了endswith和splitext方法。\n\n**输出示例**: \n```\nstaged_files:['file1.py', 'file2.py']\nuntracked_files:['/absolute/path/file1.md', '/absolute/path/file2.md']\nrepo_path:/absolute/path/to/repo\nrel_untracked_file:file1.md\ncorresponding_py_file in untracked_files:file1.py\nrel_untracked_file:file2.md\ncorresponding_py_file in untracked_files:file2.py\nunstaged_files:['file3.py', 'file4.py']\nrel_unstaged_file:file3.py\ncorresponding_py_file:file3.py\nrel_unstaged_file:file4.py\ncorresponding_py_file:file4.py\n['file1.md', 'file2.md', 'file3.py', 'file4.py']\n```"
      ],
      "code_start_line": 151,
      "code_end_line": 218,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/change_detector.py/ChangeDetector/add_unstaged_files",
        "tests/test_change_detector.py/TestChangeDetector/test_get_unstaged_mds",
        "tests/test_change_detector.py/TestChangeDetector/test_add_unstaged_mds"
      ],
      "reference_who": []
    },
    "add_unstaged_files": {
      "type": "FunctionDef",
      "name": "add_unstaged_files",
      "md_content": [
        "**add_unstaged_files**: add_unstaged_files函数的功能是将满足条件的未暂存文件添加到暂存区。\n\n**参数**: 无参数。\n\n**代码描述**: 该函数首先调用self.get_to_be_staged_files()方法获取满足条件的未暂存文件列表unstaged_files_meeting_conditions。然后，它遍历unstaged_files_meeting_conditions列表，对每个文件路径进行处理，并使用subprocess模块运行git命令将文件添加到暂存区。最后，函数返回unstaged_files_meeting_conditions列表。\n\n**注意**: \n- 函数中的文件添加使用了subprocess模块运行git命令。\n- 函数中的文件路径处理使用了os.path模块的join方法。\n\n**输出示例**: \n```\nunstaged_files_meeting_conditions:['file1.md', 'file2.md', 'file3.py', 'file4.py']\n```"
      ],
      "code_start_line": 221,
      "code_end_line": 229,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/runner.py/Runner/process_file_changes",
        "tests/test_change_detector.py/TestChangeDetector/test_add_unstaged_mds"
      ],
      "reference_who": [
        "repo_agent/change_detector.py/ChangeDetector/get_to_be_staged_files"
      ]
    }
  },
  "repo_agent/project_manager.py": {
    "ProjectManager": {
      "type": "ClassDef",
      "name": "ProjectManager",
      "md_content": [
        "**ProjectManager**: ProjectManager的功能是管理项目的类。\n\n**attributes**: \n- repo_path: 项目的存储路径\n- project: 项目对象\n- project_hierarchy: 项目层次结构的路径\n\n**Code Description**: \nProjectManager类是一个用于管理项目的类。它的构造函数接受两个参数：repo_path和project_hierarchy。repo_path是项目的存储路径，而project_hierarchy是项目层次结构的路径。在构造函数中，它会初始化repo_path和project_hierarchy属性，并创建一个jedi.Project对象来表示项目。\n\nget_project_structure方法是ProjectManager类的一个方法，用于获取项目的结构。它内部定义了一个嵌套函数walk_dir，用于递归遍历项目路径下的所有文件和文件夹。walk_dir函数会将遍历到的文件和文件夹的名称添加到一个列表structure中。最后，get_project_structure方法会返回structure列表的字符串表示，即项目的结构。\n\n**Note**: \n- 该类的构造函数需要传入repo_path和project_hierarchy参数。\n- get_project_structure方法会返回项目的结构，以字符串形式表示。\n\n**Output Example**: \n如果项目的结构如下所示：\n- project_folder\n  - file1.py\n  - file2.py\n  - subfolder1\n    - file3.py\n  - subfolder2\n    - file4.py\n\n那么get_project_structure方法的返回值将是：\n```\nproject_folder\n  file1.py\n  file2.py\n  subfolder1\n    file3.py\n  subfolder2\n    file4.py\n```"
      ],
      "code_start_line": 4,
      "code_end_line": 25,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ProjectManager:\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n\n    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/chat_engine.py",
        "repo_agent/runner.py",
        "repo_agent/runner.py/Runner/__init__"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是初始化一个ProjectManager对象。\n**参数**: 这个函数的参数有两个：\n- repo_path: 一个字符串，表示仓库的路径。\n- project_hierarchy: 一个字符串，表示项目的层次结构。\n**代码描述**: 这个函数首先将传入的repo_path赋值给self.repo_path属性，然后使用jedi.Project函数创建一个Project对象，并将其赋值给self.project属性。接下来，将传入的project_hierarchy和\".project_hierarchy.json\"拼接起来，并将结果赋值给self.project_hierarchy属性。\n**注意**: 使用这段代码时需要注意以下几点：\n- repo_path参数应该是一个有效的仓库路径。\n- project_hierarchy参数应该是一个有效的项目层次结构。"
      ],
      "code_start_line": 5,
      "code_end_line": 8,
      "parent": "ProjectManager",
      "params": [
        "self",
        "repo_path",
        "project_hierarchy"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "get_project_structure": {
      "type": "FunctionDef",
      "name": "get_project_structure",
      "md_content": [
        "**get_project_structure**: get_project_structure函数的作用是获取项目的结构。\n**参数**: 该函数没有参数。\n**代码描述**: 该函数通过遍历项目目录，获取项目的结构信息，并将结果以字符串的形式返回。\n函数内部定义了一个名为walk_dir的内部函数，用于递归遍历目录。walk_dir函数接受两个参数，root表示当前遍历的目录，prefix表示当前目录的前缀，用于控制输出的格式。在遍历过程中，如果遇到隐藏文件或目录，则忽略不处理。如果遇到子目录，则递归调用walk_dir函数进行遍历。如果遇到以.py结尾的文件，则将文件名添加到结果列表中。\n在get_project_structure函数中，首先定义了一个空的列表structure，用于存储项目的结构信息。然后调用walk_dir函数，将项目根目录作为参数传入，开始遍历项目目录。最后，将结果列表转换为字符串，并使用换行符连接起来，作为函数的返回值。\n**注意**: 该函数依赖于os模块和os.path模块，需要确保这两个模块已经导入。\n**输出示例**: 假设项目的结构如下所示:\n- project_manager.py\n  - folder1\n    - file1.py\n    - file2.py\n  - folder2\n    - file3.py\n  - file4.py\n则函数的返回值为:\nproject_manager.py\n  folder1\n    file1.py\n    file2.py\n  folder2\n    file3.py\n  file4.py"
      ],
      "code_start_line": 10,
      "code_end_line": 25,
      "parent": "ProjectManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "walk_dir": {
      "type": "FunctionDef",
      "name": "walk_dir",
      "md_content": [
        "**walk_dir**: walk_dir函数的功能是遍历指定目录下的所有文件和子目录，并将它们的结构保存到一个列表中。\n**参数**: walk_dir函数接受两个参数：\n- root：指定的根目录路径。\n- prefix：可选参数，用于指定文件和子目录的前缀，默认为空字符串。\n**代码说明**: walk_dir函数首先将根目录的基本名称添加到结构列表中，然后根据指定的前缀生成新的前缀。接下来，它遍历根目录下的所有文件和子目录。如果遇到隐藏文件或目录（以'.'开头），则忽略它们。对于每个文件或子目录，它将其路径与根目录拼接，并判断是否为目录。如果是目录，则递归调用walk_dir函数，并将新的前缀作为参数传递。如果是文件且以'.py'结尾，则将带有新前缀的文件名添加到结构列表中。\n**注意**: 使用该代码时需要注意以下几点：\n- 确保传递正确的根目录路径作为参数。\n- 可以通过指定前缀来调整输出结果的格式。\n- 该函数只会遍历根目录下的文件和子目录，不会遍历更深层次的目录。"
      ],
      "code_start_line": 11,
      "code_end_line": 21,
      "parent": "get_project_structure",
      "params": [
        "root",
        "prefix"
      ],
      "have_return": false,
      "code_content": "        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  },
  "repo_agent/utils/gitignore_checker.py": {
    "GitignoreChecker": {
      "type": "ClassDef",
      "name": "GitignoreChecker",
      "md_content": [
        "**GitignoreChecker**: GitignoreChecker的功能是检查文件和文件夹是否被.gitignore文件忽略。\n\n**属性**: \n- directory (str): 要检查的目录路径。\n- gitignore_path (str): .gitignore文件的路径。\n- folder_patterns (list): 从.gitignore文件中提取的文件夹模式列表。\n- file_patterns (list): 从.gitignore文件中提取的文件模式列表。\n\n**代码描述**: \nGitignoreChecker类用于检查文件和文件夹是否被.gitignore文件忽略。在初始化时，需要指定要检查的目录路径和.gitignore文件的路径。该类会加载并解析.gitignore文件，并将模式分为文件夹模式和文件模式。然后，通过check_files_and_folders方法，可以检查给定目录中的所有文件和文件夹是否被忽略，并返回未被忽略且具有.py扩展名的文件路径列表。\n\n**注意**: \n- 在初始化GitignoreChecker对象时，需要提供正确的目录路径和.gitignore文件的路径。\n- .gitignore文件中的模式可以是文件夹模式（以/结尾）或文件模式。\n- check_files_and_folders方法返回的文件路径是相对于指定目录路径的相对路径。\n\n**输出示例**: \n```python\ngitignore_checker = GitignoreChecker(directory='/path/to/directory', gitignore_path='/path/to/.gitignore')\nnot_ignored_files = gitignore_checker.check_files_and_folders()\nprint(not_ignored_files)\n# Output: ['file1.py', 'file2.py', 'folder/file3.py']\n```"
      ],
      "code_start_line": 5,
      "code_end_line": 116,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class GitignoreChecker:\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py",
        "repo_agent/file_handler.py/FileHandler/generate_overall_structure"
      ],
      "reference_who": []
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: __init__函数的功能是使用特定的目录和.gitignore文件的路径来初始化GitignoreChecker。\n\n**参数**: \n- directory (str): 要检查的目录。\n- gitignore_path (str): .gitignore文件的路径。\n\n**代码描述**: \n该函数首先将传入的directory和gitignore_path赋值给self.directory和self.gitignore_path属性。然后，函数调用self._load_gitignore_patterns()方法来加载和解析.gitignore文件，并将解析后的模式分别赋值给self.folder_patterns和self.file_patterns属性。\n\n**注意**: \n- 使用这段代码时需要确保传入的directory和gitignore_path参数是有效的字符串。\n- 函数假设_load_gitignore_patterns方法已经定义并可用。\n\n"
      ],
      "code_start_line": 6,
      "code_end_line": 16,
      "parent": "GitignoreChecker",
      "params": [
        "self",
        "directory",
        "gitignore_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_load_gitignore_patterns"
      ]
    },
    "_load_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_load_gitignore_patterns",
      "md_content": [
        "**_load_gitignore_patterns**: _load_gitignore_patterns函数的功能是加载和解析.gitignore文件，然后将模式分割为文件夹模式和文件模式。\n\n**参数**: 该函数没有接受任何参数。\n\n**代码描述**: 该函数首先尝试使用指定的.gitignore文件路径打开文件，并读取文件内容。如果文件不存在，则会回退到默认路径。然后，函数调用_parse_gitignore函数解析gitignore_content，并将解析后的模式传递给_split_gitignore_patterns函数进行分割。最后，函数返回一个包含文件夹模式列表和文件模式列表的元组。\n\n**注意**: 使用这段代码时需要注意以下几点：\n- 函数假设传入的gitignore_path参数是有效的.gitignore文件路径。\n- 函数假设_parse_gitignore和_split_gitignore_patterns函数已经定义并可用。\n\n**输出示例**: 假设.gitignore文件的内容为：\n```\n# 忽略以.pyc结尾的文件\n*.pyc\n\n# 忽略名为__pycache__的目录\n__pycache__\n```\n则函数的返回值为(['__pycache__'], ['*.pyc'])。"
      ],
      "code_start_line": 18,
      "code_end_line": 37,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/__init__"
      ],
      "reference_who": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_parse_gitignore",
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_split_gitignore_patterns"
      ]
    },
    "_parse_gitignore": {
      "type": "FunctionDef",
      "name": "_parse_gitignore",
      "md_content": [
        "**_parse_gitignore**: _parse_gitignore函数的功能是解析.gitignore文件的内容，并将模式作为列表返回。\n**参数**: gitignore_content (str) - .gitignore文件的内容。\n**代码描述**: 该函数通过遍历.gitignore文件的每一行，将非空且不以'#'开头的行作为模式添加到列表中，并返回该列表。\n**注意**: 该函数假设传入的gitignore_content参数是有效的.gitignore文件内容。\n**输出示例**: \n```python\ngitignore_content = \"\"\"\n# Ignore files with .pyc extension\n*.pyc\n\n# Ignore directories named __pycache__\n__pycache__\n\"\"\"\n\n_parse_gitignore(gitignore_content)\n# Output: ['*.pyc', '__pycache__']\n```"
      ],
      "code_start_line": 40,
      "code_end_line": 55,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_content"
      ],
      "have_return": true,
      "code_content": "    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_load_gitignore_patterns"
      ],
      "reference_who": []
    },
    "_split_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_split_gitignore_patterns",
      "md_content": [
        "**_split_gitignore_patterns**: _split_gitignore_patterns函数的功能是将.gitignore模式分割为文件夹模式和文件模式。\n**参数**: 这个函数的参数是gitignore_patterns，一个包含.gitignore文件中的模式的列表。\n**代码描述**: 这个函数首先创建两个空列表，folder_patterns和file_patterns，用于存储文件夹模式和文件模式。然后，它遍历gitignore_patterns列表中的每个模式。如果模式以'/'结尾，说明它是文件夹模式，将其去除末尾的'/'后添加到folder_patterns列表中；否则，说明它是文件模式，直接将其添加到file_patterns列表中。最后，函数返回一个包含folder_patterns和file_patterns的元组。\n**注意**: 使用这段代码时需要注意以下几点：\n- gitignore_patterns参数必须是一个包含.gitignore模式的列表。\n- 函数返回的元组包含两个列表，一个是文件夹模式列表，一个是文件模式列表。\n**输出示例**: 假设gitignore_patterns为['folder/', 'file.txt']，则函数的返回值为(['folder'], ['file.txt'])。"
      ],
      "code_start_line": 58,
      "code_end_line": 75,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_patterns"
      ],
      "have_return": true,
      "code_content": "    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_load_gitignore_patterns"
      ],
      "reference_who": []
    },
    "_is_ignored": {
      "type": "FunctionDef",
      "name": "_is_ignored",
      "md_content": [
        "**_is_ignored**: _is_ignored函数的功能是检查给定的路径是否与任何模式匹配。\n**参数**: 这个函数的参数。\n- path (str): 要检查的路径。\n- patterns (list): 要检查的模式列表。\n- is_dir (bool): 如果路径是目录，则为True；否则为False。\n**代码说明**: 这个函数使用给定的模式列表逐个检查路径。如果路径与任何模式匹配，则返回True；否则返回False。\n- 首先，对于每个模式，使用fnmatch模块的fnmatch函数检查路径是否与模式匹配。\n- 如果is_dir为True，并且模式以'/'结尾，并且路径与去掉最后一个字符的模式匹配，则返回True。\n- 如果没有匹配的模式，则返回False。\n**注意**: 使用该代码时需要注意以下几点：\n- patterns参数应该是一个包含模式的列表。\n- path参数应该是一个字符串，表示要检查的路径。\n- is_dir参数是一个布尔值，用于指示路径是否是一个目录。\n**输出示例**: 模拟代码返回值的可能外观。\n- 如果路径与模式匹配，则返回True。\n- 如果路径与模式不匹配，则返回False。"
      ],
      "code_start_line": 78,
      "code_end_line": 95,
      "parent": "GitignoreChecker",
      "params": [
        "path",
        "patterns",
        "is_dir"
      ],
      "have_return": true,
      "code_content": "    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/check_files_and_folders"
      ],
      "reference_who": []
    },
    "check_files_and_folders": {
      "type": "FunctionDef",
      "name": "check_files_and_folders",
      "md_content": [
        "**check_files_and_folders**: check_files_and_folders函数的功能是检查给定目录中的所有文件和文件夹是否与分割的gitignore模式匹配。返回一个列表，其中包含未被忽略且具有'.py'扩展名的文件的路径。返回的文件路径是相对于self.directory的。\n\n**参数**: 这个函数的参数。\n- 无\n\n**代码说明**: 这个函数使用os.walk函数遍历给定目录中的所有文件和文件夹。对于每个文件和文件夹，它执行以下操作：\n- 使用self._is_ignored函数检查文件夹是否被忽略。如果文件夹不被忽略，则将其添加到dirs列表中。\n- 对于每个文件，它执行以下操作：\n  - 使用self._is_ignored函数检查文件是否被忽略，并且文件扩展名以'.py'结尾。如果文件不被忽略且具有'.py'扩展名，则将其添加到not_ignored_files列表中。\n\n**注意**: 使用该代码时需要注意以下几点：\n- 该函数依赖于self._is_ignored函数，因此在调用该函数之前，需要确保self._is_ignored函数已经定义并可用。\n- self.directory应该是一个字符串，表示要检查的目录路径。\n\n**输出示例**: 模拟代码返回值的可能外观。\n- ['/path/to/file1.py', '/path/to/file2.py', ...]"
      ],
      "code_start_line": 97,
      "code_end_line": 116,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "repo_agent/file_handler.py/FileHandler/generate_overall_structure"
      ],
      "reference_who": [
        "repo_agent/utils/gitignore_checker.py/GitignoreChecker/_is_ignored"
      ]
    }
  },
  "display/book_tools/generate_summary_from_book.py": {
    "create_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_readme_if_not_exist",
      "md_content": [
        "**create_readme_if_not_exist**: create_readme_if_not_exist函数的功能是检查给定目录下是否存在README.md文件，如果不存在则创建一个。\n\n**参数**: \n- dire: 字符串类型，表示目录的路径。\n\n**代码描述**: \n- 首先，使用os.path.join函数将目录路径和README.md文件名拼接起来，得到README.md文件的完整路径。\n- 然后，使用os.path.exists函数判断README.md文件是否存在。\n- 如果README.md文件不存在，则使用open函数以写入模式打开README.md文件。\n- 使用os.path.basename函数获取目录的名称，并将其作为标题写入README.md文件。\n- 最后，使用with语句自动关闭文件。\n\n**注意**: \n- 在使用该函数之前，需要确保os模块已经导入。\n- 该函数只会在给定目录下创建一个README.md文件，不会对子目录进行操作。"
      ],
      "code_start_line": 6,
      "code_end_line": 12,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "def create_readme_if_not_exist(dire):\n    readme_path = os.path.join(dire, 'README.md')\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, 'w') as readme_file:\n            dirname = os.path.basename(dire)\n            readme_file.write('# {}\\n'.format(dirname))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/output_markdown"
      ],
      "reference_who": []
    },
    "output_markdown": {
      "type": "FunctionDef",
      "name": "output_markdown",
      "md_content": [
        "**output_markdown**: output_markdown函数的功能是根据给定的目录生成Markdown文件的目录结构。\n\n**参数**: \n- dire: 字符串类型，表示目录的路径。\n- base_dir: 字符串类型，表示基准目录的路径。\n- output_file: 文件对象，表示输出的Markdown文件。\n- iter_depth: 整数类型，表示当前目录的迭代深度，默认值为0。\n\n**代码描述**: \n- 首先，使用os.listdir函数遍历给定目录下的所有文件和子目录。\n- 对于每个文件或子目录，使用os.path.join函数将目录路径和文件名拼接起来，得到文件或子目录的完整路径。\n- 如果是子目录，调用create_readme_if_not_exist函数检查是否存在README.md文件。\n- 然后，再次使用os.listdir函数遍历给定目录下的所有文件和子目录。\n- 对于每个文件或子目录，使用os.path.join函数将目录路径和文件名拼接起来，得到文件或子目录的完整路径。\n- 如果是子目录，首先检查该目录下是否存在README.md文件。\n- 如果存在README.md文件，使用os.path.relpath函数获取相对路径，并将文件名和相对路径写入输出的Markdown文件。\n- 如果是子目录，递归调用output_markdown函数处理子目录。\n- 如果是文件，并且文件名是Markdown文件，则根据迭代深度和文件名是否为'SUMMARY.md'或'README.md'来判断是否写入输出的Markdown文件。\n\n**注意**: \n- 在使用该函数之前，需要确保os模块已经导入。\n- 该函数会遍历给定目录下的所有文件和子目录，并根据文件的后缀和迭代深度来生成Markdown文件的目录结构。\n- 该函数会递归处理子目录，并将子目录的相对路径和文件名写入输出的Markdown文件。\n- 该函数会忽略SUMMARY.md和README.md文件，除非迭代深度为0且文件名为README.md。\n\n**调用示例**: \n- output_markdown('./books', './books', output_file)\n\n**引用对象**: \n- create_readme_if_not_exist: 用于检查给定目录下是否存在README.md文件，如果不存在则创建一个。\n- is_markdown_file: 用于判断给定的文件名是否为Markdown文件。"
      ],
      "code_start_line": 42,
      "code_end_line": 65,
      "parent": null,
      "params": [
        "dire",
        "base_dir",
        "output_file",
        "iter_depth"
      ],
      "have_return": false,
      "code_content": "def output_markdown(dire, base_dir, output_file, iter_depth=0):\n    for filename in os.listdir(dire):\n        print('add readme ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            create_readme_if_not_exist(file_or_path)\n\n    for filename in os.listdir(dire):\n        print('deal with ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            # Check if README.md exists in the directory\n            readme_path = os.path.join(file_or_path, 'README.md')\n            if os.path.exists(readme_path):\n                # If README.md exists, create a markdown link to it\n                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')\n                output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(filename, relative_path))\n            # Recursively call output_markdown for nested directories\n            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)\n        else:\n            if is_markdown_file(filename):\n                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:\n                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)\n                    output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(is_markdown_file(filename), relative_path))\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/main"
      ],
      "reference_who": [
        "display/book_tools/generate_summary_from_book.py/create_readme_if_not_exist",
        "display/book_tools/generate_summary_from_book.py/is_markdown_file"
      ]
    },
    "markdown_file_in_dir": {
      "type": "FunctionDef",
      "name": "markdown_file_in_dir",
      "md_content": [
        "**markdown_file_in_dir**: markdown_file_in_dir函数的功能是在指定的目录中查找是否存在markdown文件。\n**参数**: 这个函数的参数是dire，表示要搜索的目录路径。\n**代码描述**: 这个函数使用os.walk()函数遍历指定目录dire下的所有文件和子目录。然后使用正则表达式re.search()匹配文件名是否以\".md\"或\".markdown\"结尾，如果匹配成功则返回True。如果遍历完所有文件后都没有匹配成功，则返回False。\n**注意**: 使用这段代码时需要注意以下几点：\n- 需要导入os和re模块。\n- 参数dire必须是一个有效的目录路径。\n**输出示例**: 假设在指定目录中存在一个名为\"example.md\"的markdown文件，则函数的返回值为True。"
      ],
      "code_start_line": 69,
      "code_end_line": 74,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": true,
      "code_content": "def markdown_file_in_dir(dire):\n    for root, dirs, files in os.walk(dire):\n        for filename in files:\n            if re.search('.md$|.markdown$', filename):\n                return True\n    return False\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "is_markdown_file": {
      "type": "FunctionDef",
      "name": "is_markdown_file",
      "md_content": [
        "**is_markdown_file**: is_markdown_file函数的功能是判断给定的文件名是否为Markdown文件。\n**参数**: 这个函数的参数是filename，表示要判断的文件名。\n**代码描述**: 这个函数首先使用正则表达式搜索文件名中是否包含'.md'或'.markdown'的后缀，如果没有找到匹配的结果，则返回False。如果找到了匹配的结果，根据匹配结果的长度来判断文件名的后缀是'.md'还是'.markdown'，然后根据不同的情况返回相应的结果。\n**注意**: 使用这个函数时需要传入一个文件名作为参数，函数会根据文件名的后缀判断是否为Markdown文件，并返回相应的结果。\n**输出示例**: \n- 对于文件名为'example.md'的情况，函数会返回'example'。\n- 对于文件名为'example.markdown'的情况，函数会返回'example'。\n- 对于其他文件名的情况，函数会返回False。"
      ],
      "code_start_line": 77,
      "code_end_line": 84,
      "parent": null,
      "params": [
        "filename"
      ],
      "have_return": true,
      "code_content": "def is_markdown_file(filename):\n    match = re.search('.md$|.markdown$', filename)\n    if not match:\n        return False\n    elif len(match.group()) is len('.md'):\n        return filename[:-3]\n    elif len(match.group()) is len('.markdown'):\n        return filename[:-9]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "display/book_tools/generate_summary_from_book.py/output_markdown"
      ],
      "reference_who": []
    },
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: main函数的功能是根据给定的书名生成Markdown文件的目录结构。\n\n**参数**: \n- 无\n\n**代码描述**: \n- 首先，通过sys.argv[1]获取命令行参数中的书名。\n- 然后，使用os.path.join函数将书名和'src'拼接起来，得到目录的路径。\n- 使用os.path.exists函数检查目录是否存在，如果不存在则使用os.makedirs函数创建目录。\n- 确保目录存在后，使用os.path.join函数将目录路径和'SUMMARY.md'拼接起来，得到输出文件的路径。\n- 使用open函数创建输出文件对象，并以写入模式打开。\n- 向输出文件写入Markdown文件的标题。\n- 调用output_markdown函数生成Markdown文件的目录结构。\n- 打印提示信息。\n- 返回0。\n\n**注意**: \n- 在使用该函数之前，需要确保sys和os模块已经导入。\n- 该函数会根据给定的书名生成目录的路径，并创建目录。\n- 该函数会生成Markdown文件的标题，并调用output_markdown函数生成Markdown文件的目录结构。\n- 该函数会打印提示信息，并返回0。\n\n**调用示例**: \n- main()\n\n**引用对象**: \n- output_markdown: 根据给定的目录生成Markdown文件的目录结构。\n\n**输出示例**: \n- GitBook自动生成目录完成:)"
      ],
      "code_start_line": 87,
      "code_end_line": 109,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    book_name = sys.argv[1]\n\n    # mkdir the book folder\n    dir_input = os.path.join('./books', book_name, 'src')\n\n    # check the dst_dir\n    if not os.path.exists(dir_input):\n        print(dir_input)\n        os.makedirs(dir_input)\n    # Ensure the directory exists or create it\n    if not os.path.exists(dir_input):\n        os.makedirs(dir_input)\n\n    # Then proceed to create the file\n    output_path = os.path.join(dir_input, 'SUMMARY.md')\n    output = open(output_path, 'w')\n    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')\n    output.write('# Summary\\n\\n')\n    output_markdown(dir_input, dir_input, output)\n\n    print('GitBook auto summary finished:) ')\n    return 0\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "display/book_tools/generate_summary_from_book.py/output_markdown"
      ]
    }
  },
  "display/book_tools/generate_repoagent_books.py": {
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [
        "**main**: main函数的功能是将指定的Markdown文档文件夹复制到指定的书籍文件夹中，并创建书籍的README.md文件。\n**parameters**: 该函数没有参数。\n**Code Description**: \n- 首先，从命令行参数中获取Markdown文档文件夹的路径、书籍名称和仓库路径。\n- 然后，创建书籍文件夹的目标路径dst_dir，该路径为'./books'下的书籍名称文件夹下的'src'文件夹。\n- 接着，获取Markdown文档文件夹的源路径docs_dir，该路径为仓库路径下的Markdown文档文件夹。\n- 如果目标路径dst_dir不存在，则创建该路径，并打印提示信息。\n- 遍历Markdown文档文件夹中的每个文件或文件夹：\n  - 获取源路径src_path和目标路径dst_path。\n  - 如果源路径src_path是一个文件夹，则使用shutil.copytree函数将其复制到目标路径dst_path，并打印提示信息。\n  - 如果源路径src_path是一个文件，则使用shutil.copy2函数将其复制到目标路径dst_path，并打印提示信息。\n- 定义了一个名为create_book_readme_if_not_exist的内部函数，用于创建书籍的README.md文件。\n- 调用create_book_readme_if_not_exist函数，传入目标路径dst_dir作为参数，如果书籍的README.md文件不存在，则创建该文件，并写入书籍名称作为标题。\n**Note**: \n- 在使用该函数之前，需要在命令行中传入三个参数，分别为Markdown文档文件夹的路径、书籍名称和仓库路径。\n- 该函数会将Markdown文档文件夹中的所有文件和文件夹复制到指定的书籍文件夹中，并创建书籍的README.md文件。\n- 如果目标路径dst_dir已经存在，则不会再次创建该路径。\n- 如果书籍的README.md文件已经存在，则不会再次创建该文件。"
      ],
      "code_start_line": 7,
      "code_end_line": 44,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    markdown_docs_folder = sys.argv[1]\n    book_name = sys.argv[2]\n    repo_path = sys.argv[3]\n\n    # mkdir the book folder\n    dst_dir = os.path.join('./books', book_name, 'src')\n    docs_dir = os.path.join(repo_path, markdown_docs_folder)\n\n    # check the dst_dir\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(\"mkdir %s\" % dst_dir)\n\n    # cp the Markdown_Docs_folder to dst_dir\n    for item in os.listdir(docs_dir):\n        src_path = os.path.join(docs_dir, item)\n        dst_path = os.path.join(dst_dir, item)\n\n        # check the src_path\n        if os.path.isdir(src_path):\n            # if the src_path is a folder, use shutil.copytree to copy\n            shutil.copytree(src_path, dst_path)\n            print(\"copytree %s to %s\" % (src_path, dst_path))\n        else:\n            # if the src_path is a file, use shutil.copy2 to copy\n            shutil.copy2(src_path, dst_path)\n            print(\"copy2 %s to %s\" % (src_path, dst_path))\n\n    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n\n    # create book README.md if not exist\n    create_book_readme_if_not_exist(dst_dir)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    },
    "create_book_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_book_readme_if_not_exist",
      "md_content": [
        "**create_book_readme_if_not_exist**: create_book_readme_if_not_exist函数的功能是检查指定目录下是否存在README.md文件，如果不存在则创建一个，并在文件中写入书名。\n**参数**: 这个函数的参数是dire，表示目录路径。\n**代码描述**: 这个函数首先通过os.path.join函数将目录路径和README.md文件名拼接起来，得到README.md文件的完整路径。然后通过os.path.exists函数判断该路径下是否存在README.md文件。如果不存在，则使用open函数以写入模式打开README.md文件，并使用write函数向文件中写入书名。\n**注意**: 使用这段代码时需要注意目录路径的正确性，确保指定的目录存在。另外，书名需要提前定义并传入函数中。"
      ],
      "code_start_line": 36,
      "code_end_line": 41,
      "parent": "main",
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": []
    }
  }
}