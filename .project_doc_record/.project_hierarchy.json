{
  "setup.py": {},
  "tests/test_change_detector.py": {
    "TestChangeDetector": {
      "type": "ClassDef",
      "name": "TestChangeDetector",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 89,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class TestChangeDetector(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n\n    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n\n\n    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n\n\n    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n\n\n    @classmethod\n    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "setUpClass": {
      "type": "FunctionDef",
      "name": "setUpClass",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 32,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "test_get_staged_pys": {
      "type": "FunctionDef",
      "name": "test_get_staged_pys",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 48,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "test_get_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_get_unstaged_mds",
      "md_content": [],
      "code_start_line": 51,
      "code_end_line": 64,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "test_add_unstaged_mds": {
      "type": "FunctionDef",
      "name": "test_add_unstaged_mds",
      "md_content": [],
      "code_start_line": 67,
      "code_end_line": 82,
      "parent": "TestChangeDetector",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "tearDownClass": {
      "type": "FunctionDef",
      "name": "tearDownClass",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 89,
      "parent": "TestChangeDetector",
      "params": [
        "cls"
      ],
      "have_return": false,
      "code_content": "    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "tests/test_structure_tree.py": {
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [],
      "code_start_line": 4,
      "code_end_line": 31,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 6,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 29,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/runner.py": {
    "need_to_generate": {
      "type": "FunctionDef",
      "name": "need_to_generate",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 36,
      "parent": null,
      "params": [
        "doc_item",
        "ignore_list"
      ],
      "have_return": true,
      "code_content": "def need_to_generate(doc_item: DocItem, ignore_list: List) -> bool:\n    \"\"\"只生成item的，文件及更高粒度都跳过。另外如果属于一个blacklist的文件也跳过\"\"\"\n    if doc_item.item_status == DocItemStatus.doc_up_to_date:\n        return False\n    rel_file_path = doc_item.get_full_name()\n    if doc_item.item_type in [DocItemType._file, DocItemType._dir, DocItemType._repo]:\n        return False\n    doc_item = doc_item.father\n    while doc_item:\n        if doc_item.item_type == DocItemType._file:\n            # 如果当前文件在忽略列表中，或者在忽略列表某个文件路径下，则跳过\n            if any(rel_file_path.startswith(ignore_item) for ignore_item in ignore_list):\n                return False\n            return True\n        doc_item = doc_item.father\n    return False\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "load_whitelist": {
      "type": "FunctionDef",
      "name": "load_whitelist",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 47,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def load_whitelist():\n    if CONFIG[\"whitelist_path\"] != None:\n        assert os.path.exists(CONFIG[\"whitelist_path\"]), f\"whitelist_path must be a json-file,and must exists: {CONFIG['whitelist_path']}\"\n        with open(CONFIG[\"whitelist_path\"], \"r\") as reader:\n            white_list_json_data = json.load(reader)\n        # for i in range(len(white_list_json_data)):\n        #     white_list_json_data[i][\"file_path\"] = white_list_json_data[i][\"file_path\"].replace(\"https://github.com/huggingface/transformers/blob/v4.36.1/\",\"\")\n        return white_list_json_data\n    else:\n        return None\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "Runner": {
      "type": "ClassDef",
      "name": "Runner",
      "md_content": [],
      "code_start_line": 49,
      "code_end_line": 458,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class Runner:\n    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        self.runner_lock = threading.Lock()\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n    \n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        try:\n            rel_file_path = doc_item.get_full_name()\n\n            ignore_list = CONFIG.get('ignore_list', [])\n            if not need_to_generate(doc_item, ignore_list):\n                logger.info(f\"内容被忽略/文档已生成，跳过：{doc_item.get_full_name()}\")\n            else:\n                logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...\")\n                file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                response_message = self.chat_engine.generate_doc(\n                    doc_item = doc_item,\n                    file_handler = file_handler,\n                )\n                doc_item.md_content.append(response_message.content)\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n                self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        except Exception as e:\n            logger.info(f\" 多次尝试后生成文档失败，跳过：{doc_item.get_full_name()}\")\n            logger.info(\"Error:\", e)\n            doc_item.item_status = DocItemStatus.doc_has_not_been_generated\n\n        \n\n    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(check_task_available_func) #将按照此顺序生成文档\n        # topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"成功生成了 {before_task_len - len(task_manager.task_dict)} 个文档\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {before_task_len - len(task_manager.task_dict)} docs are generated at this time\")\n\n    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        with self.runner_lock:\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n                if recursive_check(file_item) == False:\n                    # logger.info(f\"不存在文档内容，跳过：{file_item.get_full_name()}\")\n                    continue\n                rel_file_path = file_item.get_full_name()\n                # file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n                    \n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n                # 写入markdown内容到.md文件\n                file_path = os.path.join(CONFIG['Markdown_Docs_folder'], file_item.get_file_name().replace('.py', '.md'))\n                if file_path.startswith('/'):\n                    # 移除开头的 '/'\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, 'w', encoding='utf-8') as file:\n                    file.write(markdown)\n\n            logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n\n    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n\n\n    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(self.meta_info.target_repo_hierarchical_tree,task_available_func=check_task_available_func)\n        self.meta_info.print_task_list([cont.extra_info for cont in task_manager.task_dict.values()])\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        \n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n        \n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\") # 提交变更\n         \n\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n    \n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n\n\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 62,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.project_manager = ProjectManager(repo_path=CONFIG['repo_path'],project_hierarchy=CONFIG['project_hierarchy']) \n        self.change_detector = ChangeDetector(repo_path=CONFIG['repo_path'])\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy'])):\n            self.meta_info = MetaInfo.init_from_project_path(CONFIG['repo_path'])\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']))\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        self.runner_lock = threading.Lock()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_all_pys": {
      "type": "FunctionDef",
      "name": "get_all_pys",
      "md_content": [],
      "code_start_line": 64,
      "code_end_line": 81,
      "parent": "Runner",
      "params": [
        "self",
        "directory"
      ],
      "have_return": true,
      "code_content": "    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "generate_doc_for_a_single_item": {
      "type": "FunctionDef",
      "name": "generate_doc_for_a_single_item",
      "md_content": [],
      "code_start_line": 84,
      "code_end_line": 106,
      "parent": "Runner",
      "params": [
        "self",
        "doc_item"
      ],
      "have_return": false,
      "code_content": "    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\n        \"\"\"\n        try:\n            rel_file_path = doc_item.get_full_name()\n\n            ignore_list = CONFIG.get('ignore_list', [])\n            if not need_to_generate(doc_item, ignore_list):\n                logger.info(f\"内容被忽略/文档已生成，跳过：{doc_item.get_full_name()}\")\n            else:\n                logger.info(f\" -- 正在生成{doc_item.get_full_name()} 对象文档...\")\n                file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                response_message = self.chat_engine.generate_doc(\n                    doc_item = doc_item,\n                    file_handler = file_handler,\n                )\n                doc_item.md_content.append(response_message.content)\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n                self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n        except Exception as e:\n            logger.info(f\" 多次尝试后生成文档失败，跳过：{doc_item.get_full_name()}\")\n            logger.info(\"Error:\", e)\n            doc_item.item_status = DocItemStatus.doc_has_not_been_generated\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "first_generate": {
      "type": "FunctionDef",
      "name": "first_generate",
      "md_content": [],
      "code_start_line": 110,
      "code_end_line": 141,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def first_generate(self):\n        \"\"\"\n        生成所有文档,\n        如果生成结束，self.meta_info.document_version会变成0(之前是-1)\n        每生成一个obj的doc，会实时同步回文件系统里。如果中间报错了，下次会自动load，按照文件顺序接着生成。\n        **注意**：这个生成first_generate的过程中，目标仓库代码不能修改。也就是说，一个document的生成过程必须绑定代码为一个版本。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(check_task_available_func) #将按照此顺序生成文档\n        # topology_list = [item for item in topology_list if need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n        \n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']))\n            logger.info(f\"成功生成了 {before_task_len - len(task_manager.task_dict)} 个文档\")\n\n        except BaseException as e:\n            logger.info(f\"Finding an error as {e}, {before_task_len - len(task_manager.task_dict)} docs are generated at this time\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "markdown_refresh": {
      "type": "FunctionDef",
      "name": "markdown_refresh",
      "md_content": [],
      "code_start_line": 143,
      "code_end_line": 186,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def markdown_refresh(self):\n        \"\"\"将目前最新的document信息写入到一个markdown格式的文件夹里(不管markdown内容是不是变化了)\n        \"\"\"\n        with self.runner_lock:\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n                if recursive_check(file_item) == False:\n                    # logger.info(f\"不存在文档内容，跳过：{file_item.get_full_name()}\")\n                    continue\n                rel_file_path = file_item.get_full_name()\n                # file_handler = FileHandler(CONFIG['repo_path'], rel_file_path)\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n                    \n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"markdown内容为空，文件路径为{rel_file_path}\"\n                # 写入markdown内容到.md文件\n                file_path = os.path.join(CONFIG['Markdown_Docs_folder'], file_item.get_file_name().replace('.py', '.md'))\n                if file_path.startswith('/'):\n                    # 移除开头的 '/'\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, 'w', encoding='utf-8') as file:\n                    file.write(markdown)\n\n            logger.info(f\"markdown document has been refreshed at {CONFIG['Markdown_Docs_folder']}\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "recursive_check": {
      "type": "FunctionDef",
      "name": "recursive_check",
      "md_content": [],
      "code_start_line": 149,
      "code_end_line": 155,
      "parent": "markdown_refresh",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "                def recursive_check(doc_item: DocItem) -> bool: #检查一个file内是否存在doc\n                    if doc_item.md_content != []:\n                        return True\n                    for _,child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n",
      "name_column": 20,
      "item_status": "doc_has_not_been_generated"
    },
    "to_markdown": {
      "type": "FunctionDef",
      "name": "to_markdown",
      "md_content": [],
      "code_start_line": 161,
      "code_end_line": 170,
      "parent": "markdown_refresh",
      "params": [
        "item",
        "now_level"
      ],
      "have_return": true,
      "code_content": "                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += \"#\"*now_level + f\" {item.item_type.name} {item.obj_name}\"\n                    if \"params\" in item.content.keys() and len(item.content[\"params\"]) > 0:\n                        markdown_content += f\"({', '.join(item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(item.md_content) >0 else 'Doc has not been generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level+1)\n                    return markdown_content\n",
      "name_column": 20,
      "item_status": "doc_has_not_been_generated"
    },
    "git_commit": {
      "type": "FunctionDef",
      "name": "git_commit",
      "md_content": [],
      "code_start_line": 188,
      "code_end_line": 192,
      "parent": "Runner",
      "params": [
        "self",
        "commit_message"
      ],
      "have_return": false,
      "code_content": "    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(['git', 'commit', '--no-verify', '-m', commit_message])\n        except subprocess.CalledProcessError as e:\n            print(f'An error occurred while trying to commit {str(e)}')\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "run": {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [],
      "code_start_line": 195,
      "code_end_line": 251,
      "parent": "Runner",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\": \n            # 根据document version自动检测是否仍在最初生成的process里\n            self.first_generate()\n            self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'], CONFIG['project_hierarchy']), flash_reference_relation=True)\n            return\n\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            new_meta_info = MetaInfo.init_from_project_path(CONFIG[\"repo_path\"])\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        # 处理任务队列\n        ignore_list = CONFIG.get('ignore_list', [])\n        check_task_available_func = partial(need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(self.meta_info.target_repo_hierarchical_tree,task_available_func=check_task_available_func)\n        self.meta_info.print_task_list([cont.extra_info for cont in task_manager.task_dict.values()])\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [threading.Thread(target=worker, args=(task_manager,process_id, self.generate_doc_for_a_single_item)) for process_id in range(CONFIG[\"max_thread_count\"])]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(target_dir_path=os.path.join(CONFIG['repo_path'],CONFIG['project_hierarchy']), flash_reference_relation=True)\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "add_new_item": {
      "type": "FunctionDef",
      "name": "add_new_item",
      "md_content": [],
      "code_start_line": 254,
      "code_end_line": 284,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler",
        "json_data"
      ],
      "have_return": false,
      "code_content": "    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for structure_type, name, start_line, end_line, parent, params in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(structure_type, name, start_line, end_line, parent, params)\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n        # 将markdown内容写入.md文件\n        file_handler.write_file(os.path.join(self.project_manager.repo_path, CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "process_file_changes": {
      "type": "FunctionDef",
      "name": "process_file_changes",
      "md_content": [],
      "code_start_line": 287,
      "code_end_line": 335,
      "parent": "Runner",
      "params": [
        "self",
        "repo_path",
        "file_path",
        "is_new_file"
      ],
      "have_return": false,
      "code_content": "    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path) # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(self.change_detector.get_file_diff(file_path, is_new_file))\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(changed_lines, file_handler.get_functions_and_classes(source_code))\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n        \n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n        \n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(json_data[file_handler.file_path], file_handler, changes_in_pyfile)\n            # 将更新后的file写回到json文件中\n            with open(self.project_manager.project_hierarchy, 'w', encoding='utf-8') as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n            \n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(file_path=file_handler.file_path)\n            # 将markdown内容写入.md文件\n            file_handler.write_file(os.path.join(CONFIG['Markdown_Docs_folder'], file_handler.file_path.replace('.py', '.md')), markdown)\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler,json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n        \n        if len(git_add_result) > 0:\n            logger.info(f'已添加 {[file for file in git_add_result]} 到暂存区')\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "update_existing_item": {
      "type": "FunctionDef",
      "name": "update_existing_item",
      "md_content": [],
      "code_start_line": 341,
      "code_end_line": 412,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "changes_in_pyfile"
      ],
      "have_return": true,
      "code_content": "    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj: # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path) \n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\"code_start_line\"]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\"code_end_line\"]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\"name_column\"]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile['added']:\n            for current_object in current_objects.values(): # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if obj_name == current_object[\"name\"]:  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"]\n                        )\n                    }\n                    referencer_list.append(referencer_obj) # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile['added']: # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if changed_obj[0] == ref_obj[\"obj_name\"]: # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(self.update_object, file_dict, file_handler, changed_obj[0], ref_obj[\"obj_referencer_list\"])\n                        logger.info(f\"正在生成 {file_handler.file_path}中的{changed_obj[0]} 对象文档...\")\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "update_object": {
      "type": "FunctionDef",
      "name": "update_object",
      "md_content": [],
      "code_start_line": 415,
      "code_end_line": 431,
      "parent": "Runner",
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "obj_name",
        "obj_referencer_list"
      ],
      "have_return": false,
      "code_content": "    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(obj, file_handler, obj_referencer_list)\n            obj[\"md_content\"] = response_message.content\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_new_objects": {
      "type": "FunctionDef",
      "name": "get_new_objects",
      "md_content": [],
      "code_start_line": 435,
      "code_end_line": 458,
      "parent": "Runner",
      "params": [
        "self",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = file_handler.get_functions_and_classes(previous_version) if previous_version else []\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/file_handler.py": {
    "FileHandler": {
      "type": "ClassDef",
      "name": "FileHandler",
      "md_content": [],
      "code_start_line": 15,
      "code_end_line": 311,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class FileHandler:\n    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n\n    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n\n    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n\n\n    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n        \n    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                # if node.name == \"recursive_check\":\n                #     import pdb; pdb.set_trace()\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                def get_recursive_parent_name(node):\n                    now = node\n                    while \"parent\" in dir(now):\n                        if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                            assert 'name' in dir(now.parent)\n                            return now.parent.name\n                        now = now.parent\n                    return None\n                parent_name = get_recursive_parent_name(node)\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                all_names = [item[1] for item in functions_and_classes]\n                # (parent_name == None or parent_name in all_names) and \n                if node.name not in all_names:\n                    #必须父亲名字正确传入里面，并且自己不和某个已有value重名\n                    functions_and_classes.append(\n                        (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                    )\n                else:\n                    logger.info(f\"circle-definition dected, skipped: {type(node).__name__}: {node.name}\")\n        return functions_and_classes\n        \n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n    \n\n    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n    \n\n    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef']:\n                params_str = '()'\n                if obj['params']:\n                    params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 19,
      "parent": "FileHandler",
      "params": [
        "self",
        "repo_path",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, file_path):\n        self.file_path = file_path # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(repo_path, CONFIG['project_hierarchy'], \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "read_file": {
      "type": "FunctionDef",
      "name": "read_file",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 32,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_obj_code_info": {
      "type": "FunctionDef",
      "name": "get_obj_code_info",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 76,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_type",
        "code_name",
        "start_line",
        "end_line",
        "parent",
        "params",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def get_obj_code_info(self, code_type, code_name, start_line, end_line, parent, params, file_path = None):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['parent'] = parent\n        code_info['params'] = params\n\n        with open(os.path.join(self.repo_path, file_path if file_path != None else self.file_path), 'r', encoding='utf-8') as code_file:\n            lines = code_file.readlines()\n            code_content = ''.join(lines[start_line-1:end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line-1].find(code_name)\n            # 判断代码中是否有return字样\n            if 'return' in code_content:\n                have_return = True\n            else:  \n                have_return = False\n            \n            code_info['have_return'] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info['code_content'] = code_content\n            code_info['name_column'] = name_column\n                \n        return code_info\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "write_file": {
      "type": "FunctionDef",
      "name": "write_file",
      "md_content": [],
      "code_start_line": 78,
      "code_end_line": 94,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path",
        "content"
      ],
      "have_return": false,
      "code_content": "    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith('/'):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n            \n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_modified_file_versions": {
      "type": "FunctionDef",
      "name": "get_modified_file_versions",
      "md_content": [],
      "code_start_line": 97,
      "code_end_line": 121,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, 'r', encoding='utf-8') as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (commit.tree / self.file_path).data_stream.read().decode('utf-8')\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_end_lineno": {
      "type": "FunctionDef",
      "name": "get_end_lineno",
      "md_content": [],
      "code_start_line": 123,
      "code_end_line": 141,
      "parent": "FileHandler",
      "params": [
        "self",
        "node"
      ],
      "have_return": true,
      "code_content": "    def get_end_lineno(self,node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, 'lineno'):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, 'end_lineno', None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "add_parent_references": {
      "type": "FunctionDef",
      "name": "add_parent_references",
      "md_content": [],
      "code_start_line": 143,
      "code_end_line": 155,
      "parent": "FileHandler",
      "params": [
        "self",
        "node",
        "parent"
      ],
      "have_return": false,
      "code_content": "    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_functions_and_classes": {
      "type": "FunctionDef",
      "name": "get_functions_and_classes",
      "md_content": [],
      "code_start_line": 157,
      "code_end_line": 198,
      "parent": "FileHandler",
      "params": [
        "self",
        "code_content"
      ],
      "have_return": true,
      "code_content": "    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                # if node.name == \"recursive_check\":\n                #     import pdb; pdb.set_trace()\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                def get_recursive_parent_name(node):\n                    now = node\n                    while \"parent\" in dir(now):\n                        if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                            assert 'name' in dir(now.parent)\n                            return now.parent.name\n                        now = now.parent\n                    return None\n                parent_name = get_recursive_parent_name(node)\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else []\n                all_names = [item[1] for item in functions_and_classes]\n                # (parent_name == None or parent_name in all_names) and \n                if node.name not in all_names:\n                    #必须父亲名字正确传入里面，并且自己不和某个已有value重名\n                    functions_and_classes.append(\n                        (type(node).__name__, node.name, start_line, end_line, parent_name, parameters)\n                    )\n                else:\n                    logger.info(f\"circle-definition dected, skipped: {type(node).__name__}: {node.name}\")\n        return functions_and_classes\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_recursive_parent_name": {
      "type": "FunctionDef",
      "name": "get_recursive_parent_name",
      "md_content": [],
      "code_start_line": 179,
      "code_end_line": 186,
      "parent": "get_functions_and_classes",
      "params": [
        "node"
      ],
      "have_return": true,
      "code_content": "                def get_recursive_parent_name(node):\n                    now = node\n                    while \"parent\" in dir(now):\n                        if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                            assert 'name' in dir(now.parent)\n                            return now.parent.name\n                        now = now.parent\n                    return None\n",
      "name_column": 20,
      "item_status": "doc_has_not_been_generated"
    },
    "generate_file_structure": {
      "type": "FunctionDef",
      "name": "generate_file_structure",
      "md_content": [],
      "code_start_line": 200,
      "code_end_line": 237,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n        \n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path,file_path), 'r', encoding='utf-8') as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = {}\n            for struct in structures:\n                structure_type, name, start_line, end_line, parent, params = struct\n                code_info = self.get_obj_code_info(structure_type, name, start_line, end_line, parent, params, file_path)\n                file_objects[name] = code_info\n\n        return file_objects\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "generate_overall_structure": {
      "type": "FunctionDef",
      "name": "generate_overall_structure",
      "md_content": [],
      "code_start_line": 240,
      "code_end_line": 258,
      "parent": "FileHandler",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def generate_overall_structure(self) -> dict:\n        \"\"\"\n        Generate the overall structure of the repository.\n\n        Returns:\n            dict: A dictionary representing the structure of the repository.\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(directory=self.repo_path,\n                                            gitignore_path=os.path.join(self.repo_path, '.gitignore'))\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            try:\n                repo_structure[not_ignored_files] = self.generate_file_structure(not_ignored_files)\n            except Exception as e:\n                print(f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\")\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "convert_to_markdown_file": {
      "type": "FunctionDef",
      "name": "convert_to_markdown_file",
      "md_content": [],
      "code_start_line": 261,
      "code_end_line": 311,
      "parent": "FileHandler",
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n        \n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(f\"No file object found for {self.file_path} in project_hierarchy.json\")\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = ''\n            if obj['type'] in ['FunctionDef', 'AsyncFunctionDef']:\n                params_str = '()'\n                if obj['params']:\n                    params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/config.py": {},
  "repo_agent/multi_task_dispatch.py": {
    "Task": {
      "type": "ClassDef",
      "name": "Task",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 14,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class Task:\n    def __init__(self, task_id: int, dependencies: List[Task],extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0 #任务状态：0未开始，1正在进行，2已经完成，3出错了\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 14,
      "parent": "Task",
      "params": [
        "self",
        "task_id",
        "dependencies",
        "extra_info"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, task_id: int, dependencies: List[Task],extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0 #任务状态：0未开始，1正在进行，2已经完成，3出错了\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "TaskManager": {
      "type": "ClassDef",
      "name": "TaskManager",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 55,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class TaskManager:\n    def __init__(self):\n        self.task_dict: Dict[int, Task]  = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n\n    @property\n    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n\n    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        with self.task_lock:\n            denp_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(task_id=self.now_id, dependencies=denp_tasks, extra_info=extra)\n            self.now_id += 1\n            return self.now_id - 1\n\n    def get_next_task(self, process_id: int):\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (len(self.task_dict[task_id].dependencies) == 0) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    logger.info(f\"[{process_id}] get task_id {task_id}, remain task: {len(self.task_dict)}\")\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n        \n    def mark_completed(self, task_id: int):\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "all_success": {
      "type": "FunctionDef",
      "name": "all_success",
      "md_content": [],
      "code_start_line": 26,
      "code_end_line": 27,
      "parent": "TaskManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "add_task": {
      "type": "FunctionDef",
      "name": "add_task",
      "md_content": [],
      "code_start_line": 29,
      "code_end_line": 34,
      "parent": "TaskManager",
      "params": [
        "self",
        "dependency_task_id",
        "extra"
      ],
      "have_return": true,
      "code_content": "    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        with self.task_lock:\n            denp_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(task_id=self.now_id, dependencies=denp_tasks, extra_info=extra)\n            self.now_id += 1\n            return self.now_id - 1\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_next_task": {
      "type": "FunctionDef",
      "name": "get_next_task",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 47,
      "parent": "TaskManager",
      "params": [
        "self",
        "process_id"
      ],
      "have_return": true,
      "code_content": "    def get_next_task(self, process_id: int):\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (len(self.task_dict[task_id].dependencies) == 0) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    logger.info(f\"[{process_id}] get task_id {task_id}, remain task: {len(self.task_dict)}\")\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "mark_completed": {
      "type": "FunctionDef",
      "name": "mark_completed",
      "md_content": [],
      "code_start_line": 49,
      "code_end_line": 55,
      "parent": "TaskManager",
      "params": [
        "self",
        "task_id"
      ],
      "have_return": false,
      "code_content": "    def mark_completed(self, task_id: int):\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "worker": {
      "type": "FunctionDef",
      "name": "worker",
      "md_content": [],
      "code_start_line": 59,
      "code_end_line": 69,
      "parent": null,
      "params": [
        "task_manager",
        "process_id",
        "handler"
      ],
      "have_return": true,
      "code_content": "def worker(task_manager, process_id: int, handler: Callable):\n    while True:\n        if task_manager.all_success:\n            return\n        task, task_id = task_manager.get_next_task(process_id)\n        if task is None: \n            time.sleep(0.5)\n            continue\n        # print(f\"will perform task: {task_id}\")\n        handler(task.extra_info)\n        task_manager.mark_completed(task.task_id)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "some_function": {
      "type": "FunctionDef",
      "name": "some_function",
      "md_content": [],
      "code_start_line": 76,
      "code_end_line": 77,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "    def some_function(): #随机睡一会\n        time.sleep(random.random()*3)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/doc_meta_info.py": {
    "EdgeType": {
      "type": "ClassDef",
      "name": "EdgeType",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 24,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class EdgeType(Enum):\n    reference_edge = auto() #一个obj引用另一个obj\n    subfile_edge = auto() # 一个 文件/文件夹 属于一个文件夹\n    file_item_edge = auto() #一个 obj 属于一个文件\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "DocItemType": {
      "type": "ClassDef",
      "name": "DocItemType",
      "md_content": [],
      "code_start_line": 28,
      "code_end_line": 63,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItemType(Enum):\n    _repo = auto() #根节点，需要生成readme\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_function = auto()\n    _function = auto() #文件内的常规function\n    _sub_function = auto() #function内的定义的subfunction\n    _global_var = auto()\n\n    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n\n    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n\n    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "to_str": {
      "type": "FunctionDef",
      "name": "to_str",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 48,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "print_self": {
      "type": "FunctionDef",
      "name": "print_self",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 60,
      "parent": "DocItemType",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.BLUE\n        elif self == DocItemType._function:\n            color = Fore.RED\n        return color + self.name + Style.RESET_ALL\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_edge_type": {
      "type": "FunctionDef",
      "name": "get_edge_type",
      "md_content": [],
      "code_start_line": 62,
      "code_end_line": 63,
      "parent": "DocItemType",
      "params": [
        "from_item_type",
        "to_item_type"
      ],
      "have_return": false,
      "code_content": "    def get_edge_type(from_item_type: DocItemType, to_item_type: DocItemType) -> EdgeType:\n        pass\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "DocItemStatus": {
      "type": "ClassDef",
      "name": "DocItemStatus",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 71,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "class DocItemStatus(Enum):\n    doc_up_to_date = auto() #无需生成文档\n    doc_has_not_been_generated = auto() #文档还未生成，需要生成\n    code_changed = auto() #源码被修改了，需要改文档\n    add_new_referencer = auto() #添加了新的引用者\n    referencer_not_exist = auto() #曾经引用他的obj被删除了，或者不再引用他了\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "DocItem": {
      "type": "ClassDef",
      "name": "DocItem",
      "md_content": [],
      "code_start_line": 75,
      "code_end_line": 194,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class DocItem():\n    item_type: DocItemType = DocItemType._class_function\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n\n    obj_name: str = \"\" #对象的名字\n    md_content: List[str] = field(default_factory=list) #存储不同版本的doc\n    content: Dict[Any,Any] = field(default_factory=dict) #原本存储的信息\n\n    children: Dict[str, DocItem] = field(default_factory=dict) #子对象\n    father: Any[DocItem] = None\n\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list) #一整条链路，从root开始\n    max_reference_ansce: Any[DocItem] = None\n\n    reference_who: List[DocItem] = field(default_factory=list) #他引用了谁\n    who_reference_me: List[DocItem] = field(default_factory=list) #谁引用了他\n\n    reference_who_name_list: List[str] = field(default_factory=list) #他引用了谁，这个可能是老版本的\n    who_reference_me_name_list: List[str] = field(default_factory=list) #谁引用了他，这个可能是老版本的\n\n    multithread_task_id: int = -1 #在多线程中的task_id\n\n    def __eq__(self, other) -> bool:\n        # 检查other是否是MyCustomClass的实例\n        if not isinstance(other, DocItem):\n            return False\n        if self.item_type != other.item_type:\n            return False\n        if self.obj_name != other.obj_name:\n            return False\n        return self.get_full_name() == other.get_full_name()\n\n\n    @staticmethod\n    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n    \n    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n    \n    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n\n\n    \n    @staticmethod\n    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n\n    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n\n    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n    \n    \n    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__eq__": {
      "type": "FunctionDef",
      "name": "__eq__",
      "md_content": [],
      "code_start_line": 98,
      "code_end_line": 106,
      "parent": "DocItem",
      "params": [
        "self",
        "other"
      ],
      "have_return": true,
      "code_content": "    def __eq__(self, other) -> bool:\n        # 检查other是否是MyCustomClass的实例\n        if not isinstance(other, DocItem):\n            return False\n        if self.item_type != other.item_type:\n            return False\n        if self.obj_name != other.obj_name:\n            return False\n        return self.get_full_name() == other.get_full_name()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "has_ans_relation": {
      "type": "FunctionDef",
      "name": "has_ans_relation",
      "md_content": [],
      "code_start_line": 110,
      "code_end_line": 116,
      "parent": "DocItem",
      "params": [
        "now_a",
        "now_b"
      ],
      "have_return": true,
      "code_content": "    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"node之间是否是祖先关系，有的话返回更早的节点\"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_travel_list": {
      "type": "FunctionDef",
      "name": "get_travel_list",
      "md_content": [],
      "code_start_line": 118,
      "code_end_line": 122,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_travel_list(self):\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "check_depth": {
      "type": "FunctionDef",
      "name": "check_depth",
      "md_content": [],
      "code_start_line": 124,
      "code_end_line": 133,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_depth(self):\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "find_min_ances": {
      "type": "FunctionDef",
      "name": "find_min_ances",
      "md_content": [],
      "code_start_line": 138,
      "code_end_line": 144,
      "parent": "DocItem",
      "params": [
        "node_a",
        "node_b"
      ],
      "have_return": true,
      "code_content": "    def find_min_ances(node_a: DocItem, node_b: DocItem):\n        pos = 0\n        assert node_a.tree_path[pos] == node_b.tree_path[pos]\n        while True:\n            pos += 1\n            if node_a.tree_path[pos] != node_b.tree_path[pos]:\n                return node_a.tree_path[pos - 1]\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "parse_tree_path": {
      "type": "FunctionDef",
      "name": "parse_tree_path",
      "md_content": [],
      "code_start_line": 146,
      "code_end_line": 149,
      "parent": "DocItem",
      "params": [
        "self",
        "now_path"
      ],
      "have_return": false,
      "code_content": "    def parse_tree_path(self, now_path):\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_file_name": {
      "type": "FunctionDef",
      "name": "get_file_name",
      "md_content": [],
      "code_start_line": 151,
      "code_end_line": 153,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_full_name": {
      "type": "FunctionDef",
      "name": "get_full_name",
      "md_content": [],
      "code_start_line": 154,
      "code_end_line": 165,
      "parent": "DocItem",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_full_name(self): \n        \"\"\"获取从下到上所有的obj名字\"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            name_list = [now.obj_name] + name_list\n            now = now.father\n        \n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "find": {
      "type": "FunctionDef",
      "name": "find",
      "md_content": [],
      "code_start_line": 168,
      "code_end_line": 179,
      "parent": "DocItem",
      "params": [
        "self",
        "recursive_file_path"
      ],
      "have_return": true,
      "code_content": "    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"从repo根节点根据path_list找到对应的文件, 否则返回False\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "print_recursive": {
      "type": "FunctionDef",
      "name": "print_recursive",
      "md_content": [],
      "code_start_line": 181,
      "code_end_line": 194,
      "parent": "DocItem",
      "params": [
        "self",
        "indent",
        "print_content"
      ],
      "have_return": true,
      "code_content": "    def print_recursive(self, indent=0, print_content = False):\n        \"\"\"递归打印repo对象\n        \"\"\"\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n        print(print_indent(indent) + f\"{self.item_type.print_self()}: {self.obj_name}\",end=\"\")\n        if len(self.children) > 0 :\n            print(f\", {len(self.children)} children\")\n        else:\n            print()\n        for child_name, child in self.children.items():\n            child.print_recursive(indent=indent+1, print_content=print_content)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "print_indent": {
      "type": "FunctionDef",
      "name": "print_indent",
      "md_content": [],
      "code_start_line": 184,
      "code_end_line": 187,
      "parent": "print_recursive",
      "params": [
        "indent"
      ],
      "have_return": true,
      "code_content": "        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \"*indent+\"|-\"\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "find_all_referencer": {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [],
      "code_start_line": 198,
      "code_end_line": 214,
      "parent": null,
      "params": [
        "repo_path",
        "variable_name",
        "file_path",
        "line_number",
        "column_number",
        "in_file_only"
      ],
      "have_return": true,
      "code_content": "def find_all_referencer(repo_path, variable_name, file_path, line_number, column_number, in_file_only=False):\n    \"\"\"复制过来的之前的实现\"\"\"\n    script = jedi.Script(path=os.path.join(repo_path, file_path))\n\n    try:\n        if in_file_only:\n            references = script.get_references(line=line_number, column=column_number, scope=\"file\")\n        else:\n            references = script.get_references(line=line_number, column=column_number)\n        # 过滤出变量名为 variable_name 的引用，并返回它们的位置\n        variable_references = [ref for ref in references if ref.name == variable_name]\n        return [(os.path.relpath(ref.module_path, repo_path), ref.line, ref.column) for ref in variable_references if not (ref.line == line_number and ref.column == column_number)]\n    except Exception as e:\n        # 打印错误信息和相关参数\n        print(f\"Error occurred: {e}\")\n        print(f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\")\n        return []\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "MetaInfo": {
      "type": "ClassDef",
      "name": "MetaInfo",
      "md_content": [],
      "code_start_line": 218,
      "code_end_line": 643,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class MetaInfo():\n    repo_path: str = \"\"\n    document_version: str = \"\" #随时间变化，\"\"代表没完成，否则对应一个目标仓库的commit hash\n    target_repo_hierarchical_tree: DocItem = field(default_factory=\"Docitem\") #整个repo的文件结构\n    white_list: Any[List] = None\n\n    in_generation_process: bool = False\n\n    checkpoint_lock: threading.Lock = threading.Lock()\n\n    @staticmethod\n    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n    \n    @staticmethod\n    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n\n    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        with self.checkpoint_lock:\n            logger.info(f\"will save MetaInfo at {target_dir_path}\")\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n            with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            \n            with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n                meta = {\n                    \"repo_path\": self.repo_path,\n                    \"doc_version\": self.document_version,\n                    \"in_generation_process\": self.in_generation_process,\n                }\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n    \n    \n    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n\n    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n\n    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\n        一个行属于这个obj的范围，并且没法属于他的儿子的范围了\"\"\"\n        now_node = file_node\n        assert now_node != None\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num and child.content[\"code_end_line\"] >= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n\n            \n\n    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = [], [] #如果指定白名单，只处理白名单上的双向引用关系\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            if white_list_file_names != [] and (file_node.get_file_name() not in white_list_file_names): #如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (now_obj.obj_name not in white_list_obj_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    if referencer_file_item == None:\n                        # import pdb; pdb.set_trace()\n                        logger.info(f\"Jedi find {referencer_file_ral_path} referenced {now_obj.get_full_name()}, which is not in the target repo\")\n                        continue\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n            # logger.info(f\"find {ref_count} refer-relation in {file_node.get_full_name()}\")\n    \n\n    def get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n            doc_items = list(filter(in_white_list, doc_items))\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total = len(items_by_depth),desc=\"parsing topology task-list\")\n        while items_by_depth:\n            min_break_level = 1e7\n            target_item = None\n            for item in items_by_depth:\n                now_break_level = 0\n                for referenced in item.reference_who:\n                    \"\"\"一个任务依赖于所有引用者和他的子节点。\n                    我们不能保证引用不成环(也许有些仓库的废代码会出现成环)。这时就只能选择一个相对来说遵守程度最好的了\"\"\"\n                    if not (referenced in deal_items):\n                        now_break_level += 1\n                if now_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = now_break_level\n                if now_break_level == 0:\n                    break\n            \n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(child.multithread_task_id)\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(referenced_item.multithread_task_id)\n            item_denp_task_ids = list(set(item_denp_task_ids)) #去重\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(dependency_task_id=item_denp_task_ids,extra=target_item)\n                target_item.multithread_task_id = task_id\n            deal_items.append(target_item)\n            items_by_depth.remove(target_item)\n            bar.update(1)\n            if min_break_level > 0:\n                print(f\"Reference becoming a circle: have a choose break-level={min_break_level}\")\n\n\n        # Further optimization for minimizing tree distance could be added here\n        return task_manager\n\n    def get_topology(self, task_available_func = None) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n        return task_manager\n    \n    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n\n    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference() \n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n    \n    def to_hierarchy_json(self, flash_reference_relation = False):\n        \"\"\"\n        如果flash_reference_relation=True,则会将最新的双向引用关系写回到meta文件中\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n\n    @staticmethod\n    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                nonlocal file_content\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    # assert file_content[value[\"parent\"]][\"name\"] != value[\"name\"]\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "init_from_project_path": {
      "type": "FunctionDef",
      "name": "init_from_project_path",
      "md_content": [],
      "code_start_line": 229,
      "code_end_line": 237,
      "parent": "MetaInfo",
      "params": [
        "project_abs_path"
      ],
      "have_return": true,
      "code_content": "    def init_from_project_path(project_abs_path: str) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n        project_abs_path = CONFIG['repo_path']\n        logger.info(f\"initializing a new meta-info from {project_abs_path}\")\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure()\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        return metainfo\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "from_checkpoint_path": {
      "type": "FunctionDef",
      "name": "from_checkpoint_path",
      "md_content": [],
      "code_start_line": 240,
      "code_end_line": 256,
      "parent": "MetaInfo",
      "params": [
        "checkpoint_dir_path"
      ],
      "have_return": true,
      "code_content": "    def from_checkpoint_path(checkpoint_dir_path: str) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(checkpoint_dir_path, \".project_hierarchy.json\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)        \n        \n        with open(os.path.join(checkpoint_dir_path, \"meta-info.json\"),'r', encoding=\"utf-8\") as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = meta_data[\"repo_path\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n\n        logger.info(f\"loading meta-info from {checkpoint_dir_path}, document-version=\\\"{metainfo.document_version}\\\"\")\n        return metainfo   \n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "checkpoint": {
      "type": "FunctionDef",
      "name": "checkpoint",
      "md_content": [],
      "code_start_line": 258,
      "code_end_line": 273,
      "parent": "MetaInfo",
      "params": [
        "self",
        "target_dir_path",
        "flash_reference_relation"
      ],
      "have_return": false,
      "code_content": "    def checkpoint(self, target_dir_path: str, flash_reference_relation=False):\n        with self.checkpoint_lock:\n            logger.info(f\"will save MetaInfo at {target_dir_path}\")\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            now_hierarchy_json = self.to_hierarchy_json(flash_reference_relation=flash_reference_relation)\n            with open(os.path.join(target_dir_path, \".project_hierarchy.json\"), \"w\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            \n            with open(os.path.join(target_dir_path, \"meta-info.json\"), \"w\") as writer:\n                meta = {\n                    \"repo_path\": self.repo_path,\n                    \"doc_version\": self.document_version,\n                    \"in_generation_process\": self.in_generation_process,\n                }\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "print_task_list": {
      "type": "FunctionDef",
      "name": "print_task_list",
      "md_content": [],
      "code_start_line": 276,
      "code_end_line": 284,
      "parent": "MetaInfo",
      "params": [
        "self",
        "item_list"
      ],
      "have_return": false,
      "code_content": "    def print_task_list(self, item_list):\n        from prettytable import PrettyTable\n        task_table = PrettyTable([\"task_id\",\"Doc Generation Reason\", \"Path\"])\n        task_count = 0\n        for k, item in enumerate(item_list):\n            task_table.add_row([task_count, item.item_status.name, item.get_full_name()])\n            task_count += 1\n        print(\"Remain tasks to be done\")\n        print(task_table)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_all_files": {
      "type": "FunctionDef",
      "name": "get_all_files",
      "md_content": [],
      "code_start_line": 286,
      "code_end_line": 295,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "walk_tree": {
      "type": "FunctionDef",
      "name": "walk_tree",
      "md_content": [],
      "code_start_line": 289,
      "code_end_line": 293,
      "parent": "get_all_files",
      "params": [
        "now_node"
      ],
      "have_return": false,
      "code_content": "        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "find_obj_with_lineno": {
      "type": "FunctionDef",
      "name": "find_obj_with_lineno",
      "md_content": [],
      "code_start_line": 298,
      "code_end_line": 313,
      "parent": "MetaInfo",
      "params": [
        "self",
        "file_node",
        "start_line_num"
      ],
      "have_return": true,
      "code_content": "    def find_obj_with_lineno(self, file_node, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\n        一个行属于这个obj的范围，并且没法属于他的儿子的范围了\"\"\"\n        now_node = file_node\n        assert now_node != None\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if child.content[\"code_start_line\"] <= start_line_num and child.content[\"code_end_line\"] >= start_line_num:\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child: \n                return now_node\n        return now_node\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "parse_reference": {
      "type": "FunctionDef",
      "name": "parse_reference",
      "md_content": [],
      "code_start_line": 317,
      "code_end_line": 377,
      "parent": "MetaInfo",
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\n        \"\"\"\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = [], [] #如果指定白名单，只处理白名单上的双向引用关系\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            if white_list_file_names != [] and (file_node.get_file_name() not in white_list_file_names): #如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (now_obj.obj_name not in white_list_obj_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    if referencer_file_item == None:\n                        # import pdb; pdb.set_trace()\n                        logger.info(f\"Jedi find {referencer_file_ral_path} referenced {now_obj.get_full_name()}, which is not in the target repo\")\n                        continue\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_node.children.items():\n                walk_file(child)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "walk_file": {
      "type": "FunctionDef",
      "name": "walk_file",
      "md_content": [],
      "code_start_line": 333,
      "code_end_line": 374,
      "parent": "parse_reference",
      "params": [
        "now_obj"
      ],
      "have_return": false,
      "code_content": "            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (now_obj.obj_name not in white_list_obj_names):\n                    in_file_only = True #作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list: #对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(referencer_file_ral_path.split(\"/\"))\n                    if referencer_file_item == None:\n                        # import pdb; pdb.set_trace()\n                        logger.info(f\"Jedi find {referencer_file_ral_path} referenced {now_obj.get_full_name()}, which is not in the target repo\")\n                        continue\n                    referencer_node = self.find_obj_with_lineno(referencer_file_item, referencer_pos[1])\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        # print(referencer_node.get_full_name())\n                        if now_obj not in referencer_node.reference_who:\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n\n                            min_ances = DocItem.find_min_ances(referencer_node, now_obj)\n                            if referencer_node.max_reference_ansce == None:\n                                referencer_node.max_reference_ansce = min_ances\n                            else: #是否更大\n                                if min_ances in referencer_node.max_reference_ansce.tree_path:\n                                    referencer_node.max_reference_ansce = min_ances\n\n                            ref_count += 1\n                # e = time.time()\n                # print(f\"遍历reference 用时: {e-s}\")\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n",
      "name_column": 16,
      "item_status": "doc_has_not_been_generated"
    },
    "get_task_manager": {
      "type": "FunctionDef",
      "name": "get_task_manager",
      "md_content": [],
      "code_start_line": 381,
      "code_end_line": 431,
      "parent": "MetaInfo",
      "params": [
        "self",
        "now_node",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_task_manager(self, now_node: DocItem, task_available_func: Callable = None) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\n        \"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n            doc_items = list(filter(in_white_list, doc_items))\n        items_by_depth = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total = len(items_by_depth),desc=\"parsing topology task-list\")\n        while items_by_depth:\n            min_break_level = 1e7\n            target_item = None\n            for item in items_by_depth:\n                now_break_level = 0\n                for referenced in item.reference_who:\n                    \"\"\"一个任务依赖于所有引用者和他的子节点。\n                    我们不能保证引用不成环(也许有些仓库的废代码会出现成环)。这时就只能选择一个相对来说遵守程度最好的了\"\"\"\n                    if not (referenced in deal_items):\n                        now_break_level += 1\n                if now_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = now_break_level\n                if now_break_level == 0:\n                    break\n            \n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(child.multithread_task_id)\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(referenced_item.multithread_task_id)\n            item_denp_task_ids = list(set(item_denp_task_ids)) #去重\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(dependency_task_id=item_denp_task_ids,extra=target_item)\n                target_item.multithread_task_id = task_id\n            deal_items.append(target_item)\n            items_by_depth.remove(target_item)\n            bar.update(1)\n            if min_break_level > 0:\n                print(f\"Reference becoming a circle: have a choose break-level={min_break_level}\")\n\n\n        # Further optimization for minimizing tree distance could be added here\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "in_white_list": {
      "type": "FunctionDef",
      "name": "in_white_list",
      "md_content": [],
      "code_start_line": 386,
      "code_end_line": 390,
      "parent": "get_task_manager",
      "params": [
        "item"
      ],
      "have_return": true,
      "code_content": "            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if item.get_file_name() == cont[\"file_path\"] and item.obj_name == cont[\"id_text\"]:\n                        return True\n                return False\n",
      "name_column": 16,
      "item_status": "doc_has_not_been_generated"
    },
    "get_topology": {
      "type": "FunctionDef",
      "name": "get_topology",
      "md_content": [],
      "code_start_line": 433,
      "code_end_line": 438,
      "parent": "MetaInfo",
      "params": [
        "self",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_topology(self, task_available_func = None) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\n        \"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(self.target_repo_hierarchical_tree,task_available_func=task_available_func)\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "_map": {
      "type": "FunctionDef",
      "name": "_map",
      "md_content": [],
      "code_start_line": 440,
      "code_end_line": 446,
      "parent": "MetaInfo",
      "params": [
        "self",
        "deal_func"
      ],
      "have_return": false,
      "code_content": "    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n        travel(self.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "travel": {
      "type": "FunctionDef",
      "name": "travel",
      "md_content": [],
      "code_start_line": 442,
      "code_end_line": 445,
      "parent": "_map",
      "params": [
        "now_item"
      ],
      "have_return": false,
      "code_content": "        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "load_doc_from_older_meta": {
      "type": "FunctionDef",
      "name": "load_doc_from_older_meta",
      "md_content": [],
      "code_start_line": 448,
      "code_end_line": 502,
      "parent": "MetaInfo",
      "params": [
        "self",
        "older_meta"
      ],
      "have_return": true,
      "code_content": "    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n\n\n        def travel(now_older_item: DocItem): #只寻找源码是否被修改的信息\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                # print(f\"return: {now_older_item.get_full_name()}\")\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if now_older_item.content[\"code_content\"] != result_item.content[\"code_content\"]: #源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference() \n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n        travel2(older_meta.target_repo_hierarchical_tree)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "find_item": {
      "type": "FunctionDef",
      "name": "find_item",
      "md_content": [],
      "code_start_line": 453,
      "code_end_line": 463,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_item"
      ],
      "have_return": true,
      "code_content": "        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"新版的meta中能不能找到原来的某个东西\"\"\"\n            nonlocal root_item\n            if now_item.father == None: #根节点永远能找到\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            if now_item.obj_name in father_find_result.children.keys():\n                return father_find_result.children[now_item.obj_name]\n            return None\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "travel2": {
      "type": "FunctionDef",
      "name": "travel2",
      "md_content": [],
      "code_start_line": 487,
      "code_end_line": 501,
      "parent": "load_doc_from_older_meta",
      "params": [
        "now_older_item"
      ],
      "have_return": true,
      "code_content": "        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item: #新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [name.get_full_name() for name in result_item.who_reference_me]\n            old_reference_names = now_older_item.who_reference_me_name_list\n\n            if not (set(new_reference_names) == set(old_reference_names)) and (result_item.item_status == DocItemStatus.doc_up_to_date):\n                if set(new_reference_names) <= set(old_reference_names): #旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "from_project_hierarchy_path": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_path",
      "md_content": [],
      "code_start_line": 506,
      "code_end_line": 516,
      "parent": "MetaInfo",
      "params": [
        "repo_path"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\n        \"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \".project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"怪\")\n        \n        with open(project_hierarchy_json_path,'r', encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "to_hierarchy_json": {
      "type": "FunctionDef",
      "name": "to_hierarchy_json",
      "md_content": [],
      "code_start_line": 518,
      "code_end_line": 549,
      "parent": "MetaInfo",
      "params": [
        "self",
        "flash_reference_relation"
      ],
      "have_return": true,
      "code_content": "    def to_hierarchy_json(self, flash_reference_relation = False):\n        \"\"\"\n        如果flash_reference_relation=True,则会将最新的双向引用关系写回到meta文件中\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = {}\n            \n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                file_hierarchy_content[now_obj.obj_name] = now_obj.content\n                file_hierarchy_content[now_obj.obj_name][\"name\"] = now_obj.obj_name\n                file_hierarchy_content[now_obj.obj_name][\"type\"] = now_obj.item_type.to_str()\n                file_hierarchy_content[now_obj.obj_name][\"md_content\"] = now_obj.md_content\n                file_hierarchy_content[now_obj.obj_name][\"item_status\"] = now_obj.item_status.name\n                \n                if flash_reference_relation:\n                    file_hierarchy_content[now_obj.obj_name][\"who_reference_me\"] = [cont.get_full_name() for cont in now_obj.who_reference_me]\n                    file_hierarchy_content[now_obj.obj_name][\"reference_who\"] = [cont.get_full_name() for cont in now_obj.reference_who]\n\n                file_hierarchy_content[now_obj.obj_name][\"parent\"] = None\n                if now_obj.father.item_type != DocItemType._file:\n                    file_hierarchy_content[now_obj.obj_name][\"parent\"] = now_obj.father.obj_name\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _,child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "from_project_hierarchy_json": {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_json",
      "md_content": [],
      "code_start_line": 552,
      "code_end_line": 643,
      "parent": "MetaInfo",
      "params": [
        "project_hierarchy_json"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem( #根节点\n                \n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in project_hierarchy_json.items(): \n            # 首先parse file archi\n            if not os.path.exists(os.path.join(CONFIG['repo_path'],file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG['repo_path'],file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure \n        \n            # 然后parse file内容\n            assert type(file_content) == dict\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                nonlocal file_content\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    # assert file_content[value[\"parent\"]][\"name\"] != value[\"name\"]\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n\n\n            item_reflection = {}\n            for key, value in file_content.items():\n                parse_one_item(key, value, item_reflection)\n            \n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "parse_one_item": {
      "type": "FunctionDef",
      "name": "parse_one_item",
      "md_content": [],
      "code_start_line": 596,
      "code_end_line": 634,
      "parent": "from_project_hierarchy_json",
      "params": [
        "key",
        "value",
        "item_reflection"
      ],
      "have_return": true,
      "code_content": "            def parse_one_item(key, value, item_reflection):\n                #递归parse，做过了就跳过，如果有father就先parse father\n                # print(f\"key: {key}\")\n                nonlocal file_content\n                if key in item_reflection.keys():\n                    return \n                if value[\"parent\"] != None:\n                    # print(f\"will parse father {value['parent']}\")\n                    # assert file_content[value[\"parent\"]][\"name\"] != value[\"name\"]\n                    parse_one_item(value[\"parent\"], file_content[value[\"parent\"]], item_reflection)\n\n                item_reflection[key] = DocItem(\n                                        obj_name=key,\n                                        content = value,\n                                        md_content=value[\"md_content\"],\n                                    )\n                if \"item_status\" in value.keys():\n                    item_reflection[key].item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    item_reflection[key].reference_who_name_list = value[\"reference_who\"]\n                if \"who_reference_me\" in value.keys():\n                    item_reflection[key].who_reference_me_name_list = value[\"who_reference_me\"]\n                if value[\"parent\"] != None:\n                    item_reflection[value[\"parent\"]].children[key] = item_reflection[key]\n                    item_reflection[key].father = item_reflection[value[\"parent\"]]\n                else:\n                    file_item.children[key] = item_reflection[key]\n                    item_reflection[key].father = file_item\n\n                if value[\"type\"] == \"ClassDef\":\n                    item_reflection[key].item_type = DocItemType._class\n                elif value[\"type\"] == \"FunctionDef\":\n                    item_reflection[key].item_type = DocItemType._function\n                    if value[\"parent\"] != None:\n                        parent_value = file_content[value[\"parent\"]]\n                        if parent_value[\"type\"] == \"FunctionDef\":\n                            item_reflection[key].item_type = DocItemType._sub_function\n                        elif parent_value[\"type\"] == \"ClassDef\":\n                            item_reflection[key].item_type = DocItemType._class_function\n",
      "name_column": 16,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/chat_engine.py": {
    "get_import_statements": {
      "type": "FunctionDef",
      "name": "get_import_statements",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 19,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [line for line in source_lines if line.strip().startswith('import') or line.strip().startswith('from')]\n    return import_lines\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "build_path_tree": {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 48,
      "parent": null,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "tree": {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [],
      "code_start_line": 22,
      "code_end_line": 23,
      "parent": "build_path_tree",
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "tree_to_string": {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 46,
      "parent": "build_path_tree",
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "ChatEngine": {
      "type": "ClassDef",
      "name": "ChatEngine",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 298,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChatEngine:\n    \"\"\"\n    ChatEngine is used to generate the doc of functions or classes.\n    \"\"\"\n    def __init__(self, CONFIG):\n        self.config = CONFIG\n\n    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                has_relationship = \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        has_relationship = get_relationship_description(referencer_content, reference_letter)\n\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            has_relationship=has_relationship,\n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        # # 保存prompt到txt文件\n        # with open(f'prompt_output/sys_prompt_{code_name}.txt', 'w', encoding='utf-8') as f:\n        #     f.write(sys_prompt+'\\n'+ usr_prompt)\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                # print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=\"\",\n                        project_structure=\"\", \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        has_relationship=has_relationship,\n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing related callers and callees...\")\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=\"\", \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            has_relationship=has_relationship,\n                            reference_letter=\"\", \n                            referencer_content=\"\",\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    return None\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 54,
      "code_end_line": 55,
      "parent": "ChatEngine",
      "params": [
        "self",
        "CONFIG"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, CONFIG):\n        self.config = CONFIG\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "num_tokens_from_string": {
      "type": "FunctionDef",
      "name": "num_tokens_from_string",
      "md_content": [],
      "code_start_line": 57,
      "code_end_line": 61,
      "parent": "ChatEngine",
      "params": [
        "self",
        "string",
        "encoding_name"
      ],
      "have_return": true,
      "code_content": "    def num_tokens_from_string(self, string: str, encoding_name = \"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "generate_doc": {
      "type": "FunctionDef",
      "name": "generate_doc",
      "md_content": [],
      "code_start_line": 63,
      "code_end_line": 298,
      "parent": "ChatEngine",
      "params": [
        "self",
        "doc_item",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        #print(\"len(referencer):\\n\",len(referencer))\n\n        # def get_code_from_json(json_file, referencer):\n        #     '''根据给出的referencer，找出其源码\n        #     '''\n        #     with open(json_file, 'r', encoding='utf-8') as f:\n        #         data = json.load(f)\n\n        #     code_from_referencer = {}\n        #     for ref in referencer:\n        #         file_path, line_number, _ = ref\n        #         if file_path in data:\n        #             objects = data[file_path]\n        #             min_obj = None\n        #             for obj_name, obj in objects.items():\n        #                 if obj['code_start_line'] <= line_number <= obj['code_end_line']:\n        #                     if min_obj is None or (obj['code_end_line'] - obj['code_start_line'] < min_obj['code_end_line'] - min_obj['code_start_line']):\n        #                         min_obj = obj\n        #             if min_obj is not None:\n        #                 if file_path not in code_from_referencer:\n        #                     code_from_referencer[file_path] = []\n        #                 code_from_referencer[file_path].append(min_obj['code_content'])\n        #     return code_from_referencer\n                \n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list    \n        file_path = doc_item.get_full_name()\n        doc_item_path = file_path + '/' + code_name\n\n        # 树结构路径通过全局信息中的who reference me 和 reference who + 自身的file_path来获取\n        project_structure = build_path_tree(who_reference_me,reference_who, doc_item_path)\n\n        # project_manager = ProjectManager(repo_path=file_handler.repo_path, project_hierarchy=file_handler.project_hierarchy)\n        # project_structure = project_manager.get_project_structure() \n        # file_path = os.path.join(file_handler.repo_path, file_handler.file_path)\n        # code_from_referencer = get_code_from_json(project_manager.project_hierarchy, referencer) # \n        # referenced = True if len(code_from_referencer) > 0 else False\n        # referencer_content = '\\n'.join([f'File_Path:{file_path}\\n' + '\\n'.join([f'Corresponding code as follows:\\n{code}\\n[End of this part of code]' for code in codes]) + f'\\n[End of {file_path}]' for file_path, codes in code_from_referencer.items()])\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                has_relationship = \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\n\n\n        # language\n        language = self.config[\"language\"]\n        if language not in language_mapping:\n            raise KeyError(f\"Language code {language} is not given! Supported languages are: {json.dumps(language_mapping)}\")\n        \n        language = language_mapping[language]\n        \n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        have_return_tell = \"**Output Example**: Mock up a possible appearance of the code's return value.\" if have_return else \"\"\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = \"and combine it with its calling situation in the project,\" if referenced else \"\"\n        \n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        has_relationship = get_relationship_description(referencer_content, reference_letter)\n\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        sys_prompt = SYS_PROMPT.format(\n            combine_ref_situation=combine_ref_situation, \n            file_path=file_path, \n            project_structure_prefix = project_structure_prefix,\n            project_structure=project_structure, \n            code_type_tell=code_type_tell, \n            code_name=code_name, \n            code_content=code_content, \n            have_return_tell=have_return_tell, \n            # referenced=referenced, \n            has_relationship=has_relationship,\n            reference_letter=reference_letter, \n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=language\n            )\n        \n        usr_prompt = USR_PROMPT.format(language=language)\n        # import pdb; pdb.set_trace()\n        # print(\"\\nsys_prompt:\\n\",sys_prompt)\n        # print(\"\\nusr_prompt:\\n\",str(usr_prompt))\n\n        # # 保存prompt到txt文件\n        # with open(f'prompt_output/sys_prompt_{code_name}.txt', 'w', encoding='utf-8') as f:\n        #     f.write(sys_prompt+'\\n'+ usr_prompt)\n\n        max_attempts = 5  # 设置最大尝试次数\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        code_max_length = 8192 - 1024 - 1\n        if model == \"gpt-3.5-turbo\":\n            code_max_length = 4096 - 1024 -1\n        # 检查tokens长度\n        if self.num_tokens_from_string(sys_prompt) + self.num_tokens_from_string(usr_prompt) >= code_max_length:\n            print(\"The code is too long, using gpt-3.5-turbo-16k to process it.\")\n            model = \"gpt-3.5-turbo-16k\"\n        \n        attempt = 0\n        while attempt < max_attempts:\n            try:\n                # 获取基本配置\n                client = OpenAI(\n                    api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"]\n                )\n\n                messages = [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}]\n                # print(f\"tokens of system-prompt={self.num_tokens_from_string(sys_prompt)}, user-prompt={self.num_tokens_from_string(usr_prompt)}\")\n                # print(f\"message:\\n{messages}\\n\")\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=1024\n                )\n\n                response_message = response.choices[0].message\n\n                # 如果 response_message 是 None，则继续下一次循环\n                if response_message is None:\n                    attempt += 1\n                    continue\n\n                # print(f\"\\nAnswer:\\n{response_message.content}\\n\")\n\n                return response_message\n            \n            except APIConnectionError as e:\n                print(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue # Try to request again\n\n            except BadRequestError as e:\n                # import pdb; pdb.set_trace()\n                if 'context_length_exceeded' in str(e):\n                    logger.info(f\"Error: The model's maximum context length is exceeded. Reducing the length of the messages. Attempt {attempt + 1} of {max_attempts}\")\n                    logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing project_structure...\")\n                    project_structure_prefix = ''\n                    project_structure = ''\n                    # Remove project_structure and project_structure_prefix\n                    sys_prompt = SYS_PROMPT.format(\n                        reference_letter=reference_letter, \n                        combine_ref_situation=combine_ref_situation, \n                        file_path=file_path, \n                        project_structure_prefix=\"\",\n                        project_structure=\"\", \n                        code_type_tell=code_type_tell, \n                        code_name=code_name, \n                        code_content=code_content, \n                        have_return_tell=have_return_tell, \n                        has_relationship=has_relationship,\n                        referenced=referenced, \n                        referencer_content=referencer_content,\n                        parameters_or_attribute=parameters_or_attribute,\n                        language=language\n                    )\n                                     \n                    attempt += 1\n                    if attempt >= 2:\n                        # Remove related callers and callees\n                        logger.info(f\"Length of sys_prompt: {len(sys_prompt)}, removing related callers and callees...\")\n                        referenced = False\n                        referencer_content = \"\"\n                        reference_letter = \"\"\n                        combine_ref_situation = \"\"\n\n                        sys_prompt = SYS_PROMPT.format(\n                            combine_ref_situation=\"\", \n                            file_path=file_path, \n                            project_structure_prefix = project_structure_prefix,\n                            project_structure=project_structure, \n                            code_type_tell=code_type_tell, \n                            code_name=code_name, \n                            code_content=code_content, \n                            have_return_tell=have_return_tell, \n                            # referenced=referenced, \n                            has_relationship=has_relationship,\n                            reference_letter=\"\", \n                            referencer_content=\"\",\n                            parameters_or_attribute=parameters_or_attribute,\n                            language=language\n                        )\n\n                    continue  # Try to request again\n                else:\n                    print(f\"An OpenAI error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n\n            except Exception as e:\n                print(f\"An unknown error occurred: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    return None\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_referenced_prompt": {
      "type": "FunctionDef",
      "name": "get_referenced_prompt",
      "md_content": [],
      "code_start_line": 110,
      "code_end_line": 117,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = f'''obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "get_referencer_prompt": {
      "type": "FunctionDef",
      "name": "get_referencer_prompt",
      "md_content": [],
      "code_start_line": 120,
      "code_end_line": 127,
      "parent": "generate_doc",
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = f'''obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```''' + \"=\"*10\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    },
    "get_relationship_description": {
      "type": "FunctionDef",
      "name": "get_relationship_description",
      "md_content": [],
      "code_start_line": 129,
      "code_end_line": 137,
      "parent": "generate_doc",
      "params": [
        "referencer_content",
        "reference_letter"
      ],
      "have_return": true,
      "code_content": "        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                has_relationship = \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/prompt.py": {},
  "repo_agent/change_detector.py": {
    "ChangeDetector": {
      "type": "ClassDef",
      "name": "ChangeDetector",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 229,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ChangeDetector:\n    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n\n\n    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n\n    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n    \n    \n    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.\n    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.\n    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.\n    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n    \n    # TODO:可能有错，需要单元测试覆盖； 可能有更好的实现方式\n    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n\n    \n    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 24,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "repo_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_staged_pys": {
      "type": "FunctionDef",
      "name": "get_staged_pys",
      "md_content": [],
      "code_start_line": 26,
      "code_end_line": 50,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n        \n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_file_diff": {
      "type": "FunctionDef",
      "name": "get_file_diff",
      "md_content": [],
      "code_start_line": 53,
      "code_end_line": 75,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "file_path",
        "is_new_file"
      ],
      "have_return": true,
      "code_content": "    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f'git -C {repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "parse_diffs": {
      "type": "FunctionDef",
      "name": "parse_diffs",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 115,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "diffs"
      ],
      "have_return": true,
      "code_content": "    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "identify_changes_in_structure": {
      "type": "FunctionDef",
      "name": "identify_changes_in_structure",
      "md_content": [],
      "code_start_line": 121,
      "code_end_line": 148,
      "parent": "ChangeDetector",
      "params": [
        "self",
        "changed_lines",
        "structures"
      ],
      "have_return": true,
      "code_content": "    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_to_be_staged_files": {
      "type": "FunctionDef",
      "name": "get_to_be_staged_files",
      "md_content": [],
      "code_start_line": 151,
      "code_end_line": 218,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(f\"staged_files:{staged_files}\")\n\n        project_hierarchy = CONFIG['project_hierarchy']\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"untracked_files:{untracked_files}\")\n        print(f\"repo_path:{self.repo_path}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            abs_untracked_file = os.path.join(self.repo_path, '/'+untracked_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_untracked_file = os.path.relpath(abs_untracked_file, self.repo_path)\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith('.md'):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(rel_untracked_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + '.py'\n                print(f\"corresponding_py_file in untracked_files:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_untracked_file))\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file) \n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"unstaged_files:{unstaged_files}\") # 虽然是从根目录开始的，但是最前头缺少一个 ' / ' ，所以还是会被解析为相对路径\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            abs_unstaged_file = os.path.join(self.repo_path, '/'+unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith('.md'):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(rel_unstaged_file, CONFIG['Markdown_Docs_folder'])\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + '.py'\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(os.path.join(self.repo_path.lstrip('/'), CONFIG['Markdown_Docs_folder'], rel_unstaged_file))\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file) \n\n        return to_be_staged_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "add_unstaged_files": {
      "type": "FunctionDef",
      "name": "add_unstaged_files",
      "md_content": [],
      "code_start_line": 221,
      "code_end_line": 229,
      "parent": "ChangeDetector",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f'git -C {self.repo.working_dir} add \"{file_path}\"'\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/project_manager.py": {
    "ProjectManager": {
      "type": "ClassDef",
      "name": "ProjectManager",
      "md_content": [],
      "code_start_line": 4,
      "code_end_line": 25,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class ProjectManager:\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n\n    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 8,
      "parent": "ProjectManager",
      "params": [
        "self",
        "repo_path",
        "project_hierarchy"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(self.repo_path, project_hierarchy, \".project_hierarchy.json\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "get_project_structure": {
      "type": "FunctionDef",
      "name": "get_project_structure",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 25,
      "parent": "ProjectManager",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_project_structure(self):\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return '\\n'.join(structure)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "walk_dir": {
      "type": "FunctionDef",
      "name": "walk_dir",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 21,
      "parent": "get_project_structure",
      "params": [
        "root",
        "prefix"
      ],
      "have_return": false,
      "code_content": "        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith('.'):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith('.py'):\n                    structure.append(new_prefix + name)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "repo_agent/utils/gitignore_checker.py": {
    "GitignoreChecker": {
      "type": "ClassDef",
      "name": "GitignoreChecker",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 116,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "class GitignoreChecker:\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated"
    },
    "__init__": {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 16,
      "parent": "GitignoreChecker",
      "params": [
        "self",
        "directory",
        "gitignore_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "_load_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_load_gitignore_patterns",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 37,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(os.path.dirname(__file__), '..', '..', '.gitignore')\n            with open(default_path, 'r', encoding='utf-8') as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "_parse_gitignore": {
      "type": "FunctionDef",
      "name": "_parse_gitignore",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 55,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_content"
      ],
      "have_return": true,
      "code_content": "    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith('#'):\n                patterns.append(line)\n        return patterns\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "_split_gitignore_patterns": {
      "type": "FunctionDef",
      "name": "_split_gitignore_patterns",
      "md_content": [],
      "code_start_line": 58,
      "code_end_line": 75,
      "parent": "GitignoreChecker",
      "params": [
        "gitignore_patterns"
      ],
      "have_return": true,
      "code_content": "    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith('/'):\n                folder_patterns.append(pattern.rstrip('/'))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "_is_ignored": {
      "type": "FunctionDef",
      "name": "_is_ignored",
      "md_content": [],
      "code_start_line": 78,
      "code_end_line": 95,
      "parent": "GitignoreChecker",
      "params": [
        "path",
        "patterns",
        "is_dir"
      ],
      "have_return": true,
      "code_content": "    def _is_ignored(path: str, patterns: list, is_dir: bool=False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith('/') and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    },
    "check_files_and_folders": {
      "type": "FunctionDef",
      "name": "check_files_and_folders",
      "md_content": [],
      "code_start_line": 97,
      "code_end_line": 116,
      "parent": "GitignoreChecker",
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [d for d in dirs if not self._is_ignored(d, self.folder_patterns, is_dir=True)]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(file, self.file_patterns) and file_path.endswith('.py'):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "display/book_tools/generate_summary_from_book.py": {
    "create_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_readme_if_not_exist",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 12,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "def create_readme_if_not_exist(dire):\n    readme_path = os.path.join(dire, 'README.md')\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, 'w') as readme_file:\n            dirname = os.path.basename(dire)\n            readme_file.write('# {}\\n'.format(dirname))\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "output_markdown": {
      "type": "FunctionDef",
      "name": "output_markdown",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 65,
      "parent": null,
      "params": [
        "dire",
        "base_dir",
        "output_file",
        "iter_depth"
      ],
      "have_return": false,
      "code_content": "def output_markdown(dire, base_dir, output_file, iter_depth=0):\n    for filename in os.listdir(dire):\n        print('add readme ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            create_readme_if_not_exist(file_or_path)\n\n    for filename in os.listdir(dire):\n        print('deal with ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            # Check if README.md exists in the directory\n            readme_path = os.path.join(file_or_path, 'README.md')\n            if os.path.exists(readme_path):\n                # If README.md exists, create a markdown link to it\n                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')\n                output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(filename, relative_path))\n            # Recursively call output_markdown for nested directories\n            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)\n        else:\n            if is_markdown_file(filename):\n                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:\n                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)\n                    output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(is_markdown_file(filename), relative_path))\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "markdown_file_in_dir": {
      "type": "FunctionDef",
      "name": "markdown_file_in_dir",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 74,
      "parent": null,
      "params": [
        "dire"
      ],
      "have_return": true,
      "code_content": "def markdown_file_in_dir(dire):\n    for root, dirs, files in os.walk(dire):\n        for filename in files:\n            if re.search('.md$|.markdown$', filename):\n                return True\n    return False\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "is_markdown_file": {
      "type": "FunctionDef",
      "name": "is_markdown_file",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 84,
      "parent": null,
      "params": [
        "filename"
      ],
      "have_return": true,
      "code_content": "def is_markdown_file(filename):\n    match = re.search('.md$|.markdown$', filename)\n    if not match:\n        return False\n    elif len(match.group()) is len('.md'):\n        return filename[:-3]\n    elif len(match.group()) is len('.markdown'):\n        return filename[:-9]\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [],
      "code_start_line": 87,
      "code_end_line": 109,
      "parent": null,
      "params": [],
      "have_return": true,
      "code_content": "def main():\n    book_name = sys.argv[1]\n\n    # mkdir the book folder\n    dir_input = os.path.join('./books', book_name, 'src')\n\n    # check the dst_dir\n    if not os.path.exists(dir_input):\n        print(dir_input)\n        os.makedirs(dir_input)\n    # Ensure the directory exists or create it\n    if not os.path.exists(dir_input):\n        os.makedirs(dir_input)\n\n    # Then proceed to create the file\n    output_path = os.path.join(dir_input, 'SUMMARY.md')\n    output = open(output_path, 'w')\n    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')\n    output.write('# Summary\\n\\n')\n    output_markdown(dir_input, dir_input, output)\n\n    print('GitBook auto summary finished:) ')\n    return 0\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    }
  },
  "display/book_tools/generate_repoagent_books.py": {
    "main": {
      "type": "FunctionDef",
      "name": "main",
      "md_content": [],
      "code_start_line": 7,
      "code_end_line": 44,
      "parent": null,
      "params": [],
      "have_return": false,
      "code_content": "def main():\n    markdown_docs_folder = sys.argv[1]\n    book_name = sys.argv[2]\n    repo_path = sys.argv[3]\n\n    # mkdir the book folder\n    dst_dir = os.path.join('./books', book_name, 'src')\n    docs_dir = os.path.join(repo_path, markdown_docs_folder)\n\n    # check the dst_dir\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(\"mkdir %s\" % dst_dir)\n\n    # cp the Markdown_Docs_folder to dst_dir\n    for item in os.listdir(docs_dir):\n        src_path = os.path.join(docs_dir, item)\n        dst_path = os.path.join(dst_dir, item)\n\n        # check the src_path\n        if os.path.isdir(src_path):\n            # if the src_path is a folder, use shutil.copytree to copy\n            shutil.copytree(src_path, dst_path)\n            print(\"copytree %s to %s\" % (src_path, dst_path))\n        else:\n            # if the src_path is a file, use shutil.copy2 to copy\n            shutil.copy2(src_path, dst_path)\n            print(\"copy2 %s to %s\" % (src_path, dst_path))\n\n    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n\n    # create book README.md if not exist\n    create_book_readme_if_not_exist(dst_dir)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated"
    },
    "create_book_readme_if_not_exist": {
      "type": "FunctionDef",
      "name": "create_book_readme_if_not_exist",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 41,
      "parent": "main",
      "params": [
        "dire"
      ],
      "have_return": false,
      "code_content": "    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated"
    }
  }
}